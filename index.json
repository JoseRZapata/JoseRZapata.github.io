[{"authors":["joserzapata"],"categories":null,"content":"Researcher at GIDATIC and Professor at the Faculty of Information and Communication Technologies (TIC), Universidad Pontificia Bolivariana (UPB).\nMy interests are based around Data science and Audio and Music information technologies, which includes Music information retrieval, Machine Learning, Audio analysis and Data analysis. EN ESPAÑOL\n","date":1586433284,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":1590789837,"objectID":"7378ef29713b8215ab7deca02c74bf8a","permalink":"https://joserzapata.github.io/authors/joserzapata/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/authors/joserzapata/","section":"authors","summary":"Researcher at GIDATIC and Professor at the Faculty of Information and Communication Technologies (TIC), Universidad Pontificia Bolivariana (UPB).\nMy interests are based around Data science and Audio and Music information technologies, which includes Music information retrieval, Machine Learning, Audio analysis and Data analysis. EN ESPAÑOL","tags":null,"title":"Jose R. Zapata","type":"authors"},{"authors":["Jose R. Zapata"],"categories":["Data-Science"],"content":" Esta es una traduccion propia al español del Apendice B (Machine Learning Project Checklist) del libro Hands-On Machine Learning with Scikit-Learn, Keras, and TensorFlow: Concepts, Tools, and Techniques to Build Intelligent Systems 2nd Edition de Aurélien Géron con algunos pasos propios agregados o que he encontrado en otros libros o cursos que he realizado.\nEste Libro me ha gustado mucho, para mi es el libro practico mas completo sobre machine learning con python que he leido, tiene una excelente estructura, codigo en python muy bien explicado, ademas muchos tips y sugerencias para realizar un proyecto de machine learning.\nEl libro esta acompañado por un repositorio con Jupyter Notebooks: https://github.com/ageron/handson-ml2\nTable of Contents  1. Definir el problema y mirar el panorama general. 2. Obténer los datos 3. Explorar los datos para obtener información. 4. Preparación de los datos. 5. Exploración y selección de modelos 6. Afinar los modelos. 7. Presentacion de la solución. 8. Desplegar, monitorear y mantener el sistema. Referencias  \nEsta lista de verificación puede ser una guia paso a paso para proyectos de Machine Learning.\n1. Definir el problema y mirar el panorama general.  Definir el objetivo en términos del negocio. ¿Cómo se usará su solución? ¿Cuáles son las soluciones actuales (si las hay)? ¿Cómo se debe enmarcar este problema (supervisado / no supervisado, en línea / fuera de línea, etc.) ¿Cómo se debe medir el desempeño o el rendimiento de la solucion? ¿La medida de desempeño está alineada con el objetivo del negocio? ¿Cuál sería el desempeño o rendimiento mínimo necesario para alcanzar el objetivo del negocio? ¿Cuáles son los problemas parecidos? ¿Se puede reutilizar experiencias o herramientas ya creadas? ¿Hay experiencia del problema disponible? ¿Cómo se puede resolver el problema manualmente? Hacer un listado de los supuestos que hay hasta este momento. Verificar los supuestos si es posible.  2. Obténer los datos  Nota: automatizar tanto como sea posible este proceso para que pueda obtener fácilmente datos nuevos.    Enumere los datos que necesita y la cantidad que necesita. Busque y documente dónde se pueden obtener los datos. Compruebe cuánto espacio de almacenamiento ocuparán los datos. Verifique las limitaciones legales y obtener autorización a los datos si es necesario. Obtener autorizaciones de acceso a los datos. Reservar suficiente espacio de almacenamiento para el proyecto. Obtener los datos. Convertir los datos a un formato que se pueda manipular fácilmente (sin cambiar los datos en sí). Asegúrarse de que la información confidencial se elimine o se proteja (por ejemplo, anonimizar los datos). Verificar el tamaño y el tipo de datos (series de tiempo, muestra de datos, geoposicionamiento, etc.). Separar un conjunto de datos prueba, dejarlos a un lado y nunca mirarlos.  3. Explorar los datos para obtener información.  Nota: intente obtener información de un experto en el tema para estos pasos.    Crear una copia de los datos para explorarlos (muestreándolos a un tamaño manejable si necesario). Crar un Jupyter Notebook para mantener un registro de la exploración de los datos. Estudiar cada atributo y sus características (Analisis Univariable):  Nombre Tipo de dato (categórico, int / float, acotado / no acotado, texto, estructurado, etc.) porcentaje (%) de valores faltantes. Ruido y tipo de ruido (estocástico, valores atípicos, errores de redondeo, etc.) ¿Son posiblemente útiles para el proyecto? Tipo de distribución (gaussiana, uniforme, logarítmica, etc.)  Para los proyectos de aprendizaje supervisado, identifique los atributos objetivo (target). Visualizacion de los datos. Estudiar las correlaciones entre atributos (Analisis Bivariable). Estudiar cómo resolver el problema manualmente. Identificar las transformaciones que tal vez se puedan aplicar. Identificar datos adicionales que pueden ser útiles. Documentar lo que ha aprendido.  4. Preparación de los datos. Para exponer mejor los patrones de los datos y usarlos con los algoritmos de Machine Learning.\n Notas:\n Trabaje en copias de los datos (mantenga intacto el conjunto de datos original). Escriba funciones para todas las transformaciones de datos que realice, por cinco razones:  Para que pueda preparar fácilmente los datos la próxima vez que obtenga un conjunto de datos nuevo Para que pueda aplicar estas transformaciones en proyectos futuros Para limpiar y preparar el set de datos de prueba Para limpiar y preparar nuevas instancias de datos una vez que su solución esté activa (produccion) Para que sea fácil probar diferentes formas de preparación de datos como hiperparámetros       Limpieza de datos:  Eliminar registros datos duplicados (disminuir el numero de datos) Corregir o eliminar valores atípicos (opcional). Los valores atípicos pueden separarse del dataset dependiendo del problema del proyecto (por ejemplo, deteccion de anomalias). Completar los valores faltantes (por ejemplo, con cero, media, mediana \u0026hellip;) o eliminar las filas (o columnas).  Selección de atributos (Feature Selection) (opcional): Descartar los atributos que no proporcionan información útil para el proyecto. Eliminar registros duplicados (al eliminar atributos pueden quedar registros iguales) Ingeniería de atributos (Feature Engineering), cuando sea apropiado: Discretizar las atributos continuas. Descomponer en partes los atributos (p. Ej., Categóricas, fecha / hora, etc.). Agregar transformaciones prometedoras de las atributos (por ejemplo, log(x), sqrt(x), x^2, etc.). Aplicar funciones a los datos para agregar nuevos atributos. Escalado de atributos (Feature Scaling).: estandarizar o normalizar atributos.  5. Exploración y selección de modelos  Notas:\n Si se tiene una gran cantidad de datos, es posible que desee hacer un muestreo de los datos para tener conjuntos de entrenamiento más pequeños, de esta forma se pueden entrenar varios modelos diferentes en un tiempo razonable (se debe tener en cuenta que esto penaliza modelos complejos como redes neuronales grandes o Random Forest). Una vez más, intentar automatizar estos pasos tanto como sea posible.     Entrenar muchos modelos rápidos y utilizando parámetros estándar de diferentes categorías (p. Ej., Lineales, Naive Bayes, SVM, Random Forest, redes neuronales, etc.). Medir y comparar su desempeño. Para cada modelo, utilice la validación cruzada (Cross validation) de N subconjuntos y calcule la media y la desviación estándar de la medida de rendimiento en las N evaluaciones. Analice las variables más significativas para cada algoritmo. Analice los tipos de errores que cometen los modelos. ¿Qué datos habría utilizado un humano para evitar estos errores? Realizar rapidamente una selección de atributos e ingeniería de atributos (Feature selection, Feature Engineering). Realice una o dos iteraciones rápidas más de los cinco pasos anteriores. Hacer una lista corta de los tres a cinco modelos más prometedores, prefiriendo seleccionar modelos que cometan diferentes tipos de errores (diversidad de los errores).  6. Afinar los modelos.  Notas:\n Se deberá utilizar la mayor cantidad de datos posible para este paso, especialmente a medida que avanza hacia el final del ajuste fino del modelo. Como siempre, automatizar lo que se pueda.     Ajuste los hiperparámetros (hyperparameter tunning) mediante validación cruzada (cross validation). Tratar las elecciones de transformación de datos como hiperparámetros, especialmente cuando no esta seguro de ellos (por ejemplo, ¿debería reemplazar los valores faltantes con cero o con el valor medio? ¿O simplemente dejar eliminar las filas?). A menos que haya muy pocos valores de hiperparámetros para explorar, prefiera la búsqueda aleatoria (random search) a la búsqueda de cuadrícula (grid search). Si el entrenamiento es muy largo, es posible que prefiera un enfoque de optimización bayesiano (por ejemplo, utilizando procesos previos gaussianos, como lo describen Jasper Snoek, Hugo Larochelle y Ryan Adams1. Pruebe los métodos de Ensamble (ensemble methods). La combinación de sus mejores modelos a menudo tendrá un mejor rendimiento que se ejecutan individualmente (hay mejor desempeño si hay diversidad de errores entre los modelos). Una vez que esté seguro de su modelo final, mida su rendimiento en el conjunto de prueba (test set, separado al inicio) para estimar el error de generalización.   No modifique su modelo después de medir el error de generalización: simplemente comenzaría a sobreajustar el conjunto de prueba.   7. Presentacion de la solución.  Documentar lo que ha hecho. Crear una buena presentación. Asegúrese de resaltar el panorama general del proyecto o del problema primero. Explicar por qué la solución encontrada logra el objetivo buscado. No olvidar presentar puntos interesantes que se notaron en el camino. Describir qué funcionó y qué no. Enumerar los supuestos y las limitaciones del sistema. Asegúrarse de que los hallazgos clave se comuniquen a través de hermosas visualizaciones o declaraciones fáciles de recordar (por ejemplo, \u0026ldquo;el ingreso medio es el predictor número uno de los precios de la vivienda\u0026rdquo;).  8. Desplegar, monitorear y mantener el sistema.  Preparar la solución para producción (conectar las entradas de datos de producción, escribir pruebas unitarias (unit test), etc.). Escribir código de monitoreo para verificar el rendimiento en tiempo real del sistema a intervalos regulares y activar alertas cuando se caiga o falle. Tener cuidado con la lenta degradación: los modelos tienden a \u0026ldquo;pudrirse\u0026rdquo; a medida que los datos evolucionan, el modelo va perdiendo validez en el tiempo. La medición del rendimiento puede requerir supervision humana (por ejemplo, a través de un servicio de crowdsourcing). Controlar la calidad de los datos de entrada (por ejemplo, Un sensor que funciona mal y que envía valores aleatorios, o la salida de datos de otro equipo se vuelve obsoleta). Esto es particularmente importante para los sistemas de aprendizaje en línea (online learning). Vuelva a entrenar sus modelos de forma regular con datos nuevos (automatizar lo más posible).  Referencias  Hands-On Machine Learning with Scikit-Learn, Keras, and TensorFlow: Concepts, Tools, and Techniques to Build Intelligent Systems https://github.com/ageron/handson-ml2 https://machinelearningmastery.com/machine-learning-in-python-step-by-step/ https://medium.com/@mcintyreshiv/how-to-master-python-for-machine-learning-from-scratch-a-step-by-step-tutorial-8c6569895cb0   “Practical Bayesian Optimization of Machine Learning Algorithms,” J. Snoek, H. Larochelle, R. Adams (2012) ^   ","date":1586433284,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1586433284,"objectID":"1d594254846e534bf97169f0445002f4","permalink":"https://joserzapata.github.io/post/lista-proyecto-machine-learning/","publishdate":"2020-04-09T06:54:44-05:00","relpermalink":"/post/lista-proyecto-machine-learning/","section":"post","summary":"Checklist y preguntas para realizar un proyecto de machine learning","tags":["Data-Science"],"title":"Paso a paso en un Proyecto Machine Learning","type":"post"},{"authors":["Jose R. Zapata"],"categories":["Data-Science"],"content":"                 He visto en las redes sociales varias visualizaciones de los datos del COVID 19 y queria realizarlos en Python para tener la actualizacion de las graficas actualizadas cada dia, y ademas practicar el uso de plotly para visualizacion interactiva.\nPrincipalmente los datos que se tienen es del numero de personas contegiadas y personas muertas quiero visualizar los datos de personas recuperadas y casos activos.\nPueden interactuar con las graficas con el mouse y las Graficas se actualizaran diariamente con los nuevos datos!\nInformacion extraida de 2019 Novel Coronavirus COVID-19 (2019-nCoV) Data Repository by Johns Hopkins CSSE\nhttps://github.com/CSSEGISandData/COVID-19\nActualizaciones:\n 27/May/2020 Se Agregar los datos de las personas recuperadas y se calculan los casos Activos 29/May/2020 Se agrega Bar chart race  Table of Contents  Paquetes de Python y Datos  Paquetes de Python Importar datos Datos CSSEGISandData/COVID-19  Eliminar Ubicacion Casos Activos Consolidar datos Datos Mundiales   Covid19 en el Mundo  Evolucion Animada de Casos Activos por Pais Visualizacion con Plotly Valores Mundiales de Casos Confirmados, Activos, Recuperados y Muertos Mapa Mundial de Confirmados por Pais Confirmados vs Muertos por pais Progresion Mundial en el Tiempo de de Confirmados y Muertos Total Casos Confirmados de COVID 19 por Pais Total Casos Confirmados de COVID 19 por Pais (Excluyendo los 8 mas infectados) Animacion del Mapa de Evolucion Temporal del Codiv 19  Covid 19 en Colombia  Numero de Casos COVID 19 en Colombia  Actualizacion de las Graficas cada 24 Horas Codigo Fuente Jupyter notebook Refencias  \nPaquetes de Python y Datos Paquetes de Python !pip install chart_studio -qimport pandas as pd import plotly.express as px import numpy as np import chart_studio Para subir las graficas interactivas de plotly a chart studio\n#chart-studio api username = \u0026#39;\u0026#39; # your username api_key = \u0026#39;\u0026#39; # your api api_key chart_studio.tools.set_credentials_file(username=username, api_key=api_key) import chart_studio.plotly as py Importar datos confirmed = pd.read_csv(\u0026#39;https://github.com/CSSEGISandData/COVID-19/raw/master/csse_covid_19_data/csse_covid_19_time_series/time_series_covid19_confirmed_global.csv\u0026#39;) death = pd.read_csv(\u0026#39;https://github.com/CSSEGISandData/COVID-19/raw/master/csse_covid_19_data/csse_covid_19_time_series/time_series_covid19_deaths_global.csv\u0026#39;) recovered = pd.read_csv(\u0026#39;https://github.com/CSSEGISandData/COVID-19/raw/master/csse_covid_19_data/csse_covid_19_time_series/time_series_covid19_recovered_global.csv\u0026#39;) Datos CSSEGISandData/COVID-19 Descripcion de los datos en ingles\nProvince/State: China - province name; US/Canada/Australia/ - city name, state/province name; Others - name of the event (e.g., \u0026ldquo;Diamond Princess\u0026rdquo; cruise ship); other countries - blank.\nCountry/Region: country/region name conforming to WHO (will be updated).\nLast Update: MM/DD/YYYY HH:mm (24 hour format, in UTC).\nConfirmed: the number of confirmed cases. For Hubei Province: from Feb 13 (GMT +8), we report both clinically diagnosed and lab-confirmed cases. For lab-confirmed cases only (Before Feb 17), please refer to who_covid_19_situation_reports. For Italy, diagnosis standard might be changed since Feb 27 to \u0026ldquo;slow the growth of new case numbers.\u0026rdquo;\nDeaths: the number of deaths.\nRecovered: the number of recovered cases.\nconfirmed.head()  .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; }    Country/Region 1/22/20 1/23/20 1/24/20 1/25/20 1/26/20 1/27/20 1/28/20 1/29/20 1/30/20 1/31/20 2/1/20 2/2/20 2/3/20 2/4/20 2/5/20 2/6/20 2/7/20 2/8/20 2/9/20 2/10/20 2/11/20 2/12/20 2/13/20 2/14/20 2/15/20 2/16/20 2/17/20 2/18/20 2/19/20 2/20/20 2/21/20 2/22/20 2/23/20 2/24/20 2/25/20 2/26/20 2/27/20 2/28/20 2/29/20 3/1/20 3/2/20 3/3/20 3/4/20 3/5/20 3/6/20 3/7/20 3/8/20 3/9/20 3/10/20 3/11/20 3/12/20 3/13/20 3/14/20 3/15/20 3/16/20 3/17/20 3/18/20 3/19/20     0 Thailand 2 3 5 7 8 8 14 14 14 19 19 19 19 25 25 25 25 32 32 32 33 33 33 33 33 34 35 35 35 35 35 35 35 35 37 40 40 41 42 42 43 43 43 47 48 50 50 50 53 59 70 75 82 114 147 177 212 272   1 Japan 2 1 2 2 4 4 7 7 11 15 20 20 20 22 22 45 25 25 26 26 26 28 28 29 43 59 66 74 84 94 105 122 147 159 170 189 214 228 241 256 274 293 331 360 420 461 502 511 581 639 639 701 773 839 825 878 889 924   2 Singapore 0 1 3 3 4 5 7 7 10 13 16 18 18 24 28 28 30 33 40 45 47 50 58 67 72 75 77 81 84 84 85 85 89 89 91 93 93 93 102 106 108 110 110 117 130 138 150 150 160 178 178 200 212 226 243 266 313 345   3 Nepal 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1   4 Malaysia 0 0 0 3 4 4 4 7 8 8 8 8 8 10 12 12 12 16 16 18 18 18 19 19 22 22 22 22 22 22 22 22 22 22 22 22 23 23 25 29 29 36 50 50 83 93 99 117 129 149 149 197 238 428 566 673 790 900     Eliminar Ubicacion Se va realizar un analisis general de los datos y No se van a tomar los datos geograficos de latitud, longitud y los datos de Province/State estan incompletos.\nSolo se realizara un analisis por pais entonces se eliminaran las columnas mencionadas anteriormente\nconfirmed = confirmed.drop(columns=[\u0026#39;Lat\u0026#39;, \u0026#39;Long\u0026#39;,\u0026#39;Province/State\u0026#39;]) death = death.drop(columns=[\u0026#39;Lat\u0026#39;, \u0026#39;Long\u0026#39;,\u0026#39;Province/State\u0026#39;]) recovered = recovered.drop(columns=[\u0026#39;Lat\u0026#39;, \u0026#39;Long\u0026#39;,\u0026#39;Province/State\u0026#39;]) Casos Activos Se calcula a partir del número de personas confirmadas - muertos - recuperados\nactive =confirmed.copy() active.iloc[:,1:] = active.iloc[:,1:] - death.iloc[:,1:] - recovered.iloc[:,1:] Consolidar datos confirmed_group = confirmed.groupby(by=\u0026#39;Country/Region\u0026#39;).aggregate(np.sum).T confirmed_group.index.name = \u0026#39;date\u0026#39; confirmed_group = confirmed_group.reset_index()recovered_group = recovered.groupby(by=\u0026#39;Country/Region\u0026#39;).aggregate(np.sum).T recovered_group.index.name = \u0026#39;date\u0026#39; recovered_group = recovered_group.reset_index()active_group = active.groupby(by=\u0026#39;Country/Region\u0026#39;).aggregate(np.sum).T active_group.index.name = \u0026#39;date\u0026#39; active_group = active_group.reset_index()death_group = death.groupby(by=\u0026#39;Country/Region\u0026#39;).aggregate(np.sum).T death_group.index.name = \u0026#39;date\u0026#39; death_group = death_group.reset_index()confirmed_melt = confirmed_group.melt(id_vars=\u0026#34;date\u0026#34;).copy() confirmed_melt.rename(columns = {\u0026#39;value\u0026#39;:\u0026#39;Confirmados\u0026#39;, \u0026#39;date\u0026#39;:\u0026#39;Fecha\u0026#39;}, inplace = True) death_melt = death_group.melt(id_vars=\u0026#34;date\u0026#34;) death_melt.rename(columns = {\u0026#39;value\u0026#39;:\u0026#39;Muertos\u0026#39;, \u0026#39;date\u0026#39;:\u0026#39;Fecha\u0026#39;}, inplace = True) Datos Mundiales # Numero de Casos confirmados por dia en el mundo column_names = [\u0026#34;Fecha\u0026#34;, \u0026#34;Confirmados\u0026#34;, \u0026#34;Recuperados\u0026#34;,\u0026#34;Muertos\u0026#34;] world = pd.DataFrame(columns = column_names) world[\u0026#39;Fecha\u0026#39;] = confirmed_group[\u0026#39;date\u0026#39;].copy() world[\u0026#39;Confirmados\u0026#39;] = confirmed_group.iloc[:,1:].sum(1) world[\u0026#39;Muertos\u0026#39;] = death_group.iloc[:,1:].sum(1) world[\u0026#39;Recuperados\u0026#39;] = recovered_group.iloc[:,1:].sum(1) world[\u0026#39;Activos\u0026#39;] = active_group.iloc[:,1:].sum(1) Covid19 en el Mundo Evolucion Animada de Casos Activos por Pais La gráfica animada de la evolución temporal de los casos activos por país, la he creado con la libreria Pandas alive y Bar Chart Race.\n  import pandas_alive active_evol = active_group.set_index(\u0026#39;date\u0026#39;) active_evol.index = pd.to_datetime(active_evol.index) active_evol.plot_animated(filename=\u0026#39;evolucion_casos_activos.mp4\u0026#39;, n_bars=8,n_visible=8, title=\u0026#39;Evolución en el tiempo de Casos Activos COVID-19 por pais \\nhttps://joserzapata.github.io/\u0026#39;, perpendicular_bar_func=\u0026#39;mean\u0026#39;, dpi=300, period_label={\u0026#39;x\u0026#39;: .99, \u0026#39;y\u0026#39;: .25, \u0026#39;ha\u0026#39;: \u0026#39;right\u0026#39;, \u0026#39;va\u0026#39;: \u0026#39;center\u0026#39;}, period_fmt=\u0026#39;%B %d, %Y\u0026#39;, period_summary_func=lambda v: {\u0026#39;x\u0026#39;: .99, \u0026#39;y\u0026#39;: .18, \u0026#39;s\u0026#39;: f\u0026#39;Total Activos: {v.nlargest(8).sum():,.0f}\u0026#39;, \u0026#39;ha\u0026#39;: \u0026#39;right\u0026#39;, \u0026#39;size\u0026#39;: 9, \u0026#39;family\u0026#39;: \u0026#39;Courier New\u0026#39;}) Visualizacion con Plotly Valores Mundiales de Casos Confirmados, Activos, Recuperados y Muertos  temp = pd.DataFrame(world.iloc[-1,:]).T tm = temp.melt(id_vars=\u0026#34;Fecha\u0026#34;, value_vars=[ \u0026#34;Confirmados\u0026#34;,\u0026#34;Activos\u0026#34;,\u0026#34;Recuperados\u0026#34;,\u0026#34;Muertos\u0026#34;]) fig = px.bar(tm, x=\u0026#34;variable\u0026#34; , y=\u0026#34;value\u0026#34;, color= \u0026#39;variable\u0026#39;, text=\u0026#39;value\u0026#39;, color_discrete_sequence=[\u0026#34;teal\u0026#34;,\u0026#34;navy\u0026#34;,\u0026#34;green\u0026#34;, \u0026#34;coral\u0026#34;], height=500, width=600, title= f\u0026#39;Total de Casos Mundiales de COVID 19 - {str(world.iloc[-1,0])}\u0026#39;) fig.update_traces(textposition=\u0026#39;outside\u0026#39;)#poner los valores de las barras fuera fig.layout.update(showlegend=False, yaxis = {\u0026#34;title\u0026#34;: {\u0026#34;text\u0026#34;: \u0026#34;Numero de Personas\u0026#34;}}, # Cambiar texto eje y xaxis = {\u0026#34;title\u0026#34;: {\u0026#34;text\u0026#34;: \u0026#34;\u0026#34;}} #Esconder nombre eje x ) # grabar grafica en chart-studio si se proporciona el api-key if api_key: py.plot(fig, filename = \u0026#39;total_casos_general\u0026#39;, auto_open=False) fig.show() Mapa Mundial de Confirmados por Pais Mover el Mouse sobre el mapa para ver la informacion de cada pais \nconfirmed_melt[\u0026#39;Fecha\u0026#39;] = pd.to_datetime(confirmed_melt[\u0026#39;Fecha\u0026#39;]) confirmed_melt[\u0026#39;Fecha\u0026#39;] = confirmed_melt[\u0026#39;Fecha\u0026#39;].dt.strftime(\u0026#39;%m/%d/%Y\u0026#39;) confirmed_melt[\u0026#39;size\u0026#39;] = confirmed_melt[\u0026#39;Confirmados\u0026#39;].pow(0.3) max_Fecha = confirmed_melt[\u0026#39;Fecha\u0026#39;].max() conf_max = confirmed_melt[confirmed_melt[\u0026#39;Fecha\u0026#39;]== max_Fecha] conf_max.dropna(inplace=True) #eliminar filas con valores faltantes fig = px.scatter_geo(conf_max, locations=\u0026#34;Country/Region\u0026#34;, locationmode=\u0026#39;country names\u0026#39;, color=\u0026#34;Confirmados\u0026#34;, size=\u0026#39;size\u0026#39;, hover_name=\u0026#34;Country/Region\u0026#34;, range_color= [0, max(confirmed_melt[\u0026#39;Confirmados\u0026#39;])+2], projection=\u0026#34;natural earth\u0026#34;, title=\u0026#39;Mapa de Confirmados COVID 19 por Pais\u0026#39;) fig.update(layout_coloraxis_showscale=False) #py.plot(fig, filename = \u0026#39;mapa_confirmados_pais\u0026#39;, auto_open=False) fig.show() Confirmados vs Muertos por pais  death_melt[\u0026#39;Fecha\u0026#39;] = pd.to_datetime(death_melt[\u0026#39;Fecha\u0026#39;]) death_melt[\u0026#39;Fecha\u0026#39;] = death_melt[\u0026#39;Fecha\u0026#39;].dt.strftime(\u0026#39;%m/%d/%Y\u0026#39;) max_Fecha = death_melt[\u0026#39;Fecha\u0026#39;].max() death_max = death_melt[death_melt[\u0026#39;Fecha\u0026#39;]== max_Fecha].copy() death_max.dropna(inplace=True) #eliminar filas con valores faltantes fig = px.scatter(full_melt_max.sort_values(\u0026#39;Muertos\u0026#39;, ascending=False).iloc[:15, :], x=\u0026#39;Confirmados\u0026#39;, y=\u0026#39;Muertos\u0026#39;, color=\u0026#39;Country/Region\u0026#39;, size=\u0026#39;Confirmados\u0026#39;, height=500, text=\u0026#39;Country/Region\u0026#39;, log_x=True, log_y=True, title= f\u0026#39;Muertos vs Confirmados - {max_Fecha} - (15 Paises)\u0026#39;) fig.update_traces(textposition=\u0026#39;top center\u0026#39;) fig.layout.update(showlegend = False) #py.plot(fig, filename = \u0026#39;scatter_muertos_confirmados\u0026#39;, auto_open=False) fig.show() Progresion Mundial en el Tiempo de de Confirmados y Muertos  world_melt = world.melt(id_vars=\u0026#39;Fecha\u0026#39;, value_vars= list(world.columns)[1:], var_name=None) fig = px.line(world_melt, x=\u0026#34;Fecha\u0026#34;, y= \u0026#39;value\u0026#39;, color=\u0026#39;variable\u0026#39;, color_discrete_sequence=[\u0026#34;teal\u0026#34;,\u0026#34;green\u0026#34;,\u0026#34;coral\u0026#34;, \u0026#34;navy\u0026#34;], title=f\u0026#39;Total de Casos en el tiempo de COVID 19 - {world.iloc[-1,0]}\u0026#39;) for n in list(world.columns)[1:]: fig.add_annotation(x=world.iloc[-1,0], y=world.loc[world.index[-1],n], text=n, xref=\u0026#34;x\u0026#34;,yref=\u0026#34;y\u0026#34;, showarrow=True, ax=-50, ay=-20) # Indicador de numero total de confirmados fig.add_indicator( title= {\u0026#39;text\u0026#39;:\u0026#39;Confirmados\u0026#39;, \u0026#39;font\u0026#39;:{\u0026#39;color\u0026#39;:\u0026#39;teal\u0026#39;}}, value = world[\u0026#39;Confirmados\u0026#39;].iloc[-1], mode = \u0026#34;number+delta\u0026#34;, delta = {\u0026#34;reference\u0026#34;: world[\u0026#39;Confirmados\u0026#39; ].iloc[-2], \u0026#39;relative\u0026#39;: True },domain = {\u0026#39;x\u0026#39;: [0, 0.25], \u0026#39;y\u0026#39;: [0.15, .4]}) #Indicador numero total de Activos fig.add_indicator(title={\u0026#39;text\u0026#39;:\u0026#39;Activos\u0026#39;, \u0026#39;font\u0026#39;:{\u0026#39;color\u0026#39;:\u0026#39;navy\u0026#39;}}, value = world[\u0026#39;Activos\u0026#39;].iloc[-1], mode = \u0026#34;number+delta\u0026#34;, delta = {\u0026#34;reference\u0026#34;: world[\u0026#39;Activos\u0026#39; ].iloc[-2], \u0026#39;relative\u0026#39;: True },domain = {\u0026#39;x\u0026#39;: [0, 0.25], \u0026#39;y\u0026#39;: [0.6, .85]}) #Indicador numero total de Recuperados fig.add_indicator(title={\u0026#39;text\u0026#39;:\u0026#39;Recuperados\u0026#39;, \u0026#39;font\u0026#39;:{\u0026#39;color\u0026#39;:\u0026#39;green\u0026#39;}}, value = world[\u0026#39;Recuperados\u0026#39;].iloc[-1], mode = \u0026#34;number+delta\u0026#34;, delta = {\u0026#34;reference\u0026#34;: world[\u0026#39;Recuperados\u0026#39; ].iloc[-2], \u0026#39;relative\u0026#39;: True },domain = {\u0026#39;x\u0026#39;: [0.25, 0.50], \u0026#39;y\u0026#39;: [0.6, .85]}) #Indicador numero total de muertos fig.add_indicator(title={\u0026#39;text\u0026#39;:\u0026#39;Muertos\u0026#39;, \u0026#39;font\u0026#39;:{\u0026#39;color\u0026#39;:\u0026#39;coral\u0026#39;}}, value = world[\u0026#39;Muertos\u0026#39;].iloc[-1], mode = \u0026#34;number+delta\u0026#34;, delta = {\u0026#34;reference\u0026#34;: world[\u0026#39;Muertos\u0026#39; ].iloc[-2], \u0026#39;relative\u0026#39;: True },domain = {\u0026#39;x\u0026#39;: [0.25, 0.5], \u0026#39;y\u0026#39;: [0.15, .4]}) fig.layout.update(showlegend = False, yaxis = {\u0026#34;title\u0026#34;: {\u0026#34;text\u0026#34;: \u0026#34;Numero de Personas\u0026#34;}}, # Cambiar texto eje y ) # grabar grafica en chart-studio si se proporciona el api-key if api_key: py.plot(fig, filename = \u0026#39;total_casos_serie\u0026#39;, auto_open=False) fig.show() Total Casos Confirmados de COVID 19 por Pais  df1 = confirmed_group.copy() # Cambiar el nombre de la columna df1.rename(columns = {\u0026#39;date\u0026#39;:\u0026#39;Fecha\u0026#39;}, inplace = True) df_melt = df1.melt(id_vars=\u0026#39;Fecha\u0026#39;, value_vars= list(df1.columns)[1:], var_name=None) fig = px.line(df_melt, x=\u0026#39;Fecha\u0026#39; , y=\u0026#39;value\u0026#39;, color=\u0026#39;Country/Region\u0026#39;, color_discrete_sequence=px.colors.qualitative.G10, title=f\u0026#39;Total Casos Confirmados de COVID 19 por Pais (Excluyendo China) - {world.iloc[-1,0]}\u0026#39;) # 8 paises mas infectados fecha = df1[\u0026#39;Fecha\u0026#39;].iloc[-1] #obtener la fecha del ultimo dato paises = df1.iloc[-1,1:] #obtener la serie sin el primer dato, fecha paises.sort_values(ascending=False, inplace=True) mas_infectados=[] for n in range(8): fig.add_annotation(x=fecha, y=paises[n], text=paises.index[n], showarrow=True, ax=+30, ay=0) mas_infectados.append(paises.index[n]) fig.layout.update(showlegend=False, yaxis = {\u0026#34;title\u0026#34;: {\u0026#34;text\u0026#34;: \u0026#34;Numero de Personas\u0026#34;}}, # Cambiar texto eje y ) # grabar grafica en chart-studio #py.plot(fig, filename = \u0026#39;total_casos_no_china\u0026#39;, auto_open=False) fig.show() Total Casos Confirmados de COVID 19 por Pais (Excluyendo los 8 mas infectados)  df2 = confirmed_group.drop(columns= mas_infectados).copy() # Cambiar el nombre de la columna df2.rename(columns = {\u0026#39;date\u0026#39;:\u0026#39;Fecha\u0026#39;}, inplace = True) df_melt2 = df2.melt(id_vars=\u0026#39;Fecha\u0026#39;, value_vars= list(df2.columns)[1:], var_name=None) fig = px.line(df_melt2, x=\u0026#39;Fecha\u0026#39; , y=\u0026#39;value\u0026#39;, color=\u0026#39;Country/Region\u0026#39;, color_discrete_sequence=px.colors.qualitative.G10, title=f\u0026#39;Total Casos Confirmados de COVID 19 por Pais (Excluyendo los 8 mas infectados) - {world.iloc[-1,0]}\u0026#39;) fecha = df2[\u0026#39;Fecha\u0026#39;].iloc[-1] #obtener la fecha del ultimo dato paises = df2.iloc[-1,1:] #obtener la serie sin el primer dato, fecha paises.sort_values(ascending=False, inplace=True) for n in range(8): fig.add_annotation(x=fecha, y=paises[n], text=paises.index[n], showarrow=True, ax=+30, ay=0) fig.layout.update(showlegend=False, yaxis = {\u0026#34;title\u0026#34;: {\u0026#34;text\u0026#34;: \u0026#34;Numero de Personas\u0026#34;}}, # Cambiar texto eje y ) #py.plot(fig, filename = \u0026#39;total_casos_no_8_infectados\u0026#39;, auto_open=False) fig.show() Animacion del Mapa de Evolucion Temporal del Codiv 19 Mover el Mouse sobre el mapa para ver la informacion de cada pais. Presionar el boton de play para ver la animacion.\n confirmed_melt[\u0026#39;Fecha\u0026#39;] = pd.to_datetime(confirmed_melt[\u0026#39;Fecha\u0026#39;]) confirmed_melt[\u0026#39;Fecha\u0026#39;] = confirmed_melt[\u0026#39;Fecha\u0026#39;].dt.strftime(\u0026#39;%m/%d/%Y\u0026#39;) confirmed_melt[\u0026#39;size\u0026#39;] = confirmed_melt[\u0026#39;Confirmados\u0026#39;].pow(0.3) confirmed_melt.dropna(inplace=True) #eliminar filas con valores faltantes fig = px.scatter_geo(confirmed_melt, locations=\u0026#34;Country/Region\u0026#34;, locationmode=\u0026#39;country names\u0026#39;, color=\u0026#34;Confirmados\u0026#34;, size=\u0026#39;size\u0026#39;, hover_name=\u0026#34;Country/Region\u0026#34;, range_color= [0, max(confirmed_melt[\u0026#39;Confirmados\u0026#39;])+2], projection=\u0026#34;natural earth\u0026#34;, animation_frame=\u0026#34;Fecha\u0026#34;, title=\u0026#39;Contagiados COVID 19 en el Tiempo\u0026#39;) fig.update(layout_coloraxis_showscale=False) #py.plot(fig, filename = \u0026#39;mapa_evolucion_temporal\u0026#39;, auto_open=False) fig.show() Covid 19 en Colombia Numero de Casos COVID 19 en Colombia  column_names = [\u0026#34;Fecha\u0026#34;, \u0026#34;Confirmados\u0026#34;, \u0026#34;Recuperados\u0026#34;,\u0026#34;Muertos\u0026#34;, \u0026#34;Activos\u0026#34;] colombia = pd.DataFrame(columns = column_names) colombia[\u0026#39;Fecha\u0026#39;] = confirmed_group[\u0026#39;Fecha\u0026#39;] colombia[\u0026#39;Confirmados\u0026#39;] = confirmed_group[\u0026#39;Colombia\u0026#39;] colombia[\u0026#39;Muertos\u0026#39;] = death_group[\u0026#39;Colombia\u0026#39;] colombia[\u0026#39;Recuperados\u0026#39;] = recovered_group[\u0026#39;Colombia\u0026#39;] colombia[\u0026#39;Activos\u0026#39;] = active_group[\u0026#39;Colombia\u0026#39;] df_melt3 = colombia.melt(id_vars=\u0026#39;Fecha\u0026#39;, value_vars= list(colombia.columns)[1:], var_name=None) fig = px.line(df_melt3, x=\u0026#39;Fecha\u0026#39; , y=\u0026#39;value\u0026#39;, color=\u0026#39;variable\u0026#39;, color_discrete_sequence=[\u0026#34;teal\u0026#34;,\u0026#34;green\u0026#34;,\u0026#34;coral\u0026#34;, \u0026#34;navy\u0026#34;], title=f\u0026#39;Corona virus (COVID 19) en Colombia - {colombia.iloc[-1,0]}\u0026#39;) # Indicador de numero total de confirmados fig.add_indicator( title= {\u0026#39;text\u0026#39;:\u0026#39;Confirmados\u0026#39;, \u0026#39;font\u0026#39;:{\u0026#39;color\u0026#39;:\u0026#39;teal\u0026#39;}}, value = colombia[\u0026#39;Confirmados\u0026#39;].iloc[-1], mode = \u0026#34;number+delta\u0026#34;, delta = {\u0026#34;reference\u0026#34;: colombia[\u0026#39;Confirmados\u0026#39; ].iloc[-2], \u0026#39;relative\u0026#39;: True },domain = {\u0026#39;x\u0026#39;: [0, 0.25], \u0026#39;y\u0026#39;: [0.15, .4]}) #Indicador numero total de Activos fig.add_indicator(title={\u0026#39;text\u0026#39;:\u0026#39;Activos\u0026#39;, \u0026#39;font\u0026#39;:{\u0026#39;color\u0026#39;:\u0026#39;navy\u0026#39;}}, value = colombia[\u0026#39;Activos\u0026#39;].iloc[-1], mode = \u0026#34;number+delta\u0026#34;, delta = {\u0026#34;reference\u0026#34;: colombia[\u0026#39;Activos\u0026#39; ].iloc[-2], \u0026#39;relative\u0026#39;: True },domain = {\u0026#39;x\u0026#39;: [0, 0.25], \u0026#39;y\u0026#39;: [0.6, .85]}) #Indicador numero total de Recuperados fig.add_indicator(title={\u0026#39;text\u0026#39;:\u0026#39;Recuperados\u0026#39;, \u0026#39;font\u0026#39;:{\u0026#39;color\u0026#39;:\u0026#39;green\u0026#39;}}, value = colombia[\u0026#39;Recuperados\u0026#39;].iloc[-1], mode = \u0026#34;number+delta\u0026#34;, delta = {\u0026#34;reference\u0026#34;: colombia[\u0026#39;Recuperados\u0026#39; ].iloc[-2], \u0026#39;relative\u0026#39;: True },domain = {\u0026#39;x\u0026#39;: [0.25, 0.50], \u0026#39;y\u0026#39;: [0.6, .85]}) #Indicador numero total de muertos fig.add_indicator(title={\u0026#39;text\u0026#39;:\u0026#39;Muertos\u0026#39;, \u0026#39;font\u0026#39;:{\u0026#39;color\u0026#39;:\u0026#39;coral\u0026#39;}}, value = colombia[\u0026#39;Muertos\u0026#39;].iloc[-1], mode = \u0026#34;number+delta\u0026#34;, delta = {\u0026#34;reference\u0026#34;: colombia[\u0026#39;Muertos\u0026#39; ].iloc[-2], \u0026#39;relative\u0026#39;: True },domain = {\u0026#39;x\u0026#39;: [0.25, 0.5], \u0026#39;y\u0026#39;: [0.15, .4]}) fig.layout.update(showlegend=False, yaxis = {\u0026#34;title\u0026#34;: {\u0026#34;text\u0026#34;: \u0026#34;Numero de Personas\u0026#34;}}, # Cambiar texto eje y xaxis = {\u0026#34;title\u0026#34;: {\u0026#34;text\u0026#34;: \u0026#34;Fecha\u0026#34;}}) # grabar grafica en chart-studio si se proporciona el api-key if api_key: py.plot(fig, filename = \u0026#39;Colombia_general\u0026#39;, auto_open=False) fig.show() Actualizacion de las Graficas cada 24 Horas Las graficas creadas con plotly son enviadas a chart-studio y cargadas en la pagina web mediante el tag iframe de html. Las gráficas se actualizan cada 24 horas usando Github Actions\nCodigo Fuente Jupyter notebook    Google Colaboratory My binder NBviewver           Refencias Fuentes de datos, visualizaciones y análisis de datos.\n https://github.com/CSSEGISandData/COVID-19 https://www.kaggle.com/imdevskp/covid-19-analysis-viz-prediction-comparisons https://junye0798.com/post/build-a-dashboard-to-track-the-spread-of-coronavirus-using-dash/ https://github.com/Perishleaf/data-visualisation-scripts/tree/master/dash-2019-coronavirus https://medium.com/tomas-pueyo/coronavirus-por-qu%C3%A9-debemos-actuar-ya-93079c61e200 https://github.com/features/actions  ","date":1584482637,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1590789837,"objectID":"46e5195e6508837960f45a896c05991e","permalink":"https://joserzapata.github.io/post/covid19-visualizacion/","publishdate":"2020-03-17T17:03:57-05:00","relpermalink":"/post/covid19-visualizacion/","section":"post","summary":"Visualizaciones con Python y Plotly de los datos mundiales del corona virus COVID19","tags":["Python","Data-Science","Jupyter-notebook"],"title":"Visualizacion Datos Coronavirus (COVID19) Mundial con Plotly","type":"post"},{"authors":["Jose R. Zapata"],"categories":["Data-Science"],"content":"PySpark en Google Colab Automatico  Instalacion Marzo/2020 Intalacion Automatica  Instalacion Java Instalacion de Spark Ejemplo de Uso de pyspark     \nInstalacion Rapida Marzo/ 2020 De forma General para usar pyspark en Colab Marzo/2020 seria con los siguientes comandos en una celda en Colab:\nInstalar Java\n!apt-get install openjdk-8-jdk-headless -qq \u0026gt; /dev/null import os # libreria de manejo del sistema operativo os.system(\u0026#34;wget -q https://www-us.apache.org/dist/spark/spark-2.4.5/spark-2.4.5-bin-hadoop2.7.tgz\u0026#34;) os.system(\u0026#34;tar xf /spark-2.4.5-bin-hadoop2.7.tgz\u0026#34;) instalar pyspark\n!pip install -q pyspark # Variables de Entorno os.environ[\u0026#34;JAVA_HOME\u0026#34;] = \u0026#34;/usr/lib/jvm/java-8-openjdk-amd64\u0026#34; os.environ[\u0026#34;SPARK_HOME\u0026#34;] = f\u0026#34;/content/{ver_spark}-bin-hadoop2.7\u0026#34; # Cargar Pyspark from pyspark.sql import SparkSession spark = SparkSession.builder.appName(\u0026#34;Test_spark\u0026#34;).master(\u0026#34;local[*]\u0026#34;).getOrCreate() spark Pero cuando salga una nueva version de spark sera necesario actualizar los links de descarga, ya que siempre borran las versiones 2.x.x cuando sale una nueva.\nLo mejor es configurar automaticamente para que descargue la version que sea mayor que 2.3.4 que es la anterior y menor que spark 3.0.0 que aun se encuentra en desarrollo\nPara esto el siguiente codigo detecta la version actual de spark, la descarga, la descomprime y luego realiza la instalacion de spark en google colab.\nInstalacion Automatica Instalacion de Java Google Colaboratory funciona en un ambiente linux, por lo tanto se pueden usar comandos shell de linux antecedidos del caracter \u0026lsquo;!\u0026rsquo;\n!apt-get install openjdk-8-jdk-headless -qq \u0026gt; /dev/null Instalacion de Spark Obtener automaticamente la ultima version de spark de\nfrom bs4 import BeautifulSoup import requests #Obtener las versiones de spark la pagina web url = \u0026#39;https://downloads.apache.org/spark/\u0026#39; r = requests.get(url) html_doc = r.text soup = BeautifulSoup(html_doc) # leer la pagina web y obtener las versiones de spark disponibles link_files = [] for link in soup.find_all(\u0026#39;a\u0026#39;): link_files.append(link.get(\u0026#39;href\u0026#39;)) spark_link = [x for x in link_files if \u0026#39;spark\u0026#39; in x] print(spark_link) [\u0026lsquo;spark-2.3.4/\u0026rsquo;, \u0026lsquo;spark-2.4.5/\u0026rsquo;, \u0026lsquo;spark-3.0.0-preview2/']\nLa version a usar seran las superiores a spark-2.3.4 y menores a spark-3.0.0\nobtener la version y eliminar el caracter \u0026lsquo;/\u0026rsquo; del final\nver_spark = spark_link[1][:-1] # obtener la version y eliminar el caracter \u0026#39;/\u0026#39; del final print(ver_spark) spark-2.4.5  import os # libreria de manejo del sistema operativo #instalar automaticamente la version deseadda de spark link = \u0026#34;https://www-us.apache.org/dist/spark/\u0026#34; os.system(f\u0026#34;wget -q {link}{ver_spark}/{ver_spark}-bin-hadoop2.7.tgz\u0026#34;) os.system(f\u0026#34;tar xf {ver_spark}-bin-hadoop2.7.tgz\u0026#34;) # instalar pyspark !pip install -q pyspark |████████████████████████████████| 217.8MB 63kB/s |████████████████████████████████| 204kB 53.8MB/s Building wheel for pyspark (setup.py) ... done  Definir variables de entorno os.environ[\u0026quot;JAVA_HOME\u0026quot;] = \u0026quot;/usr/lib/jvm/java-8-openjdk-amd64\u0026quot; os.environ[\u0026quot;SPARK_HOME\u0026quot;] = f\u0026quot;/content/{ver_spark}-bin-hadoop2.7\u0026quot; Cargar pyspark en el sistema from pyspark.sql import SparkSession spark = SparkSession.builder.appName(\u0026quot;Test_spark\u0026quot;).master(\u0026quot;local[*]\u0026quot;).getOrCreate() spark  SparkSession - in-memory\n SparkContext\nSpark UI\n Version v2.4.5 Master local[*] AppName pyspark-shell    Ejemplo de Uso de pyspark Leer archivo de prueba\narchivo = './sample_data/california_housing_train.csv' df_spark = spark.read.csv(archivo, inferSchema=True, header=True) # imprimir tipo de archivo print(type(df_spark)) \u0026lt;class 'pyspark.sql.dataframe.DataFrame'\u0026gt;  ¿Numero de registros en el dataframe?\ndf_spark.count() 17000  Estructura del dataframe\ndf_spark.printSchema() root |-- longitude: double (nullable = true) |-- latitude: double (nullable = true) |-- housing_median_age: double (nullable = true) |-- total_rooms: double (nullable = true) |-- total_bedrooms: double (nullable = true) |-- population: double (nullable = true) |-- households: double (nullable = true) |-- median_income: double (nullable = true) |-- median_house_value: double (nullable = true)  ¿Nombre de las Columnas de dataframe?\ndf_spark.columns ['longitude', 'latitude', 'housing_median_age', 'total_rooms', 'total_bedrooms', 'population', 'households', 'median_income', 'median_house_value']  Ver los primeros 20 registros del dataframe\ndf_spark.show() +---------+--------+------------------+-----------+--------------+----------+----------+-------------+------------------+ |longitude|latitude|housing_median_age|total_rooms|total_bedrooms|population|households|median_income|median_house_value| +---------+--------+------------------+-----------+--------------+----------+----------+-------------+------------------+ | -114.31| 34.19| 15.0| 5612.0| 1283.0| 1015.0| 472.0| 1.4936| 66900.0| | -114.47| 34.4| 19.0| 7650.0| 1901.0| 1129.0| 463.0| 1.82| 80100.0| | -114.56| 33.69| 17.0| 720.0| 174.0| 333.0| 117.0| 1.6509| 85700.0| | -114.57| 33.64| 14.0| 1501.0| 337.0| 515.0| 226.0| 3.1917| 73400.0| | -114.57| 33.57| 20.0| 1454.0| 326.0| 624.0| 262.0| 1.925| 65500.0| | -114.58| 33.63| 29.0| 1387.0| 236.0| 671.0| 239.0| 3.3438| 74000.0| | -114.58| 33.61| 25.0| 2907.0| 680.0| 1841.0| 633.0| 2.6768| 82400.0| | -114.59| 34.83| 41.0| 812.0| 168.0| 375.0| 158.0| 1.7083| 48500.0| | -114.59| 33.61| 34.0| 4789.0| 1175.0| 3134.0| 1056.0| 2.1782| 58400.0| | -114.6| 34.83| 46.0| 1497.0| 309.0| 787.0| 271.0| 2.1908| 48100.0| | -114.6| 33.62| 16.0| 3741.0| 801.0| 2434.0| 824.0| 2.6797| 86500.0| | -114.6| 33.6| 21.0| 1988.0| 483.0| 1182.0| 437.0| 1.625| 62000.0| | -114.61| 34.84| 48.0| 1291.0| 248.0| 580.0| 211.0| 2.1571| 48600.0| | -114.61| 34.83| 31.0| 2478.0| 464.0| 1346.0| 479.0| 3.212| 70400.0| | -114.63| 32.76| 15.0| 1448.0| 378.0| 949.0| 300.0| 0.8585| 45000.0| | -114.65| 34.89| 17.0| 2556.0| 587.0| 1005.0| 401.0| 1.6991| 69100.0| | -114.65| 33.6| 28.0| 1678.0| 322.0| 666.0| 256.0| 2.9653| 94900.0| | -114.65| 32.79| 21.0| 44.0| 33.0| 64.0| 27.0| 0.8571| 25000.0| | -114.66| 32.74| 17.0| 1388.0| 386.0| 775.0| 320.0| 1.2049| 44000.0| | -114.67| 33.92| 17.0| 97.0| 24.0| 29.0| 15.0| 1.2656| 27500.0| +---------+--------+------------------+-----------+--------------+----------+----------+-------------+------------------+ only showing top 20 rows  Descricipcion Estadistica del dataframe df_spark.describe().toPandas().transpose()     0 1 2 3 4     summary count mean stddev min max   longitude 17000 -119.56210823529375 2.0051664084260357 -124.35 -114.31   latitude 17000 35.6252247058827 2.1373397946570867 32.54 41.95   housing_median_age 17000 28.58935294117647 12.586936981660406 1.0 52.0   total_rooms 17000 2643.664411764706 2179.947071452777 2.0 37937.0   total_bedrooms 17000 539.4108235294118 421.4994515798648 1.0 6445.0   population 17000 1429.5739411764705 1147.852959159527 3.0 35682.0   households 17000 501.2219411764706 384.5208408559016 1.0 6082.0   median_income 17000 3.883578100000021 1.9081565183791036 0.4999 15.0001   median_house_value 17000 207300.91235294117 115983.76438720895 14999.0 500001.0    Descripcion estadistica de una sola columna (\u0026lsquo;median_house_value\u0026rsquo;)\ndf_spark.describe([\u0026#39;median_house_value\u0026#39;]).show() +-------+------------------+ |summary|median_house_value| +-------+------------------+ | count| 17000| | mean|207300.91235294117| | stddev|115983.76438720895| | min| 14999.0| | max| 500001.0| +-------+------------------+  De esta forma se puede instalar automaticamente spark en google colab y hacer uno de el de forma gratis.\nEn la version gratis solo se cuenta con una CPU si se quiere aumentar la capacidad de procesamiento es necesario pagar.\n","date":1583812843,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1583812843,"objectID":"0ab34644955db890c3be41e87603ffaf","permalink":"https://joserzapata.github.io/post/pyspark-google-colab/","publishdate":"2020-03-09T23:00:43-05:00","relpermalink":"/post/pyspark-google-colab/","section":"post","summary":"Configuracion de Google Colab para usar pyspark","tags":["Python","Pyspark","Colab","Data-Science","Jupyter-notebook"],"title":"Pyspark con Google Colab","type":"post"},{"authors":["Jose R. Zapata"],"categories":[],"content":"","date":1581718199,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1581718199,"objectID":"021b3cc1a3a13daee2d277dd616cb2f2","permalink":"https://joserzapata.github.io/project/social-media-behaviour/","publishdate":"2020-02-14T17:09:59-05:00","relpermalink":"/project/social-media-behaviour/","section":"project","summary":"Research Project that combines exclusive Facebook data (Condor Dataset, Crowdtangle, Ad’s) and public data to analyze the Social Media behavior to determine if there is coordinated non-authentic behavior.\nThis project in founded by the Social Media and Democracy Research Grants from the Social Science Research Council and access to Facebook data via Social Science One. ","tags":["Data-Science","Python"],"title":"Social Media Behaviour with exclusive Facebook data","type":"project"},{"authors":["Jose R. Zapata"],"categories":null,"content":"","date":1581096900,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1581096900,"objectID":"314df99d0ecc97f9bfb77f82b143b541","permalink":"https://joserzapata.github.io/talk/pycon2020/","publishdate":"2020-02-07T21:12:33-05:00","relpermalink":"/talk/pycon2020/","section":"talk","summary":"Python para demostrar y visualizar la naturaleza de las ondas del sonido, la relación entre diferentes notas y como se unen para crear la Armonía Musical desde la Física y la Matemática","tags":["Python","Jupyter-notebook","Music","Waves"],"title":"Comprensión de la música con Python, una mirada desde la Física y la Matemática","type":"talk"},{"authors":[],"categories":[],"content":"","date":1563220869,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1563220869,"objectID":"b6545793ef267683c42022143affe8b3","permalink":"https://joserzapata.github.io/project/acmus/","publishdate":"2019-07-15T15:01:09-05:00","relpermalink":"/project/acmus/","section":"project","summary":"Research Project to explore the use of machine learning techniques for computational musicology, digital music archive managment, and music information retrieval.Two main elements are the core of our project: 1. Emphasis on semi-supervised and unsupervised machine learning techniques that minimally rely on the availability of annotated data for a specific task. 2. Traditional Colombian music as the main focus of our study.","tags":["Music-information-retrieval","Musicology","Beat-tracking"],"title":"Acmus","type":"project"},{"authors":["Jose R. Zapata"],"categories":null,"content":"","date":1538521200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1538521200,"objectID":"3da123f049e1b5f93cd320fa2bf4fccb","permalink":"https://joserzapata.github.io/talk/meetup_2018/","publishdate":"2019-11-25T22:07:22-05:00","relpermalink":"/talk/meetup_2018/","section":"talk","summary":"Mediante la extraccion automatica del contenido de las señales de audio y Machine learning se tienen aplicaciones de predicción, clasificación y Agrupamiento, como: Deteccion de emociones en la voz (Speech Emotion), Reconocieminto de Voz (Speech-to-Text), Deteccion y segmentacion de personas hablando(Speaker Diarization), Music Information Retrieval (Extraccion Automatica de Informacion Musical), entre otros.","tags":["Python","Music-information-retrieval","Machine-learning","Data-science"],"title":"Aplicaciones de Audio con Machine Learning","type":"talk"},{"authors":["Jose R. Zapata"],"categories":["Audio"],"content":" Video tutoriales sobre el uso de Reaper para produccion de audio, los videos fueron creados como apoyo para el curso de Audio del programa Ingenieria en Diseño de entrentenimiento digital de la Universidad Pontificia Bolivariana.\nDescargar REAPER para Windows, macOS o Linux.\nTable of Contents  1. Introduccion a Reaper y Edicion 2. Manejo de canales y envios con Reaper 3 Automatizaciones y Envolventes en Reaper 4. Mezcla y Masterizacion en Reaper  \nPlaylist del tutorial en youtube\n1. Introduccion a Reaper y Edicion Introduccion a la configuración Manejo de REAPER para produccion de audio.\n  Audios para Insertar\nImagen para Insertar\nVideo para Insertar\n2. Manejo de canales y envios con Reaper   Ejemplo sesión completa de Reaper\n3 Automatizaciones y Envolventes en Reaper Automatización de Volumen, Panning y Efectos mediante envolventes en Reaper\n  Audios Practica de Automatización\nProyecto Reaper de Automatización finalizado\n4. Mezcla y Masterizacion en Reaper Principios Basicos de mezcla y masterización de audio en Reaper\n  Audios practica de mezcla y masterización\nSesión final de reaper con mezcla y masterización\n","date":1535811053,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1535811053,"objectID":"42e29105f594b9a5bbd8867d57f79dbb","permalink":"https://joserzapata.github.io/post/produccion-audio-reaper/","publishdate":"2018-09-01T09:10:53-05:00","relpermalink":"/post/produccion-audio-reaper/","section":"post","summary":"Tutorial en Video y material produccion basica de Audio con Reaper","tags":["Audio","Reaper"],"title":"Tutorial de Produccion de Audio con Reaper","type":"post"},{"authors":["Jose R. Zapata"],"categories":["Academia"],"content":" Este es el resumen de algunas ideas concretas que he recopilado tanto en mi experiencia como docente, investigador y director de varios trabajos de final de carrera en pregrado y postgrado, para realizar una buena presentación del anteproyecto y del trabajo de grado.\nTable of Contents  Tips para la Creación de las Diapositivas Exposición Anteproyecto  Diapositivas: Preguntas de Evaluación:  Exposición del Proyecto de grado  Diapositivas: Preguntas de evaluación:   \nTips para la Creación de las Diapositivas  Usar letra Grande Poco Texto, solo escribir lo indispensable e importante Si las gráficas son pequeñas y no se ven (No Usarlas) Si el texto que se va poner es pequeño (Recortarlo hasta que se vea grande o quitarlo) Usar colores de la misma paleta de colores de la plantilla Tu haces la presentación no las diapositivas Los evaluadores deben estar concentrados en el presentador y solo ver las diapositivas cuando el presentador las señala o explica algo que esta en ellas como ayuda a la presentación No leer las diapositivas Ser directo Recuerde el Zen de Python Disfrutar la presentación! (Esta es la mas importante)  Exposición Anteproyecto Se puede usar para cualquier tipo de proyectos de grado, la regla del número de diapositivas (slides) es 1 diapositiva por 1 minuto de tiempo de exposicion.\nTiempo de presentación = 15 Minutos (Entre 15-17 diapositivas)\nDiapositivas:  Titulo, información del estudiante y del director Contexto Problema (un solo parrafo) Justificación Marco Conceptual Marco conceptual Estado del Arte Estado del Arte Marco Legal Objetivo General (un solo párrafo) Objetivo específicos (Poner Productos) Objetivo específicos (Poner Productos) Metodología Cronograma Presupuesto Bibliografia  Preguntas de Evaluación: Responder estas preguntas para tener claro lo que quieren saber los jurados\n Título:  ¿El titulo es claro y coherente con la temática y problema planteados?  Problema:  ¿Se describe claramente el problema y el contexto en el que se presenta? ¿Se describen claramente las causas del problema? ¿Se presentan evidencias válidas suficientes de la existencia del problema y las causas?  Justificación:  ¿Se describen claramente los beneficios y el impacto en la realización del proyecto?  Marco Conceptual:  ¿El marco conceptual reconoce los conceptos afines a las categorías desarrolladas en el anteproyecto? ¿El marco conceptual está respaldado con suficiencia en fuentes bibliográfica y documental?  Estado de arte:  ¿El estado de la cuestión da cuenta del reconocimiento del campo, por lo menos, del ámbito nacional con referencia al contexto internacional? ¿El estado del arte está respaldado con suficiencia en fuentes bibliográfica y documental?  Marco legal:  ¿Se examinan las regulaciones y/o normativas nacionales e internacionales relacionadas con la temática del proyecto?  Objetivo general:  ¿El objetivo posee un verbo en infinitivo y presenta claramente el qué, cómo y para qué se realiza el proyecto? ¿El título del proyecto está claramente relacionado con el objetivo? ¿El objetivo ofrece una solución al problema referenciado en el proyecto?  Objetivos específicos:  ¿Cada objetivo posee un verbo en infinitivo y presenta claramente el qué se desea realizar? ¿Cada uno de los productos de los objetivos específicos aporta siginificativamente al logro del objetivo general y la suma de ellos logra el objetivo general ? ¿Se presentan los objetivos cronológicamente?  Metodología:  ¿Existe una estrecha relación entre los objetivos específicos y las fases de la metodología? ¿Se describen las fases del proyecto y las actividades necesarias para cumplir con los objetivos? ¿Se describen de forma clara y coherente las técnicas, métodos, herramientas e instrumentos a utilizar en el proyecto que le aporten rigor a la realización en cada una de las fases? ¿El(los) producto(s) del proyecto son fruto de un ejercicio de investigación aplicada o caso de estudio?  Cronograma:  ¿El cronograma propuesto es claro, coherente con la metodologia y propone una duración de 6 meses?   Exposición del Proyecto de grado Las diapositivas donde se explica la metodología usada (13-18) en este caso es un ejemplo usando la metodología CRISP usada en ciencia de datos.\nTiempo de presentación = 30 Minutos (Limite Maximo 30 diapositivas)\nDiapositivas: Este es un tentativo de las diapositivas que se pueden realizar, hay algunos puntos que requieren mas diapositivas como el Estudio y comprensión de los datos y análisis de los datos (Usar Tablas con numeros grandes y graficas Grandes)\n Titulo, información del estudiante y del director Contexto Problema (un solo parrafo) Justificación Marco Conceptual Marco conceptual Marco Legal Estado del Arte Estado del Arte Objetivo General (un solo parrafo) Objetivo específicos Objetivo específicos metodología (CRISP, solo poner la gráfica y no gastar mucho tiempo en esto) a. Comprensión de la pregunta o el negocio b. Estudio y comprensión de los datos (¿ Que características tienen los datos?) c. Análisis de los datos y selección de características (Que encontraste de particular en los datos) d. Modelado (¿cuales modelos se utilizaron y por que?) e. Evaluación (¿cual medida de evaluación se tomo y por que?) Resultados Finales (¿Cual modelo se selecciono y cuales son las características mas importantes y por que?) Productos obtenidos (evidencias) Conclusiones Referencias  Preguntas de evaluación: Responder estas preguntas\n Objetivo general:\n ¿Se realizo el objetivo al 100%? ¿Que dificultades se presentaron?   Objetivos específicos:\n ¿Como se realizo cada objetivo especifico y que problemas ocurrieron en el desarrollo? ¿Cuales fueron los productos obtenidos?  Metodología:\n ¿Como se desarrollo cada etapa de la Metodología ? ¿Que problemas surgieron y como se solucionaron ?  Productos:\n Cuales productos se obtuvieron del proyecto  Conclusiones:\n ¿Que se concluye del análisis de los resultados? ¿Que te dejo de enseñanza el desarrollo de este proyecto?   ","date":1533132653,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1533132653,"objectID":"8aee31a1e213859cf9b40ad9af765ce9","permalink":"https://joserzapata.github.io/post/proyecto-grado/","publishdate":"2018-08-01T09:10:53-05:00","relpermalink":"/post/proyecto-grado/","section":"post","summary":"Tips para crear las diapositivas y para realizar la exposición del anteproyecto del trabajo de grado","tags":[],"title":"Tips para la Exposición del Proyecto de Grado","type":"post"},{"authors":["Jose R. Zapata"],"categories":null,"content":"","date":1518194435,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1518194435,"objectID":"765bb230bfa95f670be32a5ef0b37bb1","permalink":"https://joserzapata.github.io/talk/pycon2018/","publishdate":"2019-11-25T17:09:35-05:00","relpermalink":"/talk/pycon2018/","section":"talk","summary":"Using Python for Audio signal analysis and Music Information Retrieval applications.","tags":["Music-information-retrieval","Python","Jupyter-notebook"],"title":"Audio signal analysis with python","type":"talk"},{"authors":["Jose R. Zapata"],"categories":[],"content":"","date":1479424445,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1479424445,"objectID":"e27342eb41f7b23196a6aa49f2f702ba","permalink":"https://joserzapata.github.io/publication/tempo-estimation/","publishdate":"2019-10-19T07:52:23-05:00","relpermalink":"/publication/tempo-estimation/","section":"publication","summary":"","tags":["Tempo"],"title":"Tempo Estimation","type":"publication"},{"authors":["Jose R. Zapata"],"categories":null,"content":"","date":1469214000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1469214000,"objectID":"5c213eaf4ea864ae4a254a56dfa79204","permalink":"https://joserzapata.github.io/talk/aes2016/","publishdate":"2019-11-25T21:42:28-05:00","relpermalink":"/talk/aes2016/","section":"talk","summary":"El Music Information Retrieval (MIR), que tiene como objetivo extraer, analizar y procesar automaticamente la informacion musical en señales de audio para diferentes aplicaciones musicales e informaticas. Las principales tareas en MIR, son Beat tracking, deteccion de melodia, deteccion de acordes, fingerprinting, recomendacion de música, procesamiento de señales de audio, entre otras aplicaciones","tags":["Music-information-retrieval","Python"],"title":"Sistemas Automaticos para Extraccion de Información Musical (Generalidades y Aplicaciones)","type":"talk"},{"authors":["Jose R. Zapata","Matthew E.P. Davies","Emilia Gómez"],"categories":[],"content":"","date":1396543937,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1396543937,"objectID":"3b89439c94c277af830c85d968bae75c","permalink":"https://joserzapata.github.io/publication/multifeature-beattracker/","publishdate":"2019-10-19T07:03:59-05:00","relpermalink":"/publication/multifeature-beattracker/","section":"publication","summary":"","tags":["Beat-tracking","Confidence-measure","IEEE"],"title":"Multi-Feature Beat Tracking","type":"publication"},{"authors":["Dmitry Bogdanov","Nicolas Wack","Emilia Gómez","Sankalp Gulati","Perfecto Herrera","Oscar Mayor","Gerard Roma","Justin Salamon","Jose R. Zapata","Xavier Serra"],"categories":[],"content":"","date":1395378695,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1395378695,"objectID":"9f230c47ee24c206024abe1fff9e2947","permalink":"https://joserzapata.github.io/publication/essentia-sigmm/","publishdate":"2019-10-20T06:49:16-05:00","relpermalink":"/publication/essentia-sigmm/","section":"publication","summary":"","tags":["Essentia","Beat-tracking","Music-information-retrieval","Python","ACM","Open-source"],"title":"Essentia: an open source library for audio analysis","type":"publication"},{"authors":["Jose R. Zapata"],"categories":[],"content":"","date":1388894438,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1388894438,"objectID":"f740bcb76b81f488e70bad46459faf14","permalink":"https://joserzapata.github.io/project/multifeature-beat-tracker/","publishdate":"2014-01-04T23:00:38-05:00","relpermalink":"/project/multifeature-beat-tracker/","section":"project","summary":"Matlab implementation of the Multi Feature Beat Tracker (Information Gain and Regularity), The essentia beat tracker, More details in [Multi-Feature Beat Tracking](https://joserzapata.github.io/publication/multifeaturebeattracker/)","tags":["Beat-tracking","Essentia","Matlab","Mirex","Python"],"title":"Multifeature Beat tracker","type":"project"},{"authors":[],"categories":[],"content":"","date":1385210712,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1385210712,"objectID":"ae28bdb513faa73c235bc5ea7467f50e","permalink":"https://joserzapata.github.io/project/essentia/","publishdate":"2013-11-23T07:45:12-05:00","relpermalink":"/project/essentia/","section":"project","summary":"Open-source C++ library for audio analysis and audio-based music information retrieval. It contains an extensive collection of reusable algorithms which implement audio input/output functionality, standard digital signal processing blocks, statistical characterization of data, and a large set of spectral, temporal, tonal and high-level music descriptors. More details in [Essentia: An Audio Analysis Library for Music Information Retrieval](https://joserzapata.github.io/publication/essentia-ismir)","tags":["Essentia","Beat-tracking","Python","Open-source"],"title":"Essentia","type":"project"},{"authors":["Dmitry Bogdanov","Nicolas Wack","Emilia Gómez","Sankalp Gulati","Perfecto Herrera","Oscar Mayor","Gerard Roma","Justin Salamon","Jose R. Zapata","Xavier Serra"],"categories":[],"content":"","date":1383539573,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1383539573,"objectID":"b2f301b98e01a2b1c2ef517e6e72334f","permalink":"https://joserzapata.github.io/publication/essentia-ismir/","publishdate":"2019-10-19T06:54:49-05:00","relpermalink":"/publication/essentia-ismir/","section":"publication","summary":"","tags":["Essentia","Beat-tracking","Music-information-retrieval","Python","ISMIR","Open-source"],"title":"Essentia: An Audio Analysis Library for Music Information Retrieval","type":"publication"},{"authors":["Dmitry Bogdanov","Nicolas Wack","Emilia Gómez","Sankalp Gulati","Perfecto Herrera","Oscar Mayor","Gerard Roma","Justin Salamon","Jose R. Zapata","Xavier Serra"],"categories":[],"content":"","date":1382332295,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1382332295,"objectID":"266fc3e7075adaa401902b92b429edba","permalink":"https://joserzapata.github.io/publication/essentia-acm/","publishdate":"2019-10-19T06:49:16-05:00","relpermalink":"/publication/essentia-acm/","section":"publication","summary":"","tags":["Essentia","Beat-tracking","Music-information-retrieval","Python","ACM","Open-source"],"title":"Essentia: an open-source library for sound and music analysis","type":"publication"},{"authors":["Jose R. Zapata"],"categories":[],"content":"","date":1379080190,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1379080190,"objectID":"3bbaff791be1d67f99d1269bde41adc9","permalink":"https://joserzapata.github.io/publication/phd-thesis/","publishdate":"2013-09-13T14:49:50+01:00","relpermalink":"/publication/phd-thesis/","section":"publication","summary":"describe a new method for the extraction of beat times with a confidence value from music audio, based on the measurement of mutual agreement between a committee of beat tracking systems.","tags":["Beat-tracking","Tempo","Comparative-evaluation","Phd"],"title":"Comparative evaluation and combination of automatic rhythm description systems","type":"publication"},{"authors":null,"categories":null,"content":"Comparative evaluation and combination of automatic rhythm description systems Zapata, Jose R. (2013). Comparative evaluation and combination of automatic rhythm description systems. Ph.D. thesis (Information and Communication Technologies), Universitat Pompeu Fabra, Barcelona, Spain, 2013.\n  [TDX.cat] [ PDF] [ MTG] [ Slides]  Thesis advisor  Dr. Emilia Gómez, Universitat Pompeu Fabra ( MTG, UPF)  Defense board  Dr. Xavier Serra, Universitat Pompeu Fabra ( MTG, UPF) Dr. Fabien Gouyon, Institute for Systems and Computer Engineering of Porto ( SMC, INESC Porto) Dr. Juan Pablo Bello, New York University ( MARL, NYU)  Abstract The automatic analysis of musical rhythm from audio, and more specifically tempo and beat tracking, is one of the fundamental open research problems in Music Information Retrieval (MIR) research. Automatic beat tracking is a valuable tool for the solution of other MIR problems, because enables the beat-synchronous analysis of music for tasks such as: structural segmentation, chord detection, music similarity, cover song detection, automatic remixing and interactive music systems. Even though automatic rhythm description is a relatively mature research topic in MIR and various algorithms have been proposed, tempo estimation and beat tracking remain an unsolved problem.\nRecent comparative studies of automatic rhythm description systems suggest that there has been little improvement in the state of the art over the last few years. In this thesis, we describe a new method for the extraction of beat times with a confidence value from music audio, based on the measurement of mutual agreement between a committee of beat tracking systems. Additionally, we present an open source approach which only requires a single beat tracking model and uses multiple onset detection functions for the mutual agreement. The method can also be used to identify music samples that are challenging for beat tracking without the need for ground truth annotations. Using the proposed method, we compiled a new dataset that consist of pieces that are difficult for state-of-the-art beat tracking algorithms. Through an international evaluation framework we show that our method yields the highest AMLc and AMLt accuracies obtained in this evaluation to date. Moreover, we compare our method to 20 reference systems using the largest existing annotated dataset for beat tracking and show that it outperforms all of the other systems under all the evaluation criteria used.\nIn the thesis we also conduct an extensive comparative evaluation and combination of automatic rhythm description systems. We evaluated 32 tempo estimation and 16 beat tracking state-of-the-art systems in order to identify their characteristics and investigated how they can be combined to improve performance. Finally, we proposed and evaluated the use of voice suppression algorithms in music signals with predominant vocals in order to improve the performance of existing beat tracking methods.\n","date":1379030400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1379030400,"objectID":"a5b0a930c95b729922bbf991c678c31e","permalink":"https://joserzapata.github.io/phd/","publishdate":"2013-09-13T00:00:00Z","relpermalink":"/phd/","section":"","summary":"Comparative evaluation and combination of automatic rhythm description systems","tags":null,"title":"Phd Tesis","type":"page"},{"authors":["Jose R. Zapata","Emilia Gómez"],"categories":[],"content":"","date":1369590165,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1369590165,"objectID":"2c227655fdc88a1d4b7bb84bd11157dc","permalink":"https://joserzapata.github.io/publication/voice-suppression-improve-beat-tracking/","publishdate":"2019-10-19T08:04:57-05:00","relpermalink":"/publication/voice-suppression-improve-beat-tracking/","section":"publication","summary":"","tags":["Beat-tracking","Source-separation","ICASSP","IEEE"],"title":"Using voice suppression algorithms to improve beat tracking in the presence of highly predominant vocals","type":"publication"},{"authors":["Andre Holzapfel","Matthew EP Davies","Jose R. Zapata","João Lobato Oliveira","Fabien Gouyon"],"categories":[],"content":"","date":1351962965,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1351962965,"objectID":"e70c170c3be55d89c3d164fb571decb6","permalink":"https://joserzapata.github.io/publication/selective-sampling-beat-tracking/","publishdate":"2019-10-19T07:16:58-05:00","relpermalink":"/publication/selective-sampling-beat-tracking/","section":"publication","summary":"","tags":["Beat-tracking","Selective-sampling","IEEE"],"title":"Selective Sampling for Beat Tracking Evaluation","type":"publication"},{"authors":["Jose R. Zapata","Matthew EP Davies","Andre Holzapfel","João Lobato Oliveira","Fabien Gouyon"],"categories":[],"content":"","date":1349743136,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1349743136,"objectID":"d1994bb3c7f67f1de178f42e0396ed2d","permalink":"https://joserzapata.github.io/publication/assigning-confidence/","publishdate":"2019-10-19T06:04:58-05:00","relpermalink":"/publication/assigning-confidence/","section":"publication","summary":"","tags":["Beat-tracking","Confidence-measure","ISMIR"],"title":"Assigning a confidence threshold on automatic beat annotation in large datasets","type":"publication"},{"authors":["Jose R. Zapata","Emilia Gómez"],"categories":[],"content":"","date":1340939862,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1340939862,"objectID":"8dbfa88e363fc9f2218f042feb3f4b47","permalink":"https://joserzapata.github.io/publication/improving-beat-tracking/","publishdate":"2019-10-19T06:58:59-05:00","relpermalink":"/publication/improving-beat-tracking/","section":"publication","summary":"","tags":["Beat-tracking","Source-separation","CMMR"],"title":"Improving Beat Tracking in the presence of highly predominant vocals using source separation techniques: Preliminary study","type":"publication"},{"authors":[],"categories":[],"content":"","date":1338143413,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1338143413,"objectID":"fc5dbad770fbd8b3b7e8be1d0746d834","permalink":"https://joserzapata.github.io/project/smc-beat-tracker-dataset/","publishdate":"2012-05-27T13:30:13-05:00","relpermalink":"/project/smc-beat-tracker-dataset/","section":"project","summary":"This beat tracking dataset contains 217 excerpts around 40s each, of which 19 are *easy* and the remaining 198 are *hard*. This dataset has been designed for radically new techniques which can contend with challenging beat tracking situations like: quiet accompaniment, expressive timing, changes in time signature, slow tempo, poor sound quality etc. More details in [Selective Sampling for Beat Tracking Evaluation](https://joserzapata.github.io/publication/selectivesampling/)","tags":["Beat-tracking","Dataset","Mirex","Open-source"],"title":"SMC Beat tracking Dataset","type":"project"},{"authors":["Andre Holzapfel","Matthew EP Davies","Jose R. Zapata","João Lobato Oliveira","Fabien Gouyon"],"categories":[],"content":"","date":1332722673,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1332722673,"objectID":"09b7f7f1c4f9e866e31a23eb77383ce8","permalink":"https://joserzapata.github.io/publication/automatic-identification/","publishdate":"2019-10-19T06:21:05-05:00","relpermalink":"/publication/automatic-identification/","section":"publication","summary":"","tags":["Beat-tracking","Selective-sampling","ICASSP","IEEE"],"title":"On the automatic identification of difficult examples for beat tracking: towards building new evaluation datasets","type":"publication"},{"authors":["Jose R. Zapata","Emilia Gómez"],"categories":[],"content":"","date":1311356015,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1311356015,"objectID":"29692cec3e93b1c622f0e22c687464e8","permalink":"https://joserzapata.github.io/publication/comparative-tempo/","publishdate":"2019-10-19T06:25:51-05:00","relpermalink":"/publication/comparative-tempo/","section":"publication","summary":"","tags":["Tempo","Comparative-evaluation","AES","Python"],"title":"Comparative Evaluation and Combination of Audio Tempo Estimation Approaches","type":"publication"},{"authors":["Jose R. Zapata","Ricardo A. Garcia"],"categories":[],"content":"","date":1222918493,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1222918493,"objectID":"bce0a52c36fdd4243279b6e0d92aa98d","permalink":"https://joserzapata.github.io/publication/efficient-detection/","publishdate":"2019-10-19T06:44:00-05:00","relpermalink":"/publication/efficient-detection/","section":"publication","summary":"","tags":["Audio-redundancy","AES"],"title":"Efficient Detection of Exact Redundancies in Audio Signals","type":"publication"},{"authors":["Jose R. Zapata","Tony Peñarredonda"],"categories":[],"content":"","date":1090780824,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1090780824,"objectID":"91b2e9159822a7089832be91b361afb0","permalink":"https://joserzapata.github.io/publication/control-rango-dinamico/","publishdate":"2019-11-14T13:40:24-05:00","relpermalink":"/publication/control-rango-dinamico/","section":"publication","summary":"","tags":["Fuzzy-logic","Dynamic-range-compression"],"title":"Control de Rango Dinamico en Audio con Logica Difusa","type":"publication"}]