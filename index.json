[{"authors":["joserzapata"],"categories":null,"content":"Researcher at GIDATIC and Professor at the Faculty of Information and Communication Technologies (TIC), Universidad Pontificia Bolivariana (UPB).\nMy interests are based around Data science and Audio and Music information technologies, which includes Music information retrieval, Machine Learning, Audio analysis and Data analysis. EN ESPAÑOL\n","date":1601308662,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":1601308662,"objectID":"7378ef29713b8215ab7deca02c74bf8a","permalink":"https://joserzapata.github.io/author/jose-r.-zapata/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/author/jose-r.-zapata/","section":"authors","summary":"Researcher at GIDATIC and Professor at the Faculty of Information and Communication Technologies (TIC), Universidad Pontificia Bolivariana (UPB).\nMy interests are based around Data science and Audio and Music information technologies, which includes Music information retrieval, Machine Learning, Audio analysis and Data analysis.","tags":null,"title":"Jose R. Zapata","type":"authors"},{"authors":null,"categories":null,"content":"\nProposito del Curso Manejo y uso de Python como herramienta para la solución de problemas en Ciencia de Datos tales como:\n Fundamentos de programación con Python Extracción, transformación y carga de datos. Análisis estadístico descriptivo de los datos. Visualización de datos. Modelamiento y evaluación de sistemas de aprendizaje de máquina supervisados y no supervisados.  Temas  1. Python Introduccion al Lenguaje Python\n  2. Numpy Libreria de Algebra Lineal (Vectores y Matrices)\n  3. Pandas Libreria de Manipulación y análisis de datos estructurados\n  4. Visualizacion Librerias de visualizacion (matplotlib, plotly, seaborn)\n  5. Machine Learning Libreria de machine Learning que incluye varios algoritmos de clasificación, regresión y análisis de grupos, ademas de herramientas para seleccion, evaluacion de modelos.\n  6. Regresion Ejemplo Regresion con Python\n  7. Clasificacion Ejemplo Regresion con Python\n  8. Clustering Ejemplo Clustering con Python\n  Docente Jose R. Zapata   Empezar   ","date":1617494400,"expirydate":-62135596800,"kind":"section","lang":"en","lastmod":1617494400,"objectID":"e37dff9b788554e1391380bb2208d94d","permalink":"https://joserzapata.github.io/courses/python-ciencia-datos/","publishdate":"2021-04-04T00:00:00Z","relpermalink":"/courses/python-ciencia-datos/","section":"courses","summary":"Curso para aprender Python como herramienta para los procesos de Ciencia de Datos","tags":null,"title":"Ciencia de Datos con Python","type":"book"},{"authors":null,"categories":null,"content":"Por Jose R. Zapata\nEn este notebook se encontra los temas mas importantes para el uso de python en ciencia de datos, se debe tener un un conocimiento basico de programacion para entender completamente el notebook.\n\nTipos de Datos Python tiene varios tipos de datos, los mismos datos básicos que otros lenguajes: enteros, flotantes, strings y booleanos. Además, las listas son un tipo predefinido en el lenguaje, los diccionarios, tuplas y sets.\nLa funcion que se utiliza para determinar el tipo de datos es type()\nPara escribir comentarios es utiliza el caracter numeral #\nTable of Contents  Tipos de Datos  Numeros type() Asignacion de valores Variables Strings Imprimir con formato Entrada de datos del usuario Listas Diccionarios Booleans (Logicos) Tuples Casting   Operadores de Comparacion y Logicos  Operadores de Comparacion Operadores Logicos   Programacion con Python  Condicionales if,elif, else Ciclos for Ciclos while Control de ciclos break y continue List comprehension   Funciones  Funciones basicas de python len() range() enumerate()   Metodos  Metodos en strings Metodos en listas   Instalar Paquetes o Librerias  Instalacion en Linea de comando Instalacion en Jupyter Notebook   Ejemplos Referencias    Numeros IPython puede ser usado de forma interactiva como una calculadora. Esto permite que el análisis de datos pueda ser realizado de forma interactiva, de forma similar a como pueden usarse otras herramientas como el lenguaje R o Matlab. A continuación se ejemplifican los cálculos aritméticos básicos.\n2 + 3 #Suma  5  2 * 3 #Multiplicacion  6  7 / 2 #Division con respuesta de numero flotante  3.5  5//2 # parte entera de la division  2  2 ** 4 #Potenciacion  16  8 % 2 #Operacion Modulo  0  5 % 2 #Operacion Modulo  1  53 % 15  8  Orden específico para ralizar las operaciones y resolver fórmulas\n Potencias y radicales Multiplicación, división, división sesgada y módulo Suma y resta  ((2 + 3) * (5**2 + 5))/2 #Uso de varias operacion en la misma ecuacion  75.0  type() la funcion para conocer el tipo de dato\n#funcion para determinar el tipo de dato numerico type(3) # tipo entero (int)  int  type(3.0) # Tipo flotante, se determina por el uso del punto  float  type(2+2j)# Numero complejo  complex  type(5/2) # Numero flotante  float  type(5//2) # Numero entero  int  Asignacion de valores Variables Las variables en python no necesitan ser declaradas, simplemente se definen al ser utilizadas por primera vez. Además, (si bien no es recomendable) pueden cambiar de tipo volviendo a definir. la variables puede ser de cualqueire tipo de datos que maneja Python\n# Los nombres NO pueden empezar con un numero o un caracter especial name_of_var = 2 #Asignacion de valores a una variable type(name_of_var)  int  name_of_var = 5.3 #Asignacion de valores a una variable type(name_of_var)  float  x = 2 # asignado valores a diferentes variables y = 3  z = x + y # asignar la suma de dos variables  z  5  type(z) #tipo de dato  int  Strings 'Comillas sencillas'  'Comillas sencillas'  \u0026quot;Comillas dobles\u0026quot;  'Comillas dobles'  \u0026quot; Se puede escribir 'Comillas' de esta forma\u0026quot;  \u0026quot; Se puede escribir 'Comillas' de esta forma\u0026quot;  \u0026quot; Científico de datos: Persona que sabe más de estadística que cualquier programador y que a la vez sabe más de programación que cualquier estadístico \u0026quot;  ' Científico de datos: Persona que sabe más de estadística que cualquier programador y que a la vez sabe más de programación que cualquier estadístico '  \u0026quot;\u0026quot;\u0026quot; Los strings de varias lineas pueden escribirse delimitándolos tres comillas dobles y son usados corrientemente como comentarios \u0026quot;\u0026quot;\u0026quot;  '\\n Los strings de varias lineas pueden\\n escribirse delimitándolos tres comillas \\n dobles y son usados corrientemente como\\n comentarios\\n'  a = \u0026quot;El Big data es como el sexo en la adolescencia: todo el mundo habla de ello, nadie sabe realmente cómo hacerlo, todos piensan que los demás lo están haciendo, así que todos dicen que también lo hacen...\u0026quot; a  'El Big data es como el sexo en la adolescencia: todo el mundo habla de ello, nadie sabe realmente cómo hacerlo, todos piensan que los demás lo están haciendo, así que todos dicen que también lo hacen...'  type(a) # tipo de datos  str  # duplicar varias veces un string \u0026quot;Ciencia de Datos \u0026quot;*4  'Ciencia de Datos Ciencia de Datos Ciencia de Datos Ciencia de Datos '  # Concatenar strings palabra = \u0026quot;Amo \u0026quot;+\u0026quot;Programar \u0026quot;+\u0026quot;En Python\u0026quot; palabra  'Amo Programar En Python'  Imprimir con formato x = 'hello' # asignar varible tipo string  x #imprimir la variable en jupyter  'hello'  print(x) #impresion de una variable en python  hello  num = 3 # asignacion de variables name = 'Matias'  # Impresion con formato, podemos omitir la posicion si las variables estan en orden print('Me llamo: {}, y mi edad es: {}'.format(name,num))  Me llamo: Matias, y mi edad es: 3  # Nuevo desde python 3.6 #f-string o string literal format print(f'Me llamo: {name}, y mi edad es: {num}')  Me llamo: Matias, y mi edad es: 3  Entrada de datos del usuario #entrada = input('Como te llamas? ') # los datos sera tipo string #Simular entrada de texto entrada = 'Jose R. Zapata'  type(entrada) #tipo de dato  str  print(f'Hola {entrada} te deseo lo mejor aprendiendo python para analisis de datos')  Hola Jose R. Zapata te deseo lo mejor aprendiendo python para analisis de datos  Listas Las listas son una de las principales estructuras para almacenar información en Python.\n En las listas podemos ingresar todo tipos de datos. Se utilizan los corchetes [ ] para definir una lista y se separan usando coma. Las listas son elementos Mutables (se pueden cambiar sus valores)  # definicion de listas uno = [1,2,3] # las listas se crean delimitando sus elementos entre [ y ] y usando la , como separacion uno  [1, 2, 3]  type(uno) #tipo de datos  list  #Una lista puede tener diferentes tipos de datos ['hi',1,[1,2,3]] #Esta lista tiene uns string, un numero y una lista como sus elementos  ['hi', 1, [1, 2, 3]]  my_list = ['a','b','c'] #creacion de una lista  my_list  ['a', 'b', 'c']  Indexacion en Listas y Strings En Python, las listas son vectores; el primer elemento ocupa la posición 0, el segundo la 1 y así sucesivamente. Los índices negativos (iniciando en -1) se usan para indicar la posición contando desde atrás. Por ejemplo:\n +---+---+---+---+---+---+ | P | y | t | h | o | n | +---+---+---+---+---+---+ 0 1 2 3 4 5 -6 -5 -4 -3 -2 -1  NOTA IMPORTANTE: En python la numeracion de las posiciones empieza en cero 0\nPara acceder a algun elemento de la lista, se utilizan corchetes, Ejemplo\nlista_vble = [45,23,38,2,8,17] #creacion de una lista lista_vble[2] # accediendo al valor que esta en la posicion 3 \u0026gt;\u0026gt; 38  Los strings tienen un comportamiento similar a las List, pero los strings son variables inmutables, mientras que las listas si son mutables.\n# La indexacion y slicing de litas y strings es igual # La primera posicion es en 0 word = 'Python' word[0] # Caracter en la posición 0  'P'  word[5] # Caracter en la posición 5  'n'  word[-1] # Último caracter  'n'  word[-2] # Antepenúltimo caracter  'o'  word[-6] # Primer caracter en este ejemplo  'P'  Seleccion de varios elementos (Slicing) El operador : se usa para indicar rangos, [inicio:final-1], no se incluye el final\n +---+---+---+---+---+---+ | P | y | t | h | o | n | +---+---+---+---+---+---+ 0 1 2 3 4 5 -6 -5 -4 -3 -2 -1  word[0:2] # El operador `:` se usa para indicar rangos, [inicio:final-1], no se incluye el final  'Py'  word[:2] # ':2' indica desde el principio hasta la posición 2 (sin incluirla)  'Py'  word[2:5] # [inicio:final-1], no se incluye el final  'tho'  word[2:] # Desde la posición 2 hasta el final  'thon'  word[1:6:2] # [inicio:final-1:paso], Ej inicio = 1, final-1 = 5, paso cada 2 elementos  'yhn'  word[::2]  'Pto'  word[:2] + word[2:] # Concatenacion de strings  'Python'  word[:4] + word[2:] # Concatenacion de strings  'Pyththon'  word[-2:] # desde la posición -2 hasta el final, o sea, los últimos dos caracteres.  'on'  word[:] # desde el primero hasta el último caracter.  'Python'  # Asignaciones a una posicion especifica en las listas # Se podra hacer esta asignacion en los strings? #word[2] = 'e'  las listas son estructuras de datos mutables, entonces se pueden reescribir.\nLos strings NO son mutables\n# Las Listas son Variables Mutables H = [1, 2, 3, 4, 5, 6, 7, 8, 9, 10] P = H # copiar todo el contenido de Ha a P P[0] = 333 print(\u0026quot;Observar el que ocurre con 'H' cuando se cambia el valor de 'P', si P = H\u0026quot;) print(f'P = {P}') # uso de f-strings para imprimir los resultados print(f'H = {H}') # uso de f-strings para imprimir los resultados  Observar el que ocurre con 'H' cuando se cambia el valor de 'P', si P = H P = [333, 2, 3, 4, 5, 6, 7, 8, 9, 10] H = [333, 2, 3, 4, 5, 6, 7, 8, 9, 10]  id(P)  140079831399488  id(H)  140079831399488  id(word)  140079928559408  Las listas no se copian al asignarlas a otra variable, ¡es un puntero a la lista original! ¡Esto evita problemas de memoria! pero puede llevar a errores en la programacion, entonces la copia se debe hacer explicita.\n# Las Listas son Variables Mutables H = [1, 2, 3, 4, 5, 6, 7, 8, 9, 10] P = H[:] # copiar todo el contenido de H a P P[0] = 333 print(\u0026quot;Observar el que ocurre con 'H' cuando se cambia el valor de 'P', si P = H[:]\u0026quot;) print(f\u0026quot;P = {P}\u0026quot;) # uso de f-strings para imprimir los resultados print(f\u0026quot;H = {H}\u0026quot;) # uso de f-strings para imprimir los resultados  Observar el que ocurre con 'H' cuando se cambia el valor de 'P', si P = H[:] P = [333, 2, 3, 4, 5, 6, 7, 8, 9, 10] H = [1, 2, 3, 4, 5, 6, 7, 8, 9, 10]  id(P)  140079822691328  id(H)  140079822797184  Indexacion anidada nest = [1,2,3,[4,5,['target']]] # Lista con otra lista al interior nest  [1, 2, 3, [4, 5, ['target']]]  nest[3] # Cuarto Elemento de la lista, lo que se retorna es una lista  [4, 5, ['target']]  nest[3][2] # De la lista que es el elemento 4, obtener el elemento 3 # el tipo de resultado es una lista  ['target']  # obtener el string nest[3][2][0] # De la lista que es el elemento 4, obtener el elemento 3, y obtener el valor de esa lista  'target'  nest[3][2][0][:2] # De la lista que es el elemento 4, obtener el elemento 3, y obtener el valor de esa lista  'ta'  # obtener otro elemento de la lista nest[3][:2][1]  5  Agregar datos en una lista Agregar elementos a una lista se hace con los metodos: .insert() .append()\n.insert() list.insert(i, x): Inserta un ítem x en una posición i, los otros elementos despues de la posicion se mueven hacia la derecha.\nnest = [1,2,3,[4,5,['target']]] # Lista con otra lista al interior nest  [1, 2, 3, [4, 5, ['target']]]  nest.insert(2,'Amo Python') nest  [1, 2, 'Amo Python', 3, [4, 5, ['target']]]  .append() Agrega los elementos al final de la lista\n# se agrega un dato al final de la lista utilizando el metodo append() nest[4][2].append(8) nest  [1, 2, 'Amo Python', 3, [4, 5, ['target', 8]]]  nest[4].append('Pandas') nest  [1, 2, 'Amo Python', 3, [4, 5, ['target', 8], 'Pandas']]  Asignar valores a una posición nest[2] = 'Casa' # reasignar el valor a una posicion de una lista nest  [1, 2, 'Casa', 3, [4, 5, ['target', 8], 'Pandas']]  len(nest) #obtener el tamaño de la lista  5  Borrar Elementos de una lista nest[1:3] = [] # Borrar elementos de una lista nest  [1, 3, [4, 5, ['target', 8], 'Pandas']]  # Borrar elementos de una lista conociendo su index del(nest[2][2][0]) nest # observar que el valor esta dentro de una lista  [1, 3, [4, 5, [8], 'Pandas']]  Concatenar listas print(nest) print(my_list)  [1, 3, [4, 5, [8], 'Pandas']] ['a', 'b', 'c']  # Concatenar listas new_list = nest + my_list new_list  [1, 3, [4, 5, [8], 'Pandas'], 'a', 'b', 'c']  # agregar una lista al final nest.append(my_list) nest  [1, 3, [4, 5, [8], 'Pandas'], ['a', 'b', 'c']]  # Repetir varias veces una lista repetida = my_list*4 repetida  ['a', 'b', 'c', 'a', 'b', 'c', 'a', 'b', 'c', 'a', 'b', 'c']  new_list  [1, 3, [4, 5, [8], 'Pandas'], 'a', 'b', 'c']  Diccionarios Se utilizan las llaves { } para su definicion, Un Diccionario es una estructura de datos con características especiales que nos permite almacenar cualquier tipo de valor como numeros, strings, listas e incluso otras funciones. Estos diccionarios nos permiten además identificar cada elemento por una clave (Key). Las claves son únicas dentro de un diccionario, es decir que no puede haber un diccionario que tenga dos veces la misma clave, si se asigna un valor a una clave ya existente, se reemplaza el valor anterior.\nNota: El algoritmo que usa Python internamente para buscar un elemento en un diccionario es muy distinto que el que utiliza para buscar en listas. los elementos en los diccionarios no tienen un orden determinado.\nPara buscar en las listas, se utiliza un algoritmos de comparación que tarda cada vez más a medida que la lista se hace más larga. En cambio, para buscar en diccionarios se utiliza un algoritmo llamado hash, que se basa en realizar un cálculo numérico sobre la clave del elemento, y tiene una propiedad muy interesante: sin importar cuántos elementos tenga el diccionario, el tiempo de búsqueda es siempre aproximadamente igual.\nEste algoritmo de hash es también la razón por la cual las claves de los diccionarios deben ser inmutables, ya que la operación hecha sobre las claves debe dar siempre el mismo resultado, y si se utilizara una variable mutable esto no sería posible.\n# Crear un diccionario d = {'key1':'item1','key2':'item2'}  d  {'key1': 'item1', 'key2': 'item2'}  type(d)  dict  d['key1'] #acceder al elemento que tiene la clave `key1`  'item1'  # Un ejemplo de uso de los diccionarios pepito = {'edad':20,'nombre':'Pepito','Apellido':'Perez','estatura':1.77} pepito  {'edad': 20, 'nombre': 'Pepito', 'Apellido': 'Perez', 'estatura': 1.77}  print(pepito['nombre']) print(pepito['Apellido']) #Observar que esta clave empieza con Mayuscula, Python es case sensitive print(pepito['edad']) print(pepito['estatura'])  Pepito Perez 20 1.77  print (f\u0026quot;El nombre del paciente es {pepito['nombre']} {pepito['Apellido']}, tiene {pepito['edad']} años y una estatura de {pepito['estatura']} \u0026quot;)  El nombre del paciente es Pepito Perez, tiene 20 años y una estatura de 1.77  pepito.keys()  dict_keys(['edad', 'nombre', 'Apellido', 'estatura'])  # Que ocurre si quiero obtener todos los valores del diccionario? # pepito[:]  Booleans (Logicos) # las palabras True y False son palabras restringidas # valor Verdadero True  True  # valor Falso False  False  v1 = True v2 = False print(f\u0026quot;Valor de v1 = {v1} y v2 = {v2}\u0026quot;)  Valor de v1 = True y v2 = False  type(v1) # tipo de dato  bool  Tuples Las tuplas son como las listas (se indexan de la misma forma), pero al igual que los strings no se pueden modificar (son inmutables). Son como unas listas de sólo lectura. Se crean con () (paréntesis) en lugar de [ ] (corchetes). Una vez se crea una tupla, no se puede cambiar ni su contenido ni su tamaño, a menos que hagas una copia de la tupla.\nfecha = (25, \u0026quot;Mayo\u0026quot;, 1810) fecha  (25, 'Mayo', 1810)  type(fecha)  tuple  fecha[0]  25  # Se puede hacer? #fecha[1] = 'Julio'  Casting Cambio entre tipos de datos\n#Conversion de float a entero int(5.789)  5  #Conversion de entero a float float(3)  3.0  # conversion de numero a string palabra = str(78345.345) print(type(palabra)) palabra  \u0026lt;class 'str'\u0026gt; '78345.345'  # conversion de string a numero, si el string es un numero numero = int('3456') # recordar que los strings se escriben entre comillas simples o dobles print(type(numero)) numero  \u0026lt;class 'int'\u0026gt; 3456  #conversion de lista a tuple tup = ('Python','Pandas',3.0,2018) lista = list(tup) type(lista)  list  # conversion de tuple a lista lista2 = [1,2,3,4,5,6,7,8] tup2 = tuple(lista2) type(tup2)  tuple  # Conversion de un numero a Boolean # Cualquier numero diferente de cero sera True print(bool(4)) print(bool(1.65)) print(bool(-3)) print(bool(-5678))  True True True True  # La conversion ser False si el numero es cero print(bool(0))  False  # Conversion de Boolean a Numerico int(True)  1  # Conversion de Boolean a Numerico int(False)  0   Operadores de Comparacion y Logicos Operadores de Comparacion Python tiene los típicos operadores de comparación:\n   Operador Funcion     == Igual que   != Diferente a   \u0026lt; Menor que     | Mayor que \u0026lt;= | Menor o igual que = | Mayor o igual que\n Sus respuestas son del tipo Boolean\n1 \u0026gt; 2 # Mayor que  False  1 \u0026lt; 2 # Menor que  True  1 \u0026gt;= 1 # Mayor o igual que  True  1 \u0026lt;= 4 # Menor o igual que  True  1 == 1 # Igual igual  True  2 != 4 # Diferente  True  'Hola' == 'Adios' #Comparacion de igualdad  False  'Hola' != 'Adios' # Comparacion de diferencia  True  'Hola' == 'hola' # Comparacion de igualdad  False  Operadores Logicos Python implementa todos los operadores usuales de la lógica booleana.\nSe usan las palabras en inglés and, or, not en lugar de símbolos (||, \u0026amp;\u0026amp;, !, etc)\n(1 \u0026gt; 2) and (2 \u0026lt; 3) # Operador and  False  (1 \u0026gt; 2) or (2 \u0026lt; 3) # operador or  True  not(2 \u0026lt; 3) or (1 \u0026gt; 2) # operador not  False  (1 == 2) or (2 == 3) or (4 == 4)  True   Programacion con Python Condicionales if,elif, else En Python no hay llaves { } ni begin...end para marcar el comienzo y fin de un bloque, sino que eso se logra con la indentación. La indentación por defecto son 4 espacios en blanco.\nEntonces va a ser necesario indentar correctamente para utilizar sentencias if, for o para definir funciones.\nEl if es como el de otros lenguajes, pero no pide paréntesis y termina con : Su sintaxis es:\nif condicion : #se finaliza con dos puntos : codigo a realizar#(indentado con 4 espacios) elif condicion: codigo a realizar #(indentando con 4 espacios) elif condicion: codigo a realizar #(indentando con 4 espacios) else: codigo a realizar #(indentado con 4 espacios)  if 1 \u0026lt; 2: print('Viva la Musica!')  Viva la Musica!  if 1 \u0026gt; 2: print('Viva la Musica!')  if 1 \u0026lt; 2: print('Primero') else: print('Ultimo')  Primero  if 1 \u0026gt; 2: print('Primero') else: print('Ultimo')  Ultimo  if 1 == 2: print('Primero') elif 3 == 3: print('Mitad') else: print('Ultimo')  Mitad  Ciclos for Los for son parecidos a los if, pero tienen la sintaxis\nfor variable in lista:. En este caso, variable es la variable que va a ir cambiando, y lista es una lista de python (o un iterable que es parecido)\nseq = [1,2,3,4,5] #lista de valores seq  [1, 2, 3, 4, 5]  for item in seq: # en este ciclo item tomara cada uno de los valores que esta en seq uno por uno print(item)  1 2 3 4 5  for item in seq: print('Viva la Musica!')  Viva la Musica! Viva la Musica! Viva la Musica! Viva la Musica! Viva la Musica!  for casa in seq: print(casa+casa)  2 4 6 8 10  # Imprimir los strings de mi_lista por separado mi_lista=[\u0026quot;img\u0026quot;,\u0026quot;python\u0026quot;,\u0026quot;numpy\u0026quot;,\u0026quot;pandas\u0026quot;] for s in mi_lista: print(s) # este print va con indentación  img python numpy pandas  Ciclos while i = 1 while i \u0026lt; 5: print(f'i es: {i}') i = i+1  i es: 1 i es: 2 i es: 3 i es: 4  Control de ciclos break y continue Los comandos break y continue son utilizados para terminar/salir de un ciclos for o while.\nbreak Termina la iteracion actual y continua con la ejecución de la siguiente instrucción\nfor letra in \u0026quot;Python\u0026quot;: if letra == \u0026quot;h\u0026quot;: break print (f\u0026quot;Letra actual : {letra} \u0026quot;) print('Uso del comando break')  Letra actual : P Letra actual : y Letra actual : t Uso del comando break  continue Regresa al comienzo del ciclo, ignorando todos los commandos que quedan en la iteración actual del ciclo e inicia la siguiente iteración.\nfor letra in \u0026quot;Python\u0026quot;: if letra == \u0026quot;h\u0026quot;: continue print(f\u0026quot;Letra actual : {letra} \u0026quot;) print('Uso del comando continue')  Letra actual : P Letra actual : y Letra actual : t Letra actual : o Letra actual : n Uso del comando continue   List comprehension Las comprensiones de listas proveen una forma de escribir bucles for más concisamente. Pueden ser muy útiles para crear nuevas listas a partir de listas existentes o iterables, ademas tambien se puede poner condiciones. Link para mas ejemplos\n# Para elevar cada termino al cuadrado de una lista se deberia hacer asi: # Funcion para elevar al cuadrado cada uno de los elementos de una lista x = [1,2,3,4,5] #lista de valores out = [] # crear lista vacia for item in x: out.append(item**2) print(out)  [1, 4, 9, 16, 25]  # Usando List comprehension # *resultado* = [*operacion* *iteracion* *condicion* ] out = [item**2 for item in x] out  [1, 4, 9, 16, 25]  # Funcion para elevar al cuadrado cada uno de los elementos de una lista # Solo si el elemento es mayor que 3 out = [] for item in x: if item \u0026gt;= 3: out.append(item**2) print(out)  [9, 16, 25]  # *resultado* = [*operacion* *iteracion* *condicion* ] resultado = [item**2 for item in x if item \u0026gt;= 3] resultado  [9, 16, 25]   Funciones Las funciones se definen con la palabra clave ´def´ y tienen la sintaxis\ndef nombre_funcion(parametros): pasos de la funcion\nPara devolver un valor utilizamos la palabra clave return.\ndef my_func(): # el caracter `:` es obligatorio \u0026quot;\u0026quot;\u0026quot; # el cuerpo de la función esta definido por la identación (espacios en blanco) Docstring va aca. \u0026quot;\u0026quot;\u0026quot; print('Hola Python')  my_func()  Hola Python  def square(x): # el caracter `:` es obligatorio return x**2 # el cuerpo de la función esta definido por la identación (espacios en blanco) # es obligatorio usar `return` para devolver valores  out = square(2) out # Para imprimir en jupyter (IPython) no es necesario la funcion print  4  # Esta funcion recibe una lista y devuelve la suma de sus elementos def sumar_todos(lista): suma=0 # recordar la identacion en Python, se recomienda 4 espacios for v in lista: #suma = suma +v suma+=v # Se identa nuevamente ya que esta dentro de un for return suma  mi_lista=[54,12,99,15] la_suma = sumar_todos(mi_lista) print(f\u0026quot;Los elementos de la lista suman = {la_suma}\u0026quot;)  Los elementos de la lista suman = 180  Funciones basicas de python Son funciones que incluye python desde su instalacion, Built-in Functions, estas son palabras restringidas NO USAR\n   . . . . .     abs() dict() help() min() setattr()   all() dir() hex() next() slice()   any() divmod() id() object() sorted()   ascii() enumerate() input() oct() staticmethod()   bin() eval() int() open() str()   bool() exec() isinstance() ord() sum()   bytearray() filter() issubclass() pow() super()   bytes() float() iter() print() tuple()   callable() format() len() property() type()   chr() frozenset() list() range() vars()   classmethod() getattr() locals() repr() zip()   compile() globals() map() reversed() import()   complex() hasattr() max() round()    delattr() hash() memoryview() set()     # la funcion 'dir' Retorna un listado de atributos y metodos que tiene el tipo de datos especico dir('casa') #en este caso el tipo de datos es un string  ['__add__', '__class__', '__contains__', '__delattr__', '__dir__', '__doc__', '__eq__', '__format__', '__ge__', '__getattribute__', '__getitem__', '__getnewargs__', '__gt__', '__hash__', '__init__', '__init_subclass__', '__iter__', '__le__', '__len__', '__lt__', '__mod__', '__mul__', '__ne__', '__new__', '__reduce__', '__reduce_ex__', '__repr__', '__rmod__', '__rmul__', '__setattr__', '__sizeof__', '__str__', '__subclasshook__', 'capitalize', 'casefold', 'center', 'count', 'encode', 'endswith', 'expandtabs', 'find', 'format', 'format_map', 'index', 'isalnum', 'isalpha', 'isascii', 'isdecimal', 'isdigit', 'isidentifier', 'islower', 'isnumeric', 'isprintable', 'isspace', 'istitle', 'isupper', 'join', 'ljust', 'lower', 'lstrip', 'maketrans', 'partition', 'replace', 'rfind', 'rindex', 'rjust', 'rpartition', 'rsplit', 'rstrip', 'split', 'splitlines', 'startswith', 'strip', 'swapcase', 'title', 'translate', 'upper', 'zfill']  'casa'.capitalize()  'Casa'  'casa'.__class__  str  dir(8)  ['__abs__', '__add__', '__and__', '__bool__', '__ceil__', '__class__', '__delattr__', '__dir__', '__divmod__', '__doc__', '__eq__', '__float__', '__floor__', '__floordiv__', '__format__', '__ge__', '__getattribute__', '__getnewargs__', '__gt__', '__hash__', '__index__', '__init__', '__init_subclass__', '__int__', '__invert__', '__le__', '__lshift__', '__lt__', '__mod__', '__mul__', '__ne__', '__neg__', '__new__', '__or__', '__pos__', '__pow__', '__radd__', '__rand__', '__rdivmod__', '__reduce__', '__reduce_ex__', '__repr__', '__rfloordiv__', '__rlshift__', '__rmod__', '__rmul__', '__ror__', '__round__', '__rpow__', '__rrshift__', '__rshift__', '__rsub__', '__rtruediv__', '__rxor__', '__setattr__', '__sizeof__', '__str__', '__sub__', '__subclasshook__', '__truediv__', '__trunc__', '__xor__', 'as_integer_ratio', 'bit_length', 'conjugate', 'denominator', 'from_bytes', 'imag', 'numerator', 'real', 'to_bytes']  len() Nos entrega la longitud de los elementos que hay en una lista, diccionario, string, etc\nmi_lista=[54,12,99,15] len(mi_lista)  4  range() Cuando no tenemos una lista y queremos hacer un for \u0026ldquo;común\u0026rdquo; y que la variable que cambia sea un número que va incrementándose, podemos utilizar la función range.\nEn Python los índices comienzan en 0, y por eso los rangos también\nrange(5) #funcion range de 0 hasta 5 (sin incluir el 5)  range(0, 5)  list(range(5)) #se obtiene una lista de numeros enteros desde 0 hasta 5 (sin incluir el 5)  [0, 1, 2, 3, 4]  # Si se usa en un FOR no es necesario convertirlo en un a lista for i in range(5): # Este es el uso mas comun de la funcion range print(i)  0 1 2 3 4  #También se puede comenzar el rango en otro valor en lugar de 0 print(\u0026quot;- for de 2 a 5:\u0026quot;) for j in range(2,6): # range de 2 hasta 5 #aca va el codigo print(j)  - for de 2 a 5: 2 3 4 5  enumerate() mi_lista = ['apple', 'banana', 'grapes', 'pear'] # listado con frutas # funcion enumerate para obtener el numero de la iteracion # y el valor de la lista que estoy iterando for itera, valor in enumerate(mi_lista): print(f\u0026quot;En la iteracion {itera} tiene el valor asignado de {valor}\u0026quot; )  En la iteracion 0 tiene el valor asignado de apple En la iteracion 1 tiene el valor asignado de banana En la iteracion 2 tiene el valor asignado de grapes En la iteracion 3 tiene el valor asignado de pear   NOTA: Se recomienda leer sobre las funciones (map, filter, lambda y reduce) que actuan sobre listas, y que permiten reemplazar los ciclos for\nhttp://www.pythondiario.com/2017/10/programacion-funcional-lambda-map.html\n  Metodos Python permite definir clases y crear objetos de esas clases, pero esos temas están fuera de este tutorial. Los métodos se invocan de la siguiente forma objeto.metodo(parametro1,parametro2,\u0026hellip;).\nMetodos en strings Existen varios tipos de metodos para los strings, estos se utilizan mucho para hacer la preparacion de datos categoricos, para mas informacion:\n Metodos completos para strings  st = 'Hola mi nombre es Jose R'  len(st) #funcion que determina la longitud de 'st' tambien sirve con listas  24  Metodos de Formato .lower() devuelve el string en minusculas\nst.lower() #devuelve el string en minusculas  'hola mi nombre es jose r'  st # la operacion no es inplace  'Hola mi nombre es Jose R'  .upper() devuelve el string en Mayusculas\nst.upper() #devuelve el string en Mayusculas  'HOLA MI NOMBRE ES JOSE R'  st # la operacion no es inplace  'Hola mi nombre es Jose R'  .swapcase() una copia de la cadena convertidas las mayúsculas en minúsculas y viceversa.\nst  'Hola mi nombre es Jose R'  st.swapcase()  'hOLA MI NOMBRE ES jOSE r'  .capitalize() una copia de la cadena con la primera letra en mayúsculas.\nst = 'hola mi nombre es jose r' st.capitalize()  'Hola mi nombre es jose r'  .zfill() Retorna una copia de la cadena rellena con ceros a la izquierda hasta alcanzar la longitud final indicada.\nid_upb = '5789' id_upb.zfill(9)  '000005789'  Metodos de Busqueda .count() Retorna un entero representando la cantidad de apariciones de subcadena dentro de cadena\nst  'hola mi nombre es jose r'  st.count('e') # numero de veces que esta la letra 'e'  3  .find() Retorna: un entero representando la posición donde inicia la subcadena dentro de cadena. Si no la encuentra, retorna -1.\nst  'hola mi nombre es jose r'  st.find('nombre')  8  Metodos de Sustiticion .replace() Reemplazar texto en una cadena\n'hola mi nombre es jose r'.replace('jose r','Jose Ricardo')  'hola mi nombre es Jose Ricardo'  st.replace('jose r','Jose Ricardo')  'hola mi nombre es Jose Ricardo'  .strip() Eliminar caracteres a la izquierda y derecha de una cadena\nst_esp = ' hola mi nombre es jose r '# cadena de caracteres con muchos espacios st_esp  ' hola mi nombre es jose r '  st_esp.strip() # Eliminar por predeterminado los espacios de izquierda y derecha  'hola mi nombre es jose r'  st_esp = '------hola mi nombre es jose r----'# cadena de caracteres con muchos guiones st_esp  '------hola mi nombre es jose r----'  st_esp.strip('-')# Eliminar los guiones de izquierda y derecha  'hola mi nombre es jose r'  .lstrip() Eliminar caracteres a la izquierda de una cadena\nst_esp = 'www.kaggle.com' st_esp  'www.kaggle.com'  st_esp.lstrip('w.') # Eliminar desde el inicio de la cadena incluido los caracteres definidos  'kaggle.com'  '0000083746'.lstrip('0')  '83746'  .rstrip() Eliminar caracteres a la derecha de una cadena\nst_esp.rstrip('.com') # Eliminar desde el inicio de la cadena incluido los caracteres definidos  'www.kaggle'  Metodos de Union y Division .join() Retorna la cadena unida con el elemento que se desee\nformato_numero_factura = (\u0026quot;Nº 0000-0\u0026quot;, \u0026quot;-0000 (ID: \u0026quot;, \u0026quot;)\u0026quot;)  numero = \u0026quot;275\u0026quot;  numero_factura = numero.join(formato_numero_factura) numero_factura  'Nº 0000-0275-0000 (ID: 275)'  .split() Retorna una lista con todos elementos encontrados al dividir la cadena por un separador.\npalabras = \u0026quot;python, guia, curso, tutorial\u0026quot;.split(\u0026quot;, \u0026quot;) palabras  ['python', 'guia', 'curso', 'tutorial']  st  'hola mi nombre es jose r'  st.split() #divide el string en 'espacios en blanco por defecto' y devuelve una lista  ['hola', 'mi', 'nombre', 'es', 'jose', 'r']  tweet = 'Viva el Rock and Roll! #Musica #Concierto' # creando un string tweet  'Viva el Rock and Roll! #Musica #Concierto'  tweet.split('#') # dividir el string y el elemnto de division es #  ['Viva el Rock and Roll! ', 'Musica ', 'Concierto']  tweet.split('#')[1]  'Musica '  ['hola','como','estas']  ['hola', 'como', 'estas']  'hola como estas'.split()  ['hola', 'como', 'estas']  Metodos en listas Existen varios tipos de metodos para las listas, para mas informacion: Metodos completos para listas\n  list.append(x): Agrega un ítem al final de la lista. Equivale a lst[len(lst):] = [x]\n  list.insert(i, x): Inserta un ítem en una posición dada. El primer argumento es el índice del ítem delante del cual se insertará, por lo tanto a.insert(0, x) inserta al principio de la lista, y lst.insert(len(lst), x) equivale a lst.append(x)\n  list.remove(x): Quita el primer ítem de la lista cuyo valor sea x. Es un error si no existe tal ítem.\n  list.pop([i]): Quita el ítem en la posición dada de la lista, y lo devuelve. Si no se especifica un índice, a.pop() quita y devuelve el último ítem de la lista. (Los corchetes que encierran a i en la firma del método denotan que el parámetro es opcional, no que deberías escribir corchetes en esa posición. Verás esta notación con frecuencia en la Referencia de la Biblioteca de Python.)\n  list.clear(): Quita todos los elementos de la lista. Equivalente a del lst[:].\n  list.index(x[, start[, end]]): Devuelve el índice basado de la lista del primer ítem cuyo valor sea x. Levanta una excepción ValueError si no existe tal ítem. Los argumentos opcionales start y end son interpetados como la notación de slicing y se usan para limitar la búsqueda a una subsecuencia particular de la lista. El index retornado se calcula de manera relativa al inicio de la secuencia completa en lugar de con respecto al argumento start.\n  list.count(x): Devuelve el número de veces que x aparece en la lista.\n  list.sort(key=None, reverse=False): Ordena los ítems de la lista in situ (los argumentos pueden ser usados para personalizar el orden de la lista, ve sorted() para su explicación).\n  list.reverse(): Invierte los elementos de la lista in situ.\n  list.copy(): Devuelve una copia superficial de la lista. Equivalente a lst[:].\n  Las mas usadas son:\nlst = [8,2,3,45,24,53,785,98,1,3,6,9,2,1] # lista lst  [8, 2, 3, 45, 24, 53, 785, 98, 1, 3, 6, 9, 2, 1]  len(lst) # numero de elementos de la lista  14  .append(x): Agrega un ítem al final de la lista. Equivale a lst[len(lst):] = [x]\nlst.append(37) # Agrega este valro al final de la lista lst  [8, 2, 3, 45, 24, 53, 785, 98, 1, 3, 6, 9, 2, 1, 37]  .pop([i]): Quita el ítem en la posición dada de la lista, y lo devuelve. Si no se especifica un índice, lst.pop() quita y devuelve el último ítem de la lista. (Los corchetes que encierran a i en la firma del método denotan que el parámetro es opcional, no que deberías escribir corchetes en esa posición. Verás esta notación con frecuencia en la Referencia de la Biblioteca de Python.)\naa = lst.pop() # saca el ultimo elemento de la lista, para siempre aa  37  lst  [8, 2, 3, 45, 24, 53, 785, 98, 1, 3, 6, 9, 2, 1]  aa = lst.pop(4) # saca el 4 elemento de la lista, para siempre aa  24  lst  [8, 2, 3, 45, 53, 785, 98, 1, 3, 6, 9, 2, 1]  .copy(): Devuelve una copia superficial de la lista. Equivalente a list[:]\n# Hacer una copia de la lista, new_lst = lst.copy() new_lst # tambien puede ser new_lst=lst[:]  [8, 2, 3, 45, 53, 785, 98, 1, 3, 6, 9, 2, 1]  .sort(reverse=False): Ordena los ítems de la lista in situ o inplace\nlst.sort() #ordena la lista de menor a mayor lst  [1, 1, 2, 2, 3, 3, 6, 8, 9, 45, 53, 98, 785]  lst.sort(reverse=True) #ordena la lista de mayor a menor lst  [785, 98, 53, 45, 9, 8, 6, 3, 3, 2, 2, 1, 1]  Identificar Elementos en la lista 'x' in [1,2,3] # identificar si un elemento esta en una lista  False  'x' in ['x','y','z'] # identificar si un elemento esta en una lista  True  Instalar Paquetes o Librerias El repositorio Principal de donde se encuentran las librerias de Python es:\nPypi: https://pypi.org/\nSi Ha realizado la instalacion con Anaconda, las librerias se encuentran en:\nAnaconda Repo - https://anaconda.org/anaconda/repo\nInstalacion en Linea de comando PIP pip install [nombre_paquete] #sin corchetes\nEjemplo: pip install pandas\nCONDA Solo si se realizo una instalacion con Anaconda\nconda install -c [canal] [nombre_paquete] #sin corchetes\nEjemplo: conda install -c anaconda pandas\nInstalacion en Jupyter Notebook PIP !python -m pip install [nombre_paquete] #sin corchetes\nUsar este para Google Collaboratory\nEjemplo: !python -m pip install pandas\nCONDA !conda install -c [canal] [nombre_paquete] #sin corchetes\nEjemplo: !conda install -c anaconda pandas\nEjemplos Links con Jupyter notebook de ejemplo de diferentes temas, usando Python\nhttps://github.com/donnemartin/data-science-ipython-notebooks\nhttps://github.com/jupyter/jupyter/wiki/A-gallery-of-interesting-Jupyter-Notebooks\nReferencias  Tutorial official Python Tutorial Python en español Curso Python en Español Python para principiantes Practical Python Programming Python for data science Cheatsheet Python Data Science Handbook -online Free Think Python 2nd Edition by Allen B. Downey  Pensar en Python, traduccion al español   Markdown cheatsheet IPython tutorial 28 Jupyter Notebook tips, tricks, and shortcuts  Phd. Jose R. Zapata\n https://joserzapata.github.io/ https://twitter.com/joserzapata  ","date":1599609600,"expirydate":-62135596800,"kind":"section","lang":"en","lastmod":1599609600,"objectID":"c0a19dd173e9db74c6edbf46d58c22a8","permalink":"https://joserzapata.github.io/courses/python-ciencia-datos/python/","publishdate":"2020-09-09T00:00:00Z","relpermalink":"/courses/python-ciencia-datos/python/","section":"courses","summary":"Introduccion al Lenguaje Python","tags":null,"title":"Programacion Con Python","type":"book"},{"authors":["Jose R. Zapata"],"categories":null,"content":"Por Jose R. Zapata\n\nNumPy es una libreria de Algebra Lineal para Python, la razón por la cual es tan importante para Data Science con Python es que casi todas las librerias del ecosistema de PyData confían en NumPy como uno de sus principales componentes.\nNumpy también es increíblemente rápido, ya que tiene enlaces a librerias en C. Para obtener más información sobre por qué usar Arrays en lugar de listas, mira esto StackOverflow post.\nInstrucciones de instalacion Es altamente recomendable que instale Python utilizando la distribución Anaconda para asegurarse de que todas las dependencias subyacentes (como las bibliotecas de Álgebra lineal) se sincronicen con el uso de una instalación de conda. Si esta trabajando en Google Colab o Si tiene Anaconda Normalmente ya tiene instalado NumPy, de ser necesario puede instalar NumPy en el terminal escribiendo:\npip install numpy conda install numpy  Si no tiene Anaconda y no lo puede instalar, puede ver como instalar NumPy en la documentacion oficial Numpy\u0026rsquo;s official documentation on various installation instructions.\nImportando NumPy Luego de instalar Numpy la libreria se importa de la siguiente manera:\nimport numpy as np # esta es la forma estandar de importar numpy  Numpy tiene muchas funciones integradas. No los cubriremos todos, sino que nos centraremos en algunos de los aspectos más importantes de Numpy: vectores, arreglos, matrices y generación de números.\nArreglos en Numpy (Arrays) Los arreglos en NumPy son la principal forma de usar Numpy. Los arreglos de NumPy esencialmente vienen en dos tipos: vectores y matrices. Los vectores son estrictamente matrices de 1-d y las matrices son 2-d (pero debe tener en cuenta que una matriz aún puede tener solo una fila o una columna).\nCrear Arreglos desde Listas de Python Podemos crear un arreglo mediante la conversión directa de una lista o lista de listas:\nmy_list = [1,2,3] #lista en python my_list  [1, 2, 3]  np.array(my_list) #conversion de una lista a arreglo mediante el metodo 'array'  array([1, 2, 3])  a = np.array(my_list) #se peude asignar a una variable type(a) #tipo de dato  numpy.ndarray  a.dtype #tipo de datos que estan en el arreglo  dtype('int64')  my_matrix = [[1,2,3],[4,5,6],[7,8,9]] # Una lista de listas my_matrix  [[1, 2, 3], [4, 5, 6], [7, 8, 9]]  np.array(my_matrix) # conversion de una lista de listas a Matriz  array([[1, 2, 3], [4, 5, 6], [7, 8, 9]])  b = np.array(my_matrix) # aasginar la conversion a una variable type(b) # tipo de dato  numpy.ndarray  Metodos integrados (Built-in Methods) Existen muchas funciones para de generar arreglos\narange Devuelve valores espaciados uniformemente dentro de un intervalo dado. Es muy parecida a la funcion range de las basicas de python.\n# generar un arreglo de enteros desde 0 hasta 9, recordar que el valor final no se incluye np.arange(0,10) # arange(valor_inicial,valor_final-1)  array([0, 1, 2, 3, 4, 5, 6, 7, 8, 9])  np.arange(0,11,2) ## arange(valor_inicial,valor_final-1,paso)  array([ 0, 2, 4, 6, 8, 10])  zeros y ones Generar arreglos de Ceros o Unos\nnp.zeros(3) # Generar un arreglo de ceros que contenga 3 elementos  array([0., 0., 0.])  np.zeros((5,5)) # Generar una matriz de ceros de 5 x 5 elementos  array([[0., 0., 0., 0., 0.], [0., 0., 0., 0., 0.], [0., 0., 0., 0., 0.], [0., 0., 0., 0., 0.], [0., 0., 0., 0., 0.]])  np.ones(3) # Generar un arreglo de unos que contenga 3 elementos  array([1., 1., 1.])  np.ones((3,3)) # Generar una matriz de unos de 5 x 5 elementos  array([[1., 1., 1.], [1., 1., 1.], [1., 1., 1.]])  linspace Devuelve números espaciados uniformemente durante un intervalo especificado.\n# Generar 3 valores iniciando en 0 y terminando en 10 (incluyendolo) np.linspace(0,10,3) # linspace(valor_inicial, valor_final, numero_de_elementos)  array([ 0., 5., 10.])  # Generar 50 valores iniciando en 0 y terminando en 10 (incluyendolo) np.linspace(0,10,50)  array([ 0. , 0.20408163, 0.40816327, 0.6122449 , 0.81632653, 1.02040816, 1.2244898 , 1.42857143, 1.63265306, 1.83673469, 2.04081633, 2.24489796, 2.44897959, 2.65306122, 2.85714286, 3.06122449, 3.26530612, 3.46938776, 3.67346939, 3.87755102, 4.08163265, 4.28571429, 4.48979592, 4.69387755, 4.89795918, 5.10204082, 5.30612245, 5.51020408, 5.71428571, 5.91836735, 6.12244898, 6.32653061, 6.53061224, 6.73469388, 6.93877551, 7.14285714, 7.34693878, 7.55102041, 7.75510204, 7.95918367, 8.16326531, 8.36734694, 8.57142857, 8.7755102 , 8.97959184, 9.18367347, 9.3877551 , 9.59183673, 9.79591837, 10. ])  eye Crea la matriz identidad\nnp.eye(4) # crea la matriz identidad de 4x4 elementos  array([[1., 0., 0., 0.], [0., 1., 0., 0.], [0., 0., 1., 0.], [0., 0., 0., 1.]])  Numeros Aleatorios (Random) Numpy tiene diferentes formas de crear arrelgos de numeros aleatorios, el modulo para realizar esto se llama Random:\nrand Crea un arreglo de la forma dada y rellenela con muestras aleatorias de una distribución uniforme sobre [0, 1).\n# creacion de un arreglo de 2 elementos 1 una dimension np.random.rand(2) #Los numeros aleatorios seran de una distribucion uniforme  array([0.55464672, 0.60410863])  # creacion de un arreglo de 5x5 np.random.rand(5,5) #Los numeros aleatorios seran de una distribucion uniforme  array([[0.62152507, 0.23262413, 0.04667912, 0.141403 , 0.9316874 ], [0.21404498, 0.28747701, 0.79732099, 0.24004423, 0.20145937], [0.34281437, 0.62585515, 0.66643919, 0.59694899, 0.47552382], [0.66862586, 0.66065437, 0.13807543, 0.68819131, 0.30860298], [0.80021553, 0.67175583, 0.35132375, 0.85041896, 0.83744445]])  randn Devuelve una muestra (o muestras) de la distribución \u0026ldquo;estándar normal\u0026rdquo;. A diferencia del rand que es uniforme:\n# creacion de un arreglo de 2 elementos 1 una dimension np.random.randn(2) #Los numeros aleatorios seran de una distribucion \u0026quot;estándar normal\u0026quot;  array([-0.34307289, -0.619604 ])  # creacion de un arreglo de 5x5 np.random.randn(5,5) #Los numeros aleatorios seran de una distribucion \u0026quot;estándar normal\u0026quot;  array([[ 0.15919913, -0.49995497, 0.03898612, -1.75276706, -1.35012055], [ 0.95082787, -2.19552647, 1.36339451, 0.80626886, 0.0098413 ], [-1.18150005, 0.46867232, 0.60158901, 0.73681089, 0.4030547 ], [-0.5162413 , 0.88873899, -0.07352698, 0.35424756, 0.53750849], [ 1.56882644, -0.88221093, -0.05125666, -0.10572003, -0.58459871]])  randint Entrega numeros enteros aleatorios desde inicio (inclusivo) hasta final (exclusivo).\nnp.random.randint(1,100) # Genera un numero aleatorio entre 1 y 99  19  np.random.randint(1,100,(10,2)) # Genera un arreglo de 10 elementos entre 1 y 99  array([[47, 84], [16, 64], [82, 87], [33, 58], [48, 4], [69, 11], [82, 53], [76, 32], [97, 53], [ 4, 29]])  Atributos y Metodos de los arreglos import numpy as np arr = np.arange(25) #Genera un arreglo de numeros enteros del 0 al 24 ranarr = np.random.randint(0,50,10) #Genera un arreglo de 10 elementosdel 0 al 49  arr  array([ 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24])  ranarr  array([41, 20, 5, 13, 40, 1, 7, 43, 33, 34])  Reshape Devuelve una matriz que contiene los mismos datos con una nueva distribucion\n# arr es un vector de 25 elementos y se convertira en una matriz de 5x5 arr.reshape(5,5)  array([[ 0, 1, 2, 3, 4], [ 5, 6, 7, 8, 9], [10, 11, 12, 13, 14], [15, 16, 17, 18, 19], [20, 21, 22, 23, 24]])  max,min,argmax,argmin Estos son métodos útiles para encontrar valores máximos o mínimos. O para encontrar el indice de su ubicacione usando argmin o argmax\nranarr  array([41, 20, 5, 13, 40, 1, 7, 43, 33, 34])  ranarr.max() # Valor maximo del arreglo  43  ranarr.argmax() # Posicion del valor maximo del arreglo (recordar que empieza en cero)  7  ranarr.min() # Valor minimo del arreglo  1  ranarr.argmin() # Posicion del valor maximo del arreglo (recordar que empieza en cero)  5  Shape Shape es un attribute que los arreglos tienen para definir sus dimensiones (No es metodo):\n# Vector arr.shape  (25,)  # Cambiando las dimensiones del arreglo para que sea una matriz # de una sola dimension horizontal arr.reshape(1,25)  array([[ 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24]])  # Cambiando las dimensiones del arreglo para que sea una matriz # de una sola dimension vertical arr.reshape(25,1)  array([[ 0], [ 1], [ 2], [ 3], [ 4], [ 5], [ 6], [ 7], [ 8], [ 9], [10], [11], [12], [13], [14], [15], [16], [17], [18], [19], [20], [21], [22], [23], [24]])  arr.reshape(25,1).shape  (25, 1)  dtype Para obtener los tipos de datos dentro del arreglo:\narr.dtype  dtype('int64')  size Numero de elementos en un arreglo\narr_2d = np.array(([5,10,15],[20,25,30],[35,40,45])) arr_2d.size  9  ndim Numero de dimensiones del arreglo o matriz\narr_2d.ndim  2  Indexacion y Seleccion en NumPy Como indexar y seleccionar elementos o grupos de elementos de un arreglo (array)\n#Creando un arreglo de ejemplo arr = np.arange(0,11) # Generar un arreglo de enteros del 0 hasta el 10 arr  array([ 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10])  arr.dtype # tipos de datos dentro del arreglo  dtype('int64')  Indexación y Selección con corchetes La forma más sencilla de elegir uno o algunos elementos de una matriz es similar a las listas de Python:\n# Obtener un valor conociendo su indice (index) arr[8]  8  #Obtener los valores en un rango [valor_inicial, valor_final -1] arr[1:5]  array([1, 2, 3, 4])  #Obtener los valores en un rango [valor_inicial, valor_final -1] arr[0:5]  array([0, 1, 2, 3, 4])  Broadcasting (Difusion) Los arreglos de Numpy difieren de una lista normal de Python en su capacidad de Broadcasting, qeu es asignar un valor a un rango de posiciones:\n# definiendo un valor para todo un rango de posiciones (Broadcasting) arr[0:5]=100 # Asignar el numero 100 a las pocisiones desde el 0 hasta el 4 arr  array([100, 100, 100, 100, 100, 5, 6, 7, 8, 9, 10])  #crear nuevamente el arreglo con el que estabamos trabajando arr = np.arange(0,11) arr  array([ 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10])  #NOTA importante en la seleccion de rangos (sclicing) # los arreglos son mutables slice_of_arr = arr[0:6] slice_of_arr  array([0, 1, 2, 3, 4, 5])  #Cambiar todos los valores a 99 slice_of_arr[:]=99 slice_of_arr  array([99, 99, 99, 99, 99, 99])  Observe que los cambios tambien ocurren en el arreglo original\narr  array([99, 99, 99, 99, 99, 99, 6, 7, 8, 9, 10])  Los datos no se copian, ¡es un puntero a el arreglo original! ¡Esto evita problemas de memoria!\n# Para obtener una copia, se debe hacer explicito arr = np.arange(0,11) arr_copy = arr.copy() arr_copy[:] = 99 arr_copy  array([99, 99, 99, 99, 99, 99, 99, 99, 99, 99, 99])  # Observe que el arreglo original no se modifico arr  array([ 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10])  Indexacion de arreglos 2D (matrices) El formato general es arr_2d[fila][col] o arr_2d[fila,col]. Se recomienda usar la notacion con la coma por claridad.\n# Creacion de una matriz de 3x3 arr_2d = np.array(([5,10,15],[20,25,30],[35,40,45])) arr_2d  array([[ 5, 10, 15], [20, 25, 30], [35, 40, 45]])  #Indexando por filas arr_2d[1] # Obtener la fila 1 (recordar que el indice de python empieza en 0)  array([20, 25, 30])  # El formato es **arr_2d[fila][col]** o **arr_2d[fila,col]** # Obteniendo un elemento en especifico arr_2d[1][0] # elemento de la fila 1 columna 0  20  # Obteniendo un elemento en especifico arr_2d[1,0] # elemento de la fila 1 columna 0  20  # seleccion de rangos en arreglos 2D (slicing) #dimensiones (2,2) desde la esquina superior derecha arr_2d[:2,1:] # filas [0,1] y columnas 1 hasta el final  array([[10, 15], [25, 30]])  # fila de indice 2 arr_2d[2]  array([35, 40, 45])  # todos los elementos de las columnas que estan en la fila de posicion 2 arr_2d[2,:]  array([35, 40, 45])  Indexacion especial La indexación especial permite seleccionar filas o columnas enteras desordenadas\n#Creando una matriz de zeros arr2d = np.zeros((10,10)) arr2d  array([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0.], [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.], [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.], [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.], [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.], [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.], [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.], [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.], [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.], [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])  #Tamaño del arreglo arr_length = arr2d.shape[1] arr_length  10  # Creando una matriz con elementos que contienen el valor correspondiente a la posicion de la fila for i in range(arr_length): arr2d[i] = i arr2d  array([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0.], [1., 1., 1., 1., 1., 1., 1., 1., 1., 1.], [2., 2., 2., 2., 2., 2., 2., 2., 2., 2.], [3., 3., 3., 3., 3., 3., 3., 3., 3., 3.], [4., 4., 4., 4., 4., 4., 4., 4., 4., 4.], [5., 5., 5., 5., 5., 5., 5., 5., 5., 5.], [6., 6., 6., 6., 6., 6., 6., 6., 6., 6.], [7., 7., 7., 7., 7., 7., 7., 7., 7., 7.], [8., 8., 8., 8., 8., 8., 8., 8., 8., 8.], [9., 9., 9., 9., 9., 9., 9., 9., 9., 9.]])  La indexacion especial permite:\n# sacar las filas 2, 4 , 6, 8 arr2d[[2,4,6,8]] # observe el uso de los dobles corchetes  array([[2., 2., 2., 2., 2., 2., 2., 2., 2., 2.], [4., 4., 4., 4., 4., 4., 4., 4., 4., 4.], [6., 6., 6., 6., 6., 6., 6., 6., 6., 6.], [8., 8., 8., 8., 8., 8., 8., 8., 8., 8.]])  #Permite cualquier orden arr2d[[6,4,2,7]]  array([[6., 6., 6., 6., 6., 6., 6., 6., 6., 6.], [4., 4., 4., 4., 4., 4., 4., 4., 4., 4.], [2., 2., 2., 2., 2., 2., 2., 2., 2., 2.], [7., 7., 7., 7., 7., 7., 7., 7., 7., 7.]])  Ayudas para indexacion La indexación de una matriz 2d puede ser un poco confusa al principio, especialmente cuando comienza a agregar pasos en la seleccion. Pruebe buscar imagenes en google con la parala NumPy indexing y encontrara ejemplos utiles como: Seleccion basados en operadores de comparacion # creacion de un arreglo de enteros desde 0 hasta 10 arr = np.arange(1,11) # recordar que el ultimo valor no se incluye arr  array([ 1, 2, 3, 4, 5, 6, 7, 8, 9, 10])  arr \u0026gt; 4 # determinar cuales valores del arreglo son mayores que 4 # el resultado es un arreglo de Booleans  array([False, False, False, False, True, True, True, True, True, True])  bool_arr = arr\u0026gt;4 # creacion de arreglo de booleans  bool_arr  array([False, False, False, False, True, True, True, True, True, True])  # Seleccion de elementos usando un arreglo de Booleans arr[bool_arr] # Solamente se retornan los elementos en los cuales las posiciones de bool_arr sean verdaderas  array([ 5, 6, 7, 8, 9, 10])  # Se puede hacer esta seleccion mucho mas rapida realizando la comparacion dentro de los corchetes arr[arr\u0026gt;2] # Obtener los valores del arreglo mayores que 2  array([ 3, 4, 5, 6, 7, 8, 9, 10])  x = 2 # se puede hacer usando una variable arr[arr\u0026gt;x]  array([ 3, 4, 5, 6, 7, 8, 9, 10])  Concatenacion # En una dimension x = np.array([1, 2, 3]) # Vector de valores y = np.array([3, 2, 1]) # Vector de valores np.concatenate([x, y]) # Concatenacion  array([1, 2, 3, 3, 2, 1])  # En dos dimensiones de forma vertical grid = np.array([[1, 2, 3],[4, 5, 6]]) np.concatenate([grid, grid])  array([[1, 2, 3], [4, 5, 6], [1, 2, 3], [4, 5, 6]])  # En dos dimensiones de forma horizontal grid = np.array([[1, 2, 3],[4, 5, 6]]) np.concatenate([grid, grid],axis=1)  array([[1, 2, 3, 1, 2, 3], [4, 5, 6, 4, 5, 6]])  Operaciones con NumPy Con las listas de python no se pueden realizar operaciones elemento a elemento, pero con NumPy si se puede realizar.\nComo es el comportamiento de las listas ante las siguientres operaciones:\n# observar el comportamiento de las listas lista = list(range(10)) lista  [0, 1, 2, 3, 4, 5, 6, 7, 8, 9]  lista + lista # este procedimento lo que hace es concatenar las listas  [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 0, 1, 2, 3, 4, 5, 6, 7, 8, 9]  x = 2 lista*x # este procedimento hace que se repita la lista x numero de veces  [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 0, 1, 2, 3, 4, 5, 6, 7, 8, 9]  Si quiero realizar operaciones vectoriales o con matrices se realizan con arreglos NumPy\nAritmetica Puede realizar fácilmente aritmética de matriz a matriz o aritmética de escalar con matriz.\nimport numpy as np # importar la libreria de NumPy arr = np.arange(0,10) # crear un arreglo de 10 elementos arr  array([0, 1, 2, 3, 4, 5, 6, 7, 8, 9])  arr + arr # Suma elemento a elemento de los arreglos  array([ 0, 2, 4, 6, 8, 10, 12, 14, 16, 18])  # Suma de valores uno a uno con arreglos aa = np.arange(5) bb = np.arange(10) # este arreglo es mas grande que el anterios aa+bb[:5] # deben tener el mismo tamaño para realizar la suma  array([0, 2, 4, 6, 8])  arr * arr # Multiplicacion elemento a elemento  array([ 0, 1, 4, 9, 16, 25, 36, 49, 64, 81])  arr - arr # Resta elemento a elemento  array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0])  # Mensaje de advertencia si se presenta una division por cero, pero no hay error! # solo se reemplazo por el valor nan arr/arr # division elemento a elemento  array([nan, 1., 1., 1., 1., 1., 1., 1., 1., 1.])  # Tambien genera una advertencia, pero no hay error lo que se genera es un infinity # Observar el primer elemento del arreglo 1/arr  array([ inf, 1. , 0.5 , 0.33333333, 0.25 , 0.2 , 0.16666667, 0.14285714, 0.125 , 0.11111111])  arr**3 # eleva al cubo cada elemento del arreglo  array([ 0, 1, 8, 27, 64, 125, 216, 343, 512, 729])  Funciones Universales de los arreglos NumPy tiene integrado muchas funciones universales, que son esencialmente solo operaciones matemáticas que se pueden usar para realizar la operación en todo el arreglo. Las mas importantes son:\n Matematicas Trigonometria Funciones de comparacion Funciones de punto flotante  Algunos ejemplos son:\n#Calcular la raiz cuadrada de cada elemento np.sqrt(arr)  array([0. , 1. , 1.41421356, 1.73205081, 2. , 2.23606798, 2.44948974, 2.64575131, 2.82842712, 3. ])  #Calcular el exponencial (e^) de cada elemento np.exp(arr)  array([1.00000000e+00, 2.71828183e+00, 7.38905610e+00, 2.00855369e+01, 5.45981500e+01, 1.48413159e+02, 4.03428793e+02, 1.09663316e+03, 2.98095799e+03, 8.10308393e+03])  # Obtener el valor maximo como una funcion np.max(arr) #lo mismo arr.max()  9  # Calcular el Sin de cada elemento np.sin(arr)  array([ 0. , 0.84147098, 0.90929743, 0.14112001, -0.7568025 , -0.95892427, -0.2794155 , 0.6569866 , 0.98935825, 0.41211849])  # Calcular el Logaritmo de cada elemento np.log(arr)  array([ -inf, 0. , 0.69314718, 1.09861229, 1.38629436, 1.60943791, 1.79175947, 1.94591015, 2.07944154, 2.19722458])  # Calcular el valor absoluto np.abs(arr)  array([0, 1, 2, 3, 4, 5, 6, 7, 8, 9])  Estadistica Mas funciones en:\nhttps://docs.scipy.org/doc/numpy/reference/routines.statistics.html\n# Desviacion estandar np.std(arr)  2.8722813232690143  # Promedio de los valores np.mean(arr)  4.5  # Media np.median(arr)  4.5  # Varianza np.var(arr)  8.25  Operaciones de Matrices #creacion de una matriz de ejemplo arr_2d = np.array(([5,10,15],[20,25,30],[35,40,45])) arr_2d  array([[ 5, 10, 15], [20, 25, 30], [35, 40, 45]])  # transpuesta de una matriz arr_2d.T # tambien puede ser arr_2d.transpose()  array([[ 5, 20, 35], [10, 25, 40], [15, 30, 45]])  # Multiplicacion de matrices a = np.array([1,4,3]) # vector = arreglo de 1 dimension b = np.array([2,-1,5]) # vector = arreglo de 1 dimension a@b  13  # Multiplicacion de matrices # Producto Punto a = np.array(([2,0,1],[3,0,0],[5,1,1])) b = np.array(([1,0,1],[1,2,1],[1,1,0])) a@b  array([[3, 1, 2], [3, 0, 3], [7, 3, 6]])  #Tambien puede ser de esta forma np.dot(a,b)  array([[3, 1, 2], [3, 0, 3], [7, 3, 6]])  # Producto Cruz a = np.array([1,4,3]) # vector = arreglo de 1 dimension b = np.array([2,-1,5]) # vector = arreglo de 1 dimensi np.cross(a,b)  array([23, 1, -9])  Referencias  Tutorial oficial de Numpy NumPy Cheatsheet Python - Linear Algebra - cheatsheet Numpy for Matlab Users Funciones universales  Phd. Jose R. Zapata\n https://joserzapata.github.io/ https://twitter.com/joserzapata  ","date":1599609600,"expirydate":-62135596800,"kind":"section","lang":"en","lastmod":1599609600,"objectID":"55aa8b29b11d7c6b804a8896528d3d37","permalink":"https://joserzapata.github.io/courses/python-ciencia-datos/numpy/","publishdate":"2020-09-09T00:00:00Z","relpermalink":"/courses/python-ciencia-datos/numpy/","section":"courses","summary":"Libreria de Algebra Lineal (Vectores y Matrices)","tags":null,"title":"NUMPY - Vectores y Matrices","type":"book"},{"authors":null,"categories":null,"content":"Por Jose R. Zapata\n\nPandas es una herramienta de manipulación de datos de alto nivel desarrollada por Wes McKinney. Es construido sobre Numpy y permite el análisis de datos que cuenta con las estructuras de datos que necesitamos para limpiar los datos en bruto y que sean aptos para el análisis (por ejemplo, tablas). Como Pandas permite realizar tareas importantes, como alinear datos para su comparación, fusionar conjuntos de datos, gestión de datos perdidos, etc., se ha convertido en una librería muy importante para procesar datos a alto nivel en Python (es decir, estadísticas ). Pandas fue diseñada originalmente para gestionar datos financieros, y como alternativo al uso de hojas de cálculo (es decir, Microsoft Excel).\nLos principales tipos de datos que pueden representarse con pandas son:\n Datos tabulares con columnas de tipo heterogéneo con etiquetas en columnas y filas. Series temporales  Pandas proporciona herramientas que permiten:\n leer y escribir datos en diferentes formatos: CSV, JSON, Excel, bases SQL y formato HDF5 seleccionar y filtrar de manera sencilla tablas de datos en función de posición, valor o etiquetas fusionar y unir datos transformar datos aplicando funciones tanto en global como por ventanas manipulación de series temporales hacer gráficas  En pandas existen tres tipos básicos de objetos todos ellos basados a su vez en Numpy:\n Series (listas, 1D) DataFrame (tablas, 2D)  Por lo tanto, Pandas nos proporciona las estructuras de datos y funciones necesarias para el análisis de datos.\nInstalar Pandas Pandas ya esta preinstalado si se usa Google Collaboratory, si va realizar una instalacion en su computador\nPIP pip install pandas  Conda Si instalo Anaconda, pandas ya viene preinstalado\nconda install pandas  Importando Pandas La libreria Pandas se importa de la siguiente manera\nimport pandas as pd # Importacion estandar de la libreria Pandas import numpy as np # Importacion estandar de la libreria NumPy  Series Una serie es el primer tipo de datos de pandas y es muy similar a una matriz NumPy (de hecho está construida sobre el objeto de matriz NumPy). Lo que diferencia un arreglo NumPy de una serie, es que una serie puede tener etiquetas en los ejes, lo que significa que puede ser indexada por una etiqueta, en lugar de solo una ubicación numérica. Tampoco necesita contener datos numéricos, puede contener cualquier Objeto de Python arbitrario.\nCreando una Serie Puede convertir una lista, una matriz numpy o un diccionario en una serie, usando el metodo pd.Series:\n# Crear diferentes tipos de datos labels = ['a','b','c'] # lista de eetiquetas my_list = [10,20,30] # lista con valores arr = np.array([10,20,30]) # Convertir ista de valores en arreglo NumPy d = {'a':10,'b':20,'c':30} # Creacion de un diccionario  Desde Listas # Convertir una lista en series usando el metodo pd.Series # observe que se crean los nombres con las posiciones de cada elemento pd.Series(data=my_list)  0 10 1 20 2 30 dtype: int64  # Convertir una lista en series usando el metodo pd.Series # se puede ingresar el nombre de las posiciones pd.Series(data=my_list,index=labels)  a 10 b 20 c 30 dtype: int64  # No es necesario ingresar la palabra de 'data ='' en el argumento pd.Series(my_list,labels)  a 10 b 20 c 30 dtype: int64  Desde Arreglos NumPy # Convertir un arreglo en series usando el metodo pd.Series pd.Series(arr)  0 10 1 20 2 30 dtype: int64  # Convertir un arreglo en series indicando tambien los valores del index pd.Series(arr,labels)  a 10 b 20 c 30 dtype: int64  Desde un Diccionario # Convertir un diccionario en series usando el metodo pd.Series # Como el diccionario ya tiene clave entonces se le asigna como valor de la posicion pd.Series(d)  a 10 b 20 c 30 dtype: int64  Datos en una Series Una serie de pandas puede contener una variedad de tipos de objetos:\n# Creando una serie basado solo en una lista de letras pd.Series(data=labels)  0 a 1 b 2 c dtype: object  Indexacion La clave para usar una serie es entender su índice. Pandas hace uso de estos nombres o números de índice al permitir búsquedas rápidas de información (funciona como una tabla hash o diccionario).\nVeamos algunos ejemplos de cómo obtener información de una serie. Vamos a crear dos series, ser1 y ser2:\n# Creacion de una serie con sus labels o indices ser1 = pd.Series([1,2,3,4],index = ['USA', 'Germany','USSR', 'Japan'])  ser1  USA 1 Germany 2 USSR 3 Japan 4 dtype: int64  # Creacion de una serie con sus labels o indices ser2 = pd.Series([1,2,5,4],index = ['USA', 'Germany','Italy', 'Japan'])  ser2  USA 1 Germany 2 Italy 5 Japan 4 dtype: int64  # La busqueda en una serie es igual como en un diccionario ser1['USA']  1  # La busqueda en una serie es igual como en un diccionario ser2['Germany']  2  Las operaciones también se realizan según el índice:\n# Observe los resultados de los paises que solo estan en una serie y no en las dos ser1 + ser2  Germany 4.0 Italy NaN Japan 8.0 USA 2.0 USSR NaN dtype: float64  DataFrames Los DataFrames son la estructura mas importante en pandas y están directamente inspirados en el lenguaje de programación R. Se puede pensar en un DataFrame como un conjunto de Series reunidas que comparten el mismo índice. En los DataFrame tenemos la opción de especificar tanto el index (el nombre de las filas) como columns (el nombre de las columnas).\n# Importar la funcion de NumPy para crear arreglos de numeros enteros from numpy.random import randn np.random.seed(101) # Inicializar el generador aleatorio  # Forma rapida de crear una lista de python desde strings 'A B C D E'.split()  ['A', 'B', 'C', 'D', 'E']  # Crear un dataframe con numeros aleatorios de 4 Columnas y 5 Filas # Crear listas rapidamente usando la funcion split 'A B C D E'.split() # Esto evita tener que escribir repetidamente las comas df = pd.DataFrame(randn(5,4), index='A B C D E'.split(), columns='W X Y Z'.split())  df   .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; }  \n  W X Y Z     A 2.706850 0.628133 0.907969 0.503826   B 0.651118 -0.319318 -0.848077 0.605965   C -2.018168 0.740122 0.528813 -0.589001   D 0.188695 -0.758872 -0.933237 0.955057   E 0.190794 1.978757 2.605967 0.683509     Descripcion general del dataframe Numero de Filas y Columnas df.shape # retorna un Tuple asi: (filas, col)  (5, 4)  Informacion General de los datos # Informacion general de los datos de cada cloumna # Indica el numero de filas del dataset # Muestra el numero de datos No Nulos por columna (valores validos) # Tipo de dato de cada columna # Tamaño total del dataset df.info()  \u0026lt;class 'pandas.core.frame.DataFrame'\u0026gt; Index: 5 entries, A to E Data columns (total 4 columns): W 5 non-null float64 X 5 non-null float64 Y 5 non-null float64 Z 5 non-null float64 dtypes: float64(4) memory usage: 200.0+ bytes  # Tipos de datos que existen en las columnas del dataframe df.dtypes  W float64 X float64 Y float64 Z float64 dtype: object  Resumen de estadistica descriptiva General el metodo .describe() de los dataframes presenta un resumen de la estadistica descriptiva general de las columnas numericas del dataframe, presenta la informacion de:\n Promedio (mean) Desviacion estandard (std) Valor minimo Valor maximo Cuartiles (25%, 50% y 75%)  df.describe() # No muestra la informacion de las columnas categoricas   .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; }  \n  W X Y Z     count 5.000000 5.000000 5.000000 5.000000   mean 0.343858 0.453764 0.452287 0.431871   std 1.681131 1.061385 1.454516 0.594708   min -2.018168 -0.758872 -0.933237 -0.589001   25% 0.188695 -0.319318 -0.848077 0.503826   50% 0.190794 0.628133 0.528813 0.605965   75% 0.651118 0.740122 0.907969 0.683509   max 2.706850 1.978757 2.605967 0.955057     Ver Primeros elementos del dataframe df.head()   .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; }  \n  W X Y Z     A 2.706850 0.628133 0.907969 0.503826   B 0.651118 -0.319318 -0.848077 0.605965   C -2.018168 0.740122 0.528813 -0.589001   D 0.188695 -0.758872 -0.933237 0.955057   E 0.190794 1.978757 2.605967 0.683509     Ver Ultimos elementos del dataframe df.tail()   .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; }  \n  W X Y Z     A 2.706850 0.628133 0.907969 0.503826   B 0.651118 -0.319318 -0.848077 0.605965   C -2.018168 0.740122 0.528813 -0.589001   D 0.188695 -0.758872 -0.933237 0.955057   E 0.190794 1.978757 2.605967 0.683509     Seleccion y Indexacion Existen diversos métodos para tomar datos de un DataFrame\n# Regresara todos los datos de la columna W df['W']  A 2.706850 B 0.651118 C -2.018168 D 0.188695 E 0.190794 Name: W, dtype: float64  # Seleccionar dos o mas columnas # Pasar una lista con los nombres de las columnas df[['W','Z']]   .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; }  \n  W Z     A 2.706850 0.503826   B 0.651118 0.605965   C -2.018168 -0.589001   D 0.188695 0.955057   E 0.190794 0.683509     # Seleccionar dos o mas columnas # Pasar una lista con los nombres de las columnas # Puedo indicar el orden de las columnas df[['X','W','Z']]   .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; }  \n  X W Z     A 0.628133 2.706850 0.503826   B -0.319318 0.651118 0.605965   C 0.740122 -2.018168 -0.589001   D -0.758872 0.188695 0.955057   E 1.978757 0.190794 0.683509     Las columnas de un DataFrame Columns son solo Series\ntype(df['W']) # Tipos de datos  pandas.core.series.Series  Creando una Nueva Columna # Nueva columna igual a la suma de otras dos # operacion vectorizada df['new'] = df['W'] + df['Y'] df   .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; }  \n  W X Y Z new     A 2.706850 0.628133 0.907969 0.503826 3.614819   B 0.651118 -0.319318 -0.848077 0.605965 -0.196959   C -2.018168 0.740122 0.528813 -0.589001 -1.489355   D 0.188695 -0.758872 -0.933237 0.955057 -0.744542   E 0.190794 1.978757 2.605967 0.683509 2.796762     Eliminando Columnas df.drop('new',axis=1) # axis = 0 elimina filas(index) axis = 1 elimina columnas   .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; }  \n  W X Y Z     A 2.706850 0.628133 0.907969 0.503826   B 0.651118 -0.319318 -0.848077 0.605965   C -2.018168 0.740122 0.528813 -0.589001   D 0.188695 -0.758872 -0.933237 0.955057   E 0.190794 1.978757 2.605967 0.683509     # No se aplica a el dataframe a menos que se especifique. # Como se ve la operacion pasada no quedo grabada df   .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; }  \n  W X Y Z new     A 2.706850 0.628133 0.907969 0.503826 3.614819   B 0.651118 -0.319318 -0.848077 0.605965 -0.196959   C -2.018168 0.740122 0.528813 -0.589001 -1.489355   D 0.188695 -0.758872 -0.933237 0.955057 -0.744542   E 0.190794 1.978757 2.605967 0.683509 2.796762     # Para que quede grabado se puede hacer de dos formas # df = df.drop('new',axis=1) # Forma 1 df.drop('new', axis=1, inplace=True) # Forma 2 df   .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; }  \n  W X Y Z     A 2.706850 0.628133 0.907969 0.503826   B 0.651118 -0.319318 -0.848077 0.605965   C -2.018168 0.740122 0.528813 -0.589001   D 0.188695 -0.758872 -0.933237 0.955057   E 0.190794 1.978757 2.605967 0.683509     df   .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; }  \n  W X Y Z     A 2.706850 0.628133 0.907969 0.503826   B 0.651118 -0.319318 -0.848077 0.605965   C -2.018168 0.740122 0.528813 -0.589001   D 0.188695 -0.758872 -0.933237 0.955057   E 0.190794 1.978757 2.605967 0.683509     También se puede sacar filas de esta manera:\ndf.drop('E',axis=0)   .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; }  \n  W X Y Z     A 2.706850 0.628133 0.907969 0.503826   B 0.651118 -0.319318 -0.848077 0.605965   C -2.018168 0.740122 0.528813 -0.589001   D 0.188695 -0.758872 -0.933237 0.955057     df   .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; }  \n  W X Y Z     A 2.706850 0.628133 0.907969 0.503826   B 0.651118 -0.319318 -0.848077 0.605965   C -2.018168 0.740122 0.528813 -0.589001   D 0.188695 -0.758872 -0.933237 0.955057   E 0.190794 1.978757 2.605967 0.683509     # Otra manera de borrar las columnas es del df['X'] # Esta funcion es INPLACE df   .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; }  \n  W Y Z     A 2.706850 0.907969 0.503826   B 0.651118 -0.848077 0.605965   C -2.018168 0.528813 -0.589001   D 0.188695 -0.933237 0.955057   E 0.190794 2.605967 0.683509     Obtener los nombres de las columnas y los indices (index): df.columns # nombres de las columnas  Index(['W', 'Y', 'Z'], dtype='object')  df.index # nombres de los indices  Index(['A', 'B', 'C', 'D', 'E'], dtype='object')  Seleccionando Filas y Columnas las dos formas de seleccion principal son:\n DataFrame.loc[etiqueta_fila, etiqueta_columna] \u0026lt;- por etiquetas DataFrame.iloc[indice_fila, indice_columna] \u0026lt;- por indices  # la funcion loc busca por medio de los nombres de los indices y columnas df.loc['A'] # se selecciona todos los valores de la fila 'A'  W 2.706850 Y 0.907969 Z 0.503826 Name: A, dtype: float64  O basado en la posicion (index) en vez de usar la etiqueta\ndf.iloc[2] # Se seleccionan los valores de la fila con indice 2 # recordar que los index empiezan en cero  W -2.018168 Y 0.528813 Z -0.589001 Name: C, dtype: float64  Seleccionar un subconjunto de filas y columnas # Mediante etiquetas # se selecciona el elemento que esta en la fila=B Col=Y df.loc['B','Y'] # con etiquetas  -0.8480769834036315  # Mediante etiquetas # se selecciona un subconjunto de datos que estan entre # filas = A, B Cols= W, Y df.loc[['A','B'],['W','Y']]   .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; }  \n  W Y     A 2.706850 0.907969   B 0.651118 -0.848077     df.loc[['B','A'],['Y','W']]   .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; }  \n  Y W     B -0.848077 0.651118   A 0.907969 2.706850     Seleccion Condicional o Filtros Una característica importante de pandas es la selección condicional usando la notación de corchetes, muy similar a NumPy:\ndf   .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; }  \n  W Y Z     A 2.706850 0.907969 0.503826   B 0.651118 -0.848077 0.605965   C -2.018168 0.528813 -0.589001   D 0.188695 -0.933237 0.955057   E 0.190794 2.605967 0.683509     # Devuelve un dataframe con booleans # segun si se cumple o no la condicion df\u0026gt;0   .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; }  \n  W Y Z     A True True True   B True False True   C False True False   D True False True   E True True True     # Esta operacion solo mostrara los valores del dataframe que cumplen la condicion # los que no cumplen devuelve el valor NaN df[df\u0026gt;0]   .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; }  \n  W Y Z     A 2.706850 0.907969 0.503826   B 0.651118 NaN 0.605965   C NaN 0.528813 NaN   D 0.188695 NaN 0.955057   E 0.190794 2.605967 0.683509     # seleccionar todas las filas donde el valor # que esta en la columna 'W' sea mayor que cero df[df['W']\u0026gt;0]   .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; }  \n  W Y Z     A 2.706850 0.907969 0.503826   B 0.651118 -0.848077 0.605965   D 0.188695 -0.933237 0.955057   E 0.190794 2.605967 0.683509     # Seleccionar las filas donde 'W' sea mayor que cero # y de esas filas escoger los valores de la columna 'Y' df[df['W']\u0026gt;0]['Y']  A 0.907969 B -0.848077 D -0.933237 E 2.605967 Name: Y, dtype: float64  # Seleccionar las filas donde 'W' sea mayor que cero # y de esas filas escoger los valores de las columna 'Y' y 'X' df[df['W']\u0026gt;0][['Y','Z']]   .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; }  \n  Y Z     A 0.907969 0.503826   B -0.848077 0.605965   D -0.933237 0.955057   E 2.605967 0.683509     Para dos condiciones, se usa los booleanos de esta forma\n | en vez de or \u0026amp; en vez de and ~ en vez de not  Por amor a Dios, recuerde usar paréntesis:\n# Seleccionar las filas donde 'W' sea mayor que cero # y tambien donde 'Y' sea mayor que 0.5 df[(df['W']\u0026gt;0) \u0026amp; (df['Y'] \u0026gt; 0.5)]   .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; }  \n  W Y Z     A 2.706850 0.907969 0.503826   E 0.190794 2.605967 0.683509     .query() Busqueda condicional Los terminos de busqueda condicional o filtros se entregan al metodo como tipo \u0026lsquo;string\u0026rsquo;\ndf   .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; }  \n  W Y Z     A 2.706850 0.907969 0.503826   B 0.651118 -0.848077 0.605965   C -2.018168 0.528813 -0.589001   D 0.188695 -0.933237 0.955057   E 0.190794 2.605967 0.683509     # seleccionar todas las filas donde el valor # que esta en la columna 'W' sea mayor que cero #df[df['W']\u0026gt;0] df.query('W\u0026gt;0')   .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; }  \n  W Y Z     A 2.706850 0.907969 0.503826   B 0.651118 -0.848077 0.605965   D 0.188695 -0.933237 0.955057   E 0.190794 2.605967 0.683509     # Seleccionar las filas donde 'W' sea mayor que cero # y de esas filas escoger los valores de la columna 'Y' #df[df['W']\u0026gt;0]['Y'] df.query('W\u0026gt;0')['Y']  A 0.907969 B -0.848077 D -0.933237 E 2.605967 Name: Y, dtype: float64  # Seleccionar las filas donde 'W' sea mayor que cero # y de esas filas escoger los valores de las columna 'Y' y 'X' #df[df['W']\u0026gt;0][['Y','Z']] df.query('W\u0026gt;0')[['Y','Z']]   .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; }  \n  Y Z     A 0.907969 0.503826   B -0.848077 0.605965   D -0.933237 0.955057   E 2.605967 0.683509     Para dos condiciones, puede usar | = or y \u0026amp; = and con paréntesis:\n# Seleccionar las filas donde 'W' sea mayor que cero # y tambien donde 'Y' sea mayor que 0.5 #df[(df['W']\u0026gt;0) \u0026amp; (df['Y'] \u0026gt; 0.5)] df.query('W\u0026gt;0 and Y\u0026gt;0.5')   .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; }  \n  W Y Z     A 2.706850 0.907969 0.503826   E 0.190794 2.605967 0.683509     Cambio de columna de Indexacion Analicemos algunas características más de la indexación, incluido el restablecimiento del índice o el establecimiento de parametros.\ndf   .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; }  \n  W Y Z     A 2.706850 0.907969 0.503826   B 0.651118 -0.848077 0.605965   C -2.018168 0.528813 -0.589001   D 0.188695 -0.933237 0.955057   E 0.190794 2.605967 0.683509     # Reinicializar el indice a su valor por defecto 0,1...n index df = df.reset_index() df   .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; }  \n  index W Y Z     0 A 2.706850 0.907969 0.503826   1 B 0.651118 -0.848077 0.605965   2 C -2.018168 0.528813 -0.589001   3 D 0.188695 -0.933237 0.955057   4 E 0.190794 2.605967 0.683509     newind = 'CA NY WY OR CO'.split() # crear una lista con strings newind  ['CA', 'NY', 'WY', 'OR', 'CO']  # Agregar la lista creaada en el paso anterior al dataframe df['States'] = newind df   .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; }  \n  index W Y Z States     0 A 2.706850 0.907969 0.503826 CA   1 B 0.651118 -0.848077 0.605965 NY   2 C -2.018168 0.528813 -0.589001 WY   3 D 0.188695 -0.933237 0.955057 OR   4 E 0.190794 2.605967 0.683509 CO     # Redefinir la columna states como el indice df.set_index('States')   .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; }  \n  index W Y Z   States         CA A 2.706850 0.907969 0.503826   NY B 0.651118 -0.848077 0.605965   WY C -2.018168 0.528813 -0.589001   OR D 0.188695 -0.933237 0.955057   CO E 0.190794 2.605967 0.683509     # por que no queda establecido el indice? df   .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; }  \n  index W Y Z States     0 A 2.706850 0.907969 0.503826 CA   1 B 0.651118 -0.848077 0.605965 NY   2 C -2.018168 0.528813 -0.589001 WY   3 D 0.188695 -0.933237 0.955057 OR   4 E 0.190794 2.605967 0.683509 CO     # para establecer el indice debe ser una funcion inplace df.set_index('States',inplace=True) #df = df.set_index('States') # otra forma de hacerlo  df   .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; }  \n  index W Y Z   States         CA A 2.706850 0.907969 0.503826   NY B 0.651118 -0.848077 0.605965   WY C -2.018168 0.528813 -0.589001   OR D 0.188695 -0.933237 0.955057   CO E 0.190794 2.605967 0.683509      Groupby (Agrupacion por filas) El método groupby le permite agrupar filas de datos y llamar a funciones agregadas\nimport pandas as pd # Crear dataframe desde un diccionario data = {'Company':['GOOG','GOOG','MSFT','MSFT','FB','FB'], 'Person':['Sam','Charlie','Amy','Vanessa','Carl','Sarah'], 'Sales':[200,120,340,124,243,350]} data  {'Company': ['GOOG', 'GOOG', 'MSFT', 'MSFT', 'FB', 'FB'], 'Person': ['Sam', 'Charlie', 'Amy', 'Vanessa', 'Carl', 'Sarah'], 'Sales': [200, 120, 340, 124, 243, 350]}  #conversion del diccionario a dataframe df = pd.DataFrame(data) df   .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; }  \n  Company Person Sales     0 GOOG Sam 200   1 GOOG Charlie 120   2 MSFT Amy 340   3 MSFT Vanessa 124   4 FB Carl 243   5 FB Sarah 350     Se puede usar el método .groupby() para agrupar filas en función de un nombre de columna. Por ejemplo, vamos a agruparnos a partir de la Compañía. Esto creará un objeto DataFrameGroupBy:\n#agrupar por Company df.groupby('Company')  \u0026lt;pandas.core.groupby.generic.DataFrameGroupBy object at 0x7f059acb5128\u0026gt;  Se puede grabar este objeto en una nueva variable:\nby_comp = df.groupby(\u0026quot;Company\u0026quot;)  utilizar los métodos agregados del objeto:\n# Promedio de ventas por company by_comp.mean()   .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; }  \n  Sales   Company      FB 296.5   GOOG 160.0   MSFT 232.0     # agrupar por compañia y calcular el promedio por cada una df.groupby('Company').mean()   .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; }  \n  Sales   Company      FB 296.5   GOOG 160.0   MSFT 232.0     Más ejemplos de métodos agregados:\n# agrupar por compañia y calcular la desviacion estandard by_comp.std()   .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; }  \n  Sales   Company      FB 75.660426   GOOG 56.568542   MSFT 152.735065     # agrupar por compañia y calcular el minimo by_comp.min()   .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; }  \n  Person Sales   Company       FB Carl 243   GOOG Charlie 120   MSFT Amy 124     # agrupar por compañia y calcular el maximo by_comp.max()   .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; }  \n  Person Sales   Company       FB Sarah 350   GOOG Sam 200   MSFT Vanessa 340     # agrupar por compañia y sumar los elementos que hay excluyendo los NaN by_comp.count()   .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; }  \n  Person Sales   Company       FB 2 2   GOOG 2 2   MSFT 2 2     # Una de las funciones mas usadas para descripcion estadistica de un dataframe # Genera estadísticas descriptivas que resumen la tendencia central, la dispersión y la forma de la distribución de un conjunto de datos, excluyendo los valores `` NaN``. # by_comp.describe(include = 'all') # incluir todo by_comp.describe()   .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead tr th { text-align: left; } .dataframe thead tr:last-of-type th { text-align: right; }  \n   Sales    count mean std min 25% 50% 75% max   Company             FB 2.0 296.5 75.660426 243.0 269.75 296.5 323.25 350.0   GOOG 2.0 160.0 56.568542 120.0 140.00 160.0 180.00 200.0   MSFT 2.0 232.0 152.735065 124.0 178.00 232.0 286.00 340.0     # Una de las funciones mas usadas para descripcion estadistica de un dataframe # Genera estadísticas descriptivas que resumen la tendencia central, la dispersión y la forma de la distribución de un conjunto de datos, excluyendo los valores `` NaN``. # Transponer la descripcion by_comp.describe().transpose()   .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; }  \n  Company FB GOOG MSFT     Sales count 2.000000 2.000000 2.000000   mean 296.500000 160.000000 232.000000   std 75.660426 56.568542 152.735065   min 243.000000 120.000000 124.000000   25% 269.750000 140.000000 178.000000   50% 296.500000 160.000000 232.000000   75% 323.250000 180.000000 286.000000   max 350.000000 200.000000 340.000000     # Descripcion estadistica de los datos de la copmañia GOOG by_comp.describe().transpose()['GOOG']  Sales count 2.000000 mean 160.000000 std 56.568542 min 120.000000 25% 140.000000 50% 160.000000 75% 180.000000 max 200.000000 Name: GOOG, dtype: float64  Pivot Tables La funcionlidad \u0026ldquo;Pivot_table\u0026rdquo; es muy utilizada y popular en las conocidas \u0026ldquo;hojas de cálculo\u0026rdquo; tipo, OpenOffice, LibreOffice, Excel, Lotus, etc. Esta funcionalidad nos permite agrupar, ordenar, calcular datos y manejar datos de una forma muy similar a la que se hace con las hojas de cálculo. mas informacion\nLa principal función del \u0026ldquo;Pivot_table\u0026rdquo; son las agrupaciones de datos a las que se les suelen aplicar funciones matemáticas como sumatorios, promedios, etc\nimport seaborn as sns # importar la libreria seaborn # cargar dataset del titanic titanic = sns.load_dataset('titanic') titanic.head()   .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; }  \n  survived pclass sex age sibsp parch fare embarked class who adult_male deck embark_town alive alone     0 0 3 male 22.0 1 0 7.2500 S Third man True NaN Southampton no False   1 1 1 female 38.0 1 0 71.2833 C First woman False C Cherbourg yes False   2 1 3 female 26.0 0 0 7.9250 S Third woman False NaN Southampton yes True   3 1 1 female 35.0 1 0 53.1000 S First woman False C Southampton yes False   4 0 3 male 35.0 0 0 8.0500 S Third man True NaN Southampton no True     Haciendo el Pivot table a mano para obtener el promedio de personas que sobrevivieron por genero\n# 1. Agrupar por genero # 2. Obtener los sobrevivientes # 3. Calcular el promedio titanic.groupby('sex')[['survived']].mean()   .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; }  \n  survived   sex      female 0.742038   male 0.188908     promedio de cuantos sobrevivieron por genero divididos por clase\n# 1. Agrupar por genero y clase # 2. Obtener los sobrevivientes # 3. Calcular el promedio # 4. Poner el resultado como una tabla (.unstack) titanic.groupby(['sex', 'class'])['survived'].mean().unstack()   .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; }  \n class First Second Third   sex        female 0.968085 0.921053 0.500000   male 0.368852 0.157407 0.135447     Usando Pivot tables\ntitanic.pivot_table('survived', index='sex', columns='class')   .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; }  \n class First Second Third   sex        female 0.968085 0.921053 0.500000   male 0.368852 0.157407 0.135447     titanic.pivot_table('survived', index='sex', columns='class', margins=True)   .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; }  \n class First Second Third All   sex         female 0.968085 0.921053 0.500000 0.742038   male 0.368852 0.157407 0.135447 0.188908   All 0.629630 0.472826 0.242363 0.383838     Otro ejemplo mas simple:\ndata = {'A':['foo','foo','foo','bar','bar','bar'], 'B':['one','one','two','two','one','one'], 'C':['x','y','x','y','x','y'], 'D':[1,3,2,5,4,1]} df = pd.DataFrame(data) df   .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; }  \n  A B C D     0 foo one x 1   1 foo one y 3   2 foo two x 2   3 bar two y 5   4 bar one x 4   5 bar one y 1     # pivot tables df.pivot_table(values='D',index=['A', 'B'],columns=['C'])   .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; }  \n  C x y   A B       bar one 4.0 1.0   two NaN 5.0   foo one 1.0 3.0   two 2.0 NaN      Concatenar, Fusionar y Unir (Concatenating, Merging, Joining) Hay 3 formas principales de combinar DataFrames: concatenar, fusionar y unir.\n# DataFrames de ejemplo para concatenacion import pandas as pd  df1 = pd.DataFrame({'A': ['A0', 'A1', 'A2', 'A3'], 'B': ['B0', 'B1', 'B2', 'B3'], 'C': ['C0', 'C1', 'C2', 'C3'], 'D': ['D0', 'D1', 'D2', 'D3']}, index=[0, 1, 2, 3])  df2 = pd.DataFrame({'A': ['A4', 'A5', 'A6', 'A7'], 'B': ['B4', 'B5', 'B6', 'B7'], 'C': ['C4', 'C5', 'C6', 'C7'], 'D': ['D4', 'D5', 'D6', 'D7']}, index=[4, 5, 6, 7])  df3 = pd.DataFrame({'A': ['A8', 'A9', 'A10', 'A11'], 'B': ['B8', 'B9', 'B10', 'B11'], 'C': ['C8', 'C9', 'C10', 'C11'], 'D': ['D8', 'D9', 'D10', 'D11']}, index=[8, 9, 10, 11])  df1   .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; }  \n  A B C D     0 A0 B0 C0 D0   1 A1 B1 C1 D1   2 A2 B2 C2 D2   3 A3 B3 C3 D3     df2   .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; }  \n  A B C D     4 A4 B4 C4 D4   5 A5 B5 C5 D5   6 A6 B6 C6 D6   7 A7 B7 C7 D7     df3   .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; }  \n  A B C D     8 A8 B8 C8 D8   9 A9 B9 C9 D9   10 A10 B10 C10 D10   11 A11 B11 C11 D11     Concatenacion (Concatenation) La concatenación básicamente combina DataFrames. Tenga en cuenta que las dimensiones deben coincidir a lo largo del eje con el que se está concatenando.\nla concatenacion se hace con dataframes de diferentes indices\nPuede usar .concat() y pasar una lista de DataFrames para concatenar juntos:\n# Concatenar cada dateframe verticalmente, ya que coinciden los nombres de las columnas pd.concat([df1,df2,df3])   .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; }  \n  A B C D     0 A0 B0 C0 D0   1 A1 B1 C1 D1   2 A2 B2 C2 D2   3 A3 B3 C3 D3   4 A4 B4 C4 D4   5 A5 B5 C5 D5   6 A6 B6 C6 D6   7 A7 B7 C7 D7   8 A8 B8 C8 D8   9 A9 B9 C9 D9   10 A10 B10 C10 D10   11 A11 B11 C11 D11     #concatenar dataframe horizontalmente, como no coinciden los index observar lo que ocurre pd.concat([df1,df2,df3],axis=1)   .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; }  \n  A B C D A B C D A B C D     0 A0 B0 C0 D0 NaN NaN NaN NaN NaN NaN NaN NaN   1 A1 B1 C1 D1 NaN NaN NaN NaN NaN NaN NaN NaN   2 A2 B2 C2 D2 NaN NaN NaN NaN NaN NaN NaN NaN   3 A3 B3 C3 D3 NaN NaN NaN NaN NaN NaN NaN NaN   4 NaN NaN NaN NaN A4 B4 C4 D4 NaN NaN NaN NaN   5 NaN NaN NaN NaN A5 B5 C5 D5 NaN NaN NaN NaN   6 NaN NaN NaN NaN A6 B6 C6 D6 NaN NaN NaN NaN   7 NaN NaN NaN NaN A7 B7 C7 D7 NaN NaN NaN NaN   8 NaN NaN NaN NaN NaN NaN NaN NaN A8 B8 C8 D8   9 NaN NaN NaN NaN NaN NaN NaN NaN A9 B9 C9 D9   10 NaN NaN NaN NaN NaN NaN NaN NaN A10 B10 C10 D10   11 NaN NaN NaN NaN NaN NaN NaN NaN A11 B11 C11 D11     Fusion (Merging) La función merge() le permite fusionar DataFrames juntos utilizando una lógica similar a la combinación de Tablas SQL. Por ejemplo:\n# DataFrames de ejemplo para merging left = pd.DataFrame({'key': ['K0', 'K1', 'K2', 'K3'], 'A': ['A0', 'A1', 'A2', 'A3'], 'B': ['B0', 'B1', 'B2', 'B3']}) right = pd.DataFrame({'key': ['K0', 'K1', 'K2', 'K3'], 'C': ['C0', 'C1', 'C2', 'C3'], 'D': ['D0', 'D1', 'D2', 'D3']})  left   .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; }  \n  key A B     0 K0 A0 B0   1 K1 A1 B1   2 K2 A2 B2   3 K3 A3 B3     right   .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; }  \n  key C D     0 K0 C0 D0   1 K1 C1 D1   2 K2 C2 D2   3 K3 C3 D3     # how='inner' utilice la intersección de las claves de ambos marcos, similar a una combinación interna de SQL; # las keys son comunes pd.merge(left,right,how='inner',on='key')   .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; }  \n  key A B C D     0 K0 A0 B0 C0 D0   1 K1 A1 B1 C1 D1   2 K2 A2 B2 C2 D2   3 K3 A3 B3 C3 D3     Un ejemplo mas complicado:  Natural join: para mantener solo las filas que coinciden con los marcos de datos, especifique el argumento how = \u0026lsquo;inner\u0026rsquo;. Full outer join: para mantener todas las filas de ambos dataframe, especifique how = \u0026lsquo;OUTER\u0026rsquo;. Left outer join: para incluir todas las filas de su dataframe x y solo aquellas de y que coincidan, especifique how=‘left’. Right outer join: para incluir todas las filas de su dataframe y y solo aquellas de x que coincidan, especifique how=‘right’.  left = pd.DataFrame({'key1': ['K0', 'K0', 'K1', 'K2'], 'key2': ['K0', 'K1', 'K0', 'K1'], 'A': ['A0', 'A1', 'A2', 'A3'], 'B': ['B0', 'B1', 'B2', 'B3']}) right = pd.DataFrame({'key1': ['K0', 'K1', 'K1', 'K2'], 'key2': ['K0', 'K0', 'K0', 'K0'], 'C': ['C0', 'C1', 'C2', 'C3'], 'D': ['D0', 'D1', 'D2', 'D3']})  left   .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; }  \n  key1 key2 A B     0 K0 K0 A0 B0   1 K0 K1 A1 B1   2 K1 K0 A2 B2   3 K2 K1 A3 B3     right   .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; }  \n  key1 key2 C D     0 K0 K0 C0 D0   1 K1 K0 C1 D1   2 K1 K0 C2 D2   3 K2 K0 C3 D3     # fusionando comparando las mismas claves que tengan comunes pd.merge(left, right, on=['key1', 'key2'])   .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; }  \n  key1 key2 A B C D     0 K0 K0 A0 B0 C0 D0   1 K1 K0 A2 B2 C1 D1   2 K1 K0 A2 B2 C2 D2     # fusionando totalmente las dos tablas con las claves pd.merge(left, right, how='outer', on=['key1', 'key2'])   .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; }  \n  key1 key2 A B C D     0 K0 K0 A0 B0 C0 D0   1 K0 K1 A1 B1 NaN NaN   2 K1 K0 A2 B2 C1 D1   3 K1 K0 A2 B2 C2 D2   4 K2 K1 A3 B3 NaN NaN   5 K2 K0 NaN NaN C3 D3     # fusionando usando las claves de la tabla right pd.merge(left, right, how='right', on=['key1', 'key2'])   .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; }  \n  key1 key2 A B C D     0 K0 K0 A0 B0 C0 D0   1 K1 K0 A2 B2 C1 D1   2 K1 K0 A2 B2 C2 D2   3 K2 K0 NaN NaN C3 D3     # fusionando usando las claves de la tabla left pd.merge(left, right, how='left', on=['key1', 'key2'])   .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; }  \n  key1 key2 A B C D     0 K0 K0 A0 B0 C0 D0   1 K0 K1 A1 B1 NaN NaN   2 K1 K0 A2 B2 C1 D1   3 K1 K0 A2 B2 C2 D2   4 K2 K1 A3 B3 NaN NaN     Unir (Joining) Unir (join) es un método conveniente para combinar las columnas de dos DataFrames potencialmente indexados de forma diferente en un solo DataFrame. Join hace uniones de índices sobre índices o de índices sobre columnas\nleft = pd.DataFrame({'A': ['A0', 'A1', 'A2'], 'B': ['B0', 'B1', 'B2']}, index=['K0', 'K1', 'K2']) right = pd.DataFrame({'C': ['C0', 'C2', 'C3'], 'D': ['D0', 'D2', 'D3']}, index=['K0', 'K2', 'K3'])  left   .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; }  \n  A B     K0 A0 B0   K1 A1 B1   K2 A2 B2     right   .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; }  \n  C D     K0 C0 D0   K2 C2 D2   K3 C3 D3     # unir el dataframe 'right' a el dataframe 'left' left.join(right)   .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; }  \n  A B C D     K0 A0 B0 C0 D0   K1 A1 B1 NaN NaN   K2 A2 B2 C2 D2     # unir el dataframe 'left' a el dataframe 'right' right.join(left)   .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; }  \n  C D A B     K0 C0 D0 A0 B0   K2 C2 D2 A2 B2   K3 C3 D3 NaN NaN     # unir el dataframe 'left' a el dataframe 'rigth' #how = outer left.join(right, how='outer')   .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; }  \n  A B C D     K0 A0 B0 C0 D0   K1 A1 B1 NaN NaN   K2 A2 B2 C2 D2   K3 NaN NaN C3 D3     Datos Categoricos  La busqueda en datos categoricos es mucho mas rapida Ocupan Menos memoria que si los datos estan como string Se pueden tener datos categoricos Ordinales  Mas informacion de datos categoricos:\nhttps://pandas.pydata.org/pandas-docs/stable/user_guide/categorical.html\nCreacion # Creacion de una serie de datos categoricos cate = pd.Series([\u0026quot;manzana\u0026quot;, \u0026quot;banano\u0026quot;, \u0026quot;corozo\u0026quot;, \u0026quot;manzana\u0026quot;,\u0026quot;pera\u0026quot;], dtype=\u0026quot;category\u0026quot;) cate  0 manzana 1 banano 2 corozo 3 manzana 4 pera dtype: category Categories (4, object): [banano, corozo, manzana, pera]  cate.dtypes  CategoricalDtype(categories=['banano', 'corozo', 'manzana', 'pera'], ordered=False)  cate.describe()  count 5 unique 4 top manzana freq 2 dtype: object  # Creando primero los datos y luego convirtiendolos en categoricos df_cate = pd.DataFrame({\u0026quot;Fruta\u0026quot;:[\u0026quot;manzana\u0026quot;, \u0026quot;banano\u0026quot;, \u0026quot;corozo\u0026quot;, \u0026quot;manzana\u0026quot;,\u0026quot;pera\u0026quot;]}) df_cate   .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; }  \n  Fruta     0 manzana   1 banano   2 corozo   3 manzana   4 pera     # Observar los tipos de datos en el data frame df_cate.dtypes  Fruta object dtype: object  df_cate[\u0026quot;Fruta2\u0026quot;] = df_cate[\u0026quot;Fruta\u0026quot;].astype('category') df_cate   .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; }  \n  Fruta Fruta2     0 manzana manzana   1 banano banano   2 corozo corozo   3 manzana manzana   4 pera pera     # Observar los tipos de datos en el dataframe df_cate.dtypes  Fruta object Fruta2 category dtype: object  # Crear los datos categoricos desde a declaracion de los datos df_cate = pd.DataFrame({'A': list('abca'), 'B': list('bccd')}, dtype=\u0026quot;category\u0026quot;) df_cate   .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; }  \n  A B     0 a b   1 b c   2 c c   3 a d     df_cate.dtypes  A category B category dtype: object  df_cate.describe()   .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; }  \n  A B     count 4 4   unique 3 3   top a c   freq 2 2     Categoricos Ordinales from pandas.api.types import CategoricalDtype  # creacion de dataframe con datos df_cate = pd.DataFrame({'A': list('abca'), 'B': list('bccd')}) df_cate   .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; }  \n  A B     0 a b   1 b c   2 c c   3 a d     df_cate.dtypes  A object B object dtype: object  # Definicion de los tipos de datos y que estan en orden df_cate[\u0026quot;A\u0026quot;] = df_cate[\u0026quot;A\u0026quot;].astype(CategoricalDtype(['a','b','c','d'], ordered=True)) df_cate   .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; }  \n  A B     0 a b   1 b c   2 c c   3 a d     df_cate[\u0026quot;A\u0026quot;]  0 a 1 b 2 c 3 a Name: A, dtype: category Categories (4, object): [a \u0026lt; b \u0026lt; c \u0026lt; d]  # Los datos no tienen que ser strngs para qeu sean categoricos s = pd.Series([1, 2, 3, 1], dtype=\u0026quot;category\u0026quot;) s  0 1 1 2 2 3 3 1 dtype: category Categories (3, int64): [1, 2, 3]  s = s.cat.set_categories([2, 3, 1], ordered=True) s  0 1 1 2 2 3 3 1 dtype: category Categories (3, int64): [2 \u0026lt; 3 \u0026lt; 1]   Datos Faltantes (Missing data) Métodos para Manejar datos faltantes en pandas:\n# Declaracion de data frame con algunos datos faltantes # NaN = Not a Number df = pd.DataFrame({'A':[1,2,np.nan], 'B':[5,np.nan,np.nan], 'C':[1,2,3]}) df   .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; }  \n  A B C     0 1.0 5.0 1   1 2.0 NaN 2   2 NaN NaN 3     Detectar si Faltan datos # verificar cuales valores son NaN o nulos (Null) df.isna()   .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; }  \n  A B C     0 False False False   1 False True False   2 True True False     # verificar cuales valores son na # el metodo .isnull() es igual a .isna() df.isnull()   .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; }  \n  A B C     0 False False False   1 False True False   2 True True False     # Verificar si hay datos faltantes por columna df.isnull().any()  A True B True C False dtype: bool  Numero de datos faltantes Calcular el numero de datos nulos que hay por columna\n# Numero de datos faltantes por columna df.isnull().sum()  A 1 B 2 C 0 dtype: int64  Eliminar datos Faltantes # Eliminar todas las filas que tengan datos faltantes df.dropna(axis=0) # cuando son filas no es neceario escribir axis=0   .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; }  \n  A B C     0 1.0 5.0 1     # Eliminar todas las columnas que tengan datos faltantes df.dropna(axis=1)   .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; }  \n  C     0 1   1 2   2 3     # eliminar las filas que tengas 2 o mas valores NaN df.dropna(thresh=2)   .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; }  \n  A B C     0 1.0 5.0 1   1 2.0 NaN 2     Reemplazar los datos faltantes # Llenar los datos faltantes con el dato que nos interese df.fillna(value='FILL VALUE') # llenar los espacios con un string # puede ser una palabra, numero , etc   .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; }  \n  A B C     0 1 5 1   1 2 FILL VALUE 2   2 FILL VALUE FILL VALUE 3     # Llenar los datos faltantes con el dato que nos interese df.fillna(value=99) # llenar los espacios con un numero   .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; }  \n  A B C     0 1.0 5.0 1   1 2.0 99.0 2   2 99.0 99.0 3     # Llenar los datos faltantes con el promedio de esa columna df['A'].fillna(value=df['A'].mean())  0 1.0 1 2.0 2 1.5 Name: A, dtype: float64  # Llenar los datos faltantes con el promedio de cada columna df.fillna(value=df.mean())   .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; }  \n  A B C     0 1.0 5.0 1   1 2.0 5.0 2   2 1.5 5.0 3     Datos unicos (Unique Values) import pandas as pd # crear un dataframe df = pd.DataFrame({'col1':[1,2,3,4],'col2':[444,555,666,444],'col3':['abc','def','ghi','xyz']}) df.head() # solamente mostrar los primeros elementos del dataframe   .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; }  \n  col1 col2 col3     0 1 444 abc   1 2 555 def   2 3 666 ghi   3 4 444 xyz     # valores unicos de la columna col2 df['col2'].unique()  array([444, 555, 666])  # Numero de valores unicos en el dataframe df['col2'].nunique()  3  # contar cuanto se repiten cada uno de los valores df['col2'].value_counts()  444 2 555 1 666 1 Name: col2, dtype: int64  Datos Duplicados Se puede borrar los registros que son exactamente iguales en todos los valores de las columnas\nimport pandas as pd  datos = {'name': ['James', 'Jason', 'Rogers', 'Jason'], 'age': [18, 20, 22, 20], 'job': ['Assistant', 'Manager', 'Clerk', 'Manager']} df_dup = pd.DataFrame(datos) df_dup   .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; }  \n  name age job     0 James 18 Assistant   1 Jason 20 Manager   2 Rogers 22 Clerk   3 Jason 20 Manager     # Metodo para detectar los datos duplicados # me sirve para ver si existen registros duplicados df_dup.duplicated()  0 False 1 False 2 False 3 True dtype: bool  # Contar cuantos datos duplicados existen df_dup.duplicated().sum()  1  # Para remover los datos duplicados df_dup.drop_duplicates()   .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; }  \n  name age job     0 James 18 Assistant   1 Jason 20 Manager   2 Rogers 22 Clerk     # El metodo drop_duplicates entrega el data frame sin duplicados # Pero la funcion no es inplace, osea que el dataframe original sigue igual df_dup   .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; }  \n  name age job     0 James 18 Assistant   1 Jason 20 Manager   2 Rogers 22 Clerk   3 Jason 20 Manager     df_dup.drop_duplicates(inplace=True) df_dup   .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; }  \n  name age job     0 James 18 Assistant   1 Jason 20 Manager   2 Rogers 22 Clerk     Duplicados por Columna Se pueden borrar los valores que se repitan solamente verificando la columna\nframe_datos = {'name': ['James', 'Jason', 'Rogers', 'Jason'], 'age': [18, 20, 22, 21], 'job': ['Assistant', 'Manager', 'Clerk', 'Employee']} df_dup = pd.DataFrame(frame_datos) df_dup   .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; }  \n  name age job     0 James 18 Assistant   1 Jason 20 Manager   2 Rogers 22 Clerk   3 Jason 21 Employee     el valor de jason esta duplicado en la columna name\n# recordar que estas funciones no son inplace df_dup.drop_duplicates(['name'])   .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; }  \n  name age job     0 James 18 Assistant   1 Jason 20 Manager   2 Rogers 22 Clerk     Outliers Una de las formas de eliminar los aoutliers es identificando cual sera el rango en el que queremos nuestros datos y limitar los datos entre ese rango\nimport seaborn as sns # importar la libreria seaborn # cargar dataset del titanic titanic = sns.load_dataset('titanic') titanic.head()   .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; }  \n  survived pclass sex age sibsp parch fare embarked class who adult_male deck embark_town alive alone     0 0 3 male 22.0 1 0 7.2500 S Third man True NaN Southampton no False   1 1 1 female 38.0 1 0 71.2833 C First woman False C Cherbourg yes False   2 1 3 female 26.0 0 0 7.9250 S Third woman False NaN Southampton yes True   3 1 1 female 35.0 1 0 53.1000 S First woman False C Southampton yes False   4 0 3 male 35.0 0 0 8.0500 S Third man True NaN Southampton no True     edad = titanic['age'] edad  0 22.0 1 38.0 2 26.0 3 35.0 4 35.0 5 NaN 6 54.0 7 2.0 8 27.0 9 14.0 10 4.0 11 58.0 12 20.0 13 39.0 14 14.0 15 55.0 16 2.0 17 NaN 18 31.0 19 NaN 20 35.0 21 34.0 22 15.0 23 28.0 24 8.0 25 38.0 26 NaN 27 19.0 28 NaN 29 NaN ... 861 21.0 862 48.0 863 NaN 864 24.0 865 42.0 866 27.0 867 31.0 868 NaN 869 4.0 870 26.0 871 47.0 872 33.0 873 47.0 874 28.0 875 15.0 876 20.0 877 19.0 878 NaN 879 56.0 880 25.0 881 33.0 882 22.0 883 28.0 884 25.0 885 39.0 886 27.0 887 19.0 888 NaN 889 26.0 890 32.0 Name: age, Length: 891, dtype: float64  # Cual es la edad maxima edad.max()  80.0  # Cual es la edad Minima edad.min()  0.42  Solo por hacer el ejercicio se delimitaran las edades entre 1 y 70 años\nesto se puede hacer con el metodo .clip(lower = Valor mas bajo, upper = valor mas alto)\nedad = edad.clip(lower=1,upper = 70)  edad.max()  70.0  edad.min()  1.0  Tablas de Contingencia (two way tables) # Crear Datos raw_data = {'regiment': ['Nighthawks', 'Nighthawks', 'Nighthawks', 'Nighthawks', 'Dragoons', 'Dragoons', 'Dragoons', 'Dragoons', 'Scouts', 'Scouts', 'Scouts', 'Scouts'], 'company': ['infantry', 'infantry', 'cavalry', 'cavalry', 'infantry', 'infantry', 'cavalry', 'cavalry','infantry', 'infantry', 'cavalry', 'cavalry'], 'experience': ['veteran', 'rookie', 'veteran', 'rookie', 'veteran', 'rookie', 'veteran', 'rookie','veteran', 'rookie', 'veteran', 'rookie'], 'name': ['Miller', 'Jacobson', 'Ali', 'Milner', 'Cooze', 'Jacon', 'Ryaner', 'Sone', 'Sloan', 'Piger', 'Riani', 'Ali'], 'preTestScore': [4, 24, 31, 2, 3, 4, 24, 31, 2, 3, 2, 3], 'postTestScore': [25, 94, 57, 62, 70, 25, 94, 57, 62, 70, 62, 70]} df = pd.DataFrame(raw_data, columns = ['regiment', 'company', 'experience', 'name', 'preTestScore', 'postTestScore']) df   .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; }  \n  regiment company experience name preTestScore postTestScore     0 Nighthawks infantry veteran Miller 4 25   1 Nighthawks infantry rookie Jacobson 24 94   2 Nighthawks cavalry veteran Ali 31 57   3 Nighthawks cavalry rookie Milner 2 62   4 Dragoons infantry veteran Cooze 3 70   5 Dragoons infantry rookie Jacon 4 25   6 Dragoons cavalry veteran Ryaner 24 94   7 Dragoons cavalry rookie Sone 31 57   8 Scouts infantry veteran Sloan 2 62   9 Scouts infantry rookie Piger 3 70   10 Scouts cavalry veteran Riani 2 62   11 Scouts cavalry rookie Ali 3 70     # Tabla de contingencia por compañía y regimiento pd.crosstab(df['regiment'], df['company'], margins=True)   .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; }  \n company cavalry infantry All   regiment        Dragoons 2 2 4   Nighthawks 2 2 4   Scouts 2 2 4   All 6 6 12     # Tabla de contingencia de compañia y experiencia por regimiento pd.crosstab([df['company'], df['experience']], df['regiment'], margins=True)   .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; }  \n  regiment Dragoons Nighthawks Scouts All   company experience         cavalry rookie 1 1 1 3   veteran 1 1 1 3   infantry rookie 1 1 1 3   veteran 1 1 1 3   All  4 4 4 12     Metodos y Funciones en Pandas Todos los metodos de los dataframe de pandas se pueden encontrar en:\nhttp://pandas.pydata.org/pandas-docs/stable/reference/frame.html\nimport pandas as pd # crear un dataframe df = pd.DataFrame({'col1':[1,2,3,4],'col2':[444,555,666,444], 'col3':['mama ',' papa',' HIJO ','HiJa']}) df.head() # solamente mostrar los primeros elementos del dataframe   .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; }  \n  col1 col2 col3     0 1 444 mama   1 2 555 papa   2 3 666 HIJO   3 4 444 HiJa     Metodos Basicos Pandas Ejemplos simples de los metodos de los dataframe de pandas\nPara informacion completa de los metodos de computacion y estadisticos ver:\nhttp://pandas.pydata.org/pandas-docs/stable/reference/frame.html#computations-descriptive-stats\n# Suma total de cada Columna, si es categorico no lo suma df.sum()  col1 10 col2 2109 col3 mama papa HIJO HiJa dtype: object  # Metodo en una sola columna df['col1'].sum()  10  # Metodo en varias columnas df[['col1','col2']].sum()  col1 10 col2 2109 dtype: int64  # Valor Minimo cada Columna df.min()  col1 1 col2 444 col3 HIJO dtype: object  # Valor Maximo cada Columna df.max()  col1 4 col2 666 col3 mama dtype: object  Metodos de Informacion General # Cargar la base de datos 'mpg' de la libreria seaborn import seaborn as sns # los datos se cargan en un dataframe data = sns.load_dataset('mpg')  data.info()  \u0026lt;class 'pandas.core.frame.DataFrame'\u0026gt; RangeIndex: 398 entries, 0 to 397 Data columns (total 9 columns): mpg 398 non-null float64 cylinders 398 non-null int64 displacement 398 non-null float64 horsepower 392 non-null float64 weight 398 non-null int64 acceleration 398 non-null float64 model_year 398 non-null int64 origin 398 non-null object name 398 non-null object dtypes: float64(4), int64(3), object(2) memory usage: 28.1+ KB  data.head()   .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; }  \n  mpg cylinders displacement horsepower weight acceleration model_year origin name     0 18.0 8 307.0 130.0 3504 12.0 70 usa chevrolet chevelle malibu   1 15.0 8 350.0 165.0 3693 11.5 70 usa buick skylark 320   2 18.0 8 318.0 150.0 3436 11.0 70 usa plymouth satellite   3 16.0 8 304.0 150.0 3433 12.0 70 usa amc rebel sst   4 17.0 8 302.0 140.0 3449 10.5 70 usa ford torino     data.describe() # ver datos estadisticos de las columnas numericas   .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; }  \n  mpg cylinders displacement horsepower weight acceleration model_year     count 398.000000 398.000000 398.000000 392.000000 398.000000 398.000000 398.000000   mean 23.514573 5.454774 193.425879 104.469388 2970.424623 15.568090 76.010050   std 7.815984 1.701004 104.269838 38.491160 846.841774 2.757689 3.697627   min 9.000000 3.000000 68.000000 46.000000 1613.000000 8.000000 70.000000   25% 17.500000 4.000000 104.250000 75.000000 2223.750000 13.825000 73.000000   50% 23.000000 4.000000 148.500000 93.500000 2803.500000 15.500000 76.000000   75% 29.000000 8.000000 262.000000 126.000000 3608.000000 17.175000 79.000000   max 46.600000 8.000000 455.000000 230.000000 5140.000000 24.800000 82.000000     data.describe(include = 'all') # ver todos los datos incluidos los categoricos   .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; }  \n  mpg cylinders displacement horsepower weight acceleration model_year origin name     count 398.000000 398.000000 398.000000 392.000000 398.000000 398.000000 398.000000 398 398   unique NaN NaN NaN NaN NaN NaN NaN 3 305   top NaN NaN NaN NaN NaN NaN NaN usa ford pinto   freq NaN NaN NaN NaN NaN NaN NaN 249 6   mean 23.514573 5.454774 193.425879 104.469388 2970.424623 15.568090 76.010050 NaN NaN   std 7.815984 1.701004 104.269838 38.491160 846.841774 2.757689 3.697627 NaN NaN   min 9.000000 3.000000 68.000000 46.000000 1613.000000 8.000000 70.000000 NaN NaN   25% 17.500000 4.000000 104.250000 75.000000 2223.750000 13.825000 73.000000 NaN NaN   50% 23.000000 4.000000 148.500000 93.500000 2803.500000 15.500000 76.000000 NaN NaN   75% 29.000000 8.000000 262.000000 126.000000 3608.000000 17.175000 79.000000 NaN NaN   max 46.600000 8.000000 455.000000 230.000000 5140.000000 24.800000 82.000000 NaN NaN     # Descripcion de una sola columna data['cylinders'].describe()  count 398.000000 mean 5.454774 std 1.701004 min 3.000000 25% 4.000000 50% 4.000000 75% 8.000000 max 8.000000 Name: cylinders, dtype: float64  data['mpg'].describe()  count 398.000000 mean 23.514573 std 7.815984 min 9.000000 25% 17.500000 50% 23.000000 75% 29.000000 max 46.600000 Name: mpg, dtype: float64  data['cylinders'].dtypes  dtype('int64')  Estadistica Descriptiva Metodos de Estadistica Descriptiva, de medida central, simetria, momentos, etc. para mas informacion: http://pandas.pydata.org/pandas-docs/stable/reference/frame.html#computations-descriptive-stats\nLos calculos de las medidas estadisticas se hacen siempre en columnas, es algo predeterminado, si se quiere hacer por filas, se debe especificar dentro de los metodos el parametro axis=1.\nEjemplo:\n Calculo de la media por columnas (predeterminado) = df.mean() Calculo de la media por filas = df.mean(axis=1)  Esto funciona con los otros tipos de medidas estadisticas\n# Se tomara la columna 'mpg' para realizar los calculos X = data['mpg'] type(X)  pandas.core.series.Series  Medidas de centralizacion estas funciones se aplican sobre un dataframe de pandas\nMedia # Media Aritmetica X.mean()  23.514572864321615  # Media aritmetica en un dataframe data.mean()  mpg 23.514573 cylinders 5.454774 displacement 193.425879 horsepower 104.469388 weight 2970.424623 acceleration 15.568090 model_year 76.010050 dtype: float64  Mediana # Mediana X.median()  23.0  # Mediana en un dataframe data.median()  mpg 23.0 cylinders 4.0 displacement 148.5 horsepower 93.5 weight 2803.5 acceleration 15.5 model_year 76.0 dtype: float64  Maximo y Minimo # Maximo X.max()  46.6  # Maximo en un dataframe data.max()  mpg 46.6 cylinders 8 displacement 455 horsepower 230 weight 5140 acceleration 24.8 model_year 82 origin usa name vw rabbit custom dtype: object  # Minimo X.min()  9.0  # Minimo en un dataframe data.min()  mpg 9 cylinders 3 displacement 68 horsepower 46 weight 1613 acceleration 8 model_year 70 origin europe name amc ambassador brougham dtype: object  Moda # Moda X.mode()  0 13.0 dtype: float64  # Moda en un dataframe data.mode()   .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; }  \n  mpg cylinders displacement horsepower weight acceleration model_year origin name     0 13.0 4.0 97.0 150.0 1985 14.5 73.0 usa ford pinto   1 NaN NaN NaN NaN 2130 NaN NaN NaN NaN     Cuartiles # Valores de los cuartiles X.quantile([0, .25, .5, .75, 1])  0.00 9.0 0.25 17.5 0.50 23.0 0.75 29.0 1.00 46.6 Name: mpg, dtype: float64  # Valores de los cuartiles en un dataframe data.quantile([0, .25, .5, .75, 1])   .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; }  \n  mpg cylinders displacement horsepower weight acceleration model_year     0.00 9.0 3.0 68.00 46.0 1613.00 8.000 70.0   0.25 17.5 4.0 104.25 75.0 2223.75 13.825 73.0   0.50 23.0 4.0 148.50 93.5 2803.50 15.500 76.0   0.75 29.0 8.0 262.00 126.0 3608.00 17.175 79.0   1.00 46.6 8.0 455.00 230.0 5140.00 24.800 82.0     Medidas de dispersion Varianza # Varianza X.var() #unbiased Normalized by N-1 by default.  61.089610774274405  # Varianza en un dataframe data.var()  mpg 61.089611 cylinders 2.893415 displacement 10872.199152 horsepower 1481.569393 weight 717140.990526 acceleration 7.604848 model_year 13.672443 dtype: float64  Desviacion Estandard # Desviacion tipica o desviacion estandard X.std()  7.815984312565782  # Desviacion tipica o desviacion estandard en un dataframe data.std()  mpg 7.815984 cylinders 1.701004 displacement 104.269838 horsepower 38.491160 weight 846.841774 acceleration 2.757689 model_year 3.697627 dtype: float64  Coeficiente de Variacion # Coeficiente de Variacion X.std()/X.mean()  0.3323889554645019  Medidas de Asimetria Asimetria de Fisher (skewness) La asimetría es la medida que indica la simetría de la distribución de una variable respecto a la media aritmética, sin necesidad de hacer la representación gráfica. Los coeficientes de asimetría indican si hay el mismo número de elementos a izquierda y derecha de la media.\nExisten tres tipos de curva de distribución según su asimetría:\n Asimetría negativa: la cola de la distribución se alarga para valores inferiores a la media. Simétrica: hay el mismo número de elementos a izquierda y derecha de la media. En este caso, coinciden la media, la mediana y la moda. La distribución se adapta a la forma de la campana de Gauss, o distribución normal. Asimetría positiva: la cola de la distribución se alarga para valores superiores a la media.  #unbiased skew, Normalized by N-1 X.skew()  0.45706634399491913  #unbiased skew, Normalized by N-1 en un dataframe data.skew()  mpg 0.457066 cylinders 0.526922 displacement 0.719645 horsepower 1.087326 weight 0.531063 acceleration 0.278777 model_year 0.011535 dtype: float64  Curtosis Esta medida determina el grado de concentración que presentan los valores en la región central de la distribución. Por medio del Coeficiente de Curtosis, podemos identificar si existe una gran concentración de valores (Leptocúrtica), una concentración normal (Mesocúrtica) ó una baja concentración (Platicúrtica). # unbiased kurtosis over requested axis using Fisher's definition X.kurtosis()  -0.5107812652123154  # unbiased kurtosis over requested axis using Fisher's definition # en un dataframe data.kurtosis()  mpg -0.510781 cylinders -1.376662 displacement -0.746597 horsepower 0.696947 weight -0.785529 acceleration 0.419497 model_year -1.181232 dtype: float64  Covarianza Entre Series s1 = pd.Series(np.random.randn(1000)) s2 = pd.Series(np.random.randn(1000))  s1.cov(s2)  -0.028354705528775087  # Numpy np.cov(s1,s2)  array([[ 1.09020966, -0.02835471], [-0.02835471, 1.07417272]])  Dataframe frame = pd.DataFrame(np.random.randn(1000, 5),columns=['a', 'b', 'c', 'd', 'e']) frame.head()   .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; }  \n  a b c d e     0 0.576828 1.814651 -0.261503 -0.254697 0.974428   1 0.030171 0.605864 0.079012 -0.172524 3.052445   2 -0.842097 -0.351725 0.628591 0.878268 -0.055329   3 -0.273158 0.049033 -0.301981 0.562631 -1.241831   4 -0.305087 0.474497 0.648280 -0.026315 0.792715     frame.cov()   .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; }  \n  a b c d e     a 1.076575 0.000666 -0.039799 -0.002462 -0.043501   b 0.000666 1.016370 -0.015872 -0.002416 -0.021047   c -0.039799 -0.015872 1.036459 -0.058312 -0.057863   d -0.002462 -0.002416 -0.058312 1.000640 0.004340   e -0.043501 -0.021047 -0.057863 0.004340 0.952529     # Con Numpy np.cov(frame)  array([[ 0.7710721 , 0.51639576, -0.42821644, ..., -0.23771807, -0.63698462, 0.44026319], [ 0.51639576, 1.78416315, -0.17363496, ..., -1.00370658, -1.14416467, 0.59572484], [-0.42821644, -0.17363496, 0.49727435, ..., 0.08767001, 0.33921621, -0.06352345], ..., [-0.23771807, -1.00370658, 0.08767001, ..., 0.64880976, 0.65597959, -0.23257827], [-0.63698462, -1.14416467, 0.33921621, ..., 0.65597959, 0.90660696, -0.45814778], [ 0.44026319, 0.59572484, -0.06352345, ..., -0.23257827, -0.45814778, 0.46902464]])  Correlacion Metodos:\n pearson (predeterminado) kendall spearman  # Creacion de dataframe con datos aleatorios frame = pd.DataFrame(np.random.randn(1000, 5), columns=['a', 'b', 'c', 'd', 'e']) frame.head()   .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; }  \n  a b c d e     0 0.302665 1.693723 -1.706086 -1.159119 -0.134841   1 0.390528 0.166905 0.184502 0.807706 0.072960   2 0.638787 0.329646 -0.497104 -0.754070 -0.943406   3 0.484752 -0.116773 1.901755 0.238127 1.996652   4 -0.993263 0.196800 -1.136645 0.000366 1.025984     Entre Series frame['a'].corr(frame['b']) # Pearson que es el predeterminado  -0.052592953776030495  frame['a'].corr(frame['b'], method='spearman') # Metodo spearman  -0.04775690375690376  frame['a'].corr(frame['b'], method='kendall') # Metodo Kendall  -0.03213613613613614  # Con Numpy se realiza el coefficiente de Pearson # realiza la correlacion entre dos vectores np.corrcoef(frame['a'],frame['b'])  array([[ 1. , -0.05259295], [-0.05259295, 1. ]])  Dataframe frame.corr()   .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; }  \n  a b c d e     a 1.000000 -0.052593 -0.039170 0.001333 -0.001645   b -0.052593 1.000000 0.084488 0.007218 0.009969   c -0.039170 0.084488 1.000000 0.080168 0.006809   d 0.001333 0.007218 0.080168 1.000000 -0.039776   e -0.001645 0.009969 0.006809 -0.039776 1.000000     # con Numpy np.corrcoef(frame)  array([[ 1. , -0.58501996, 0.55616525, ..., 0.26806025, -0.35940809, -0.00452158], [-0.58501996, 1. , -0.3400534 , ..., 0.11257458, -0.37590609, -0.58877942], [ 0.55616525, -0.3400534 , 1. , ..., 0.70442968, 0.13326316, -0.19220235], ..., [ 0.26806025, 0.11257458, 0.70442968, ..., 1. , 0.19271014, -0.79265039], [-0.35940809, -0.37590609, 0.13326316, ..., 0.19271014, 1. , 0.14871875], [-0.00452158, -0.58877942, -0.19220235, ..., -0.79265039, 0.14871875, 1. ]])  Funciones Agregadas Para aplicar una o mas funciones en cada columna de los dataframe\ndf.agg(['sum', 'min'])   .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; }  \n  col1 col2 col3     sum 10 2109 mama papa HIJO HiJa   min 1 444 HIJO     # Aplicar los metodos en columnas especificas df[['col1','col2']].agg(['sum', 'min'])   .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; }  \n  col1 col2     sum 10 2109   min 1 444     Aplicando Funciones a cada elemento #definicion de funcion def times2(x): return x*2  # es mas o menos lo que hace la funcion map # aplicar la funcion times2 a cada elemento de la col1 de dataframe df df['col1'].apply(times2)  0 2 1 4 2 6 3 8 Name: col1, dtype: int64  df['col3'].apply(len) #longitud de cada uno de los datos de la col3  0 7 1 6 2 9 3 4 Name: col3, dtype: int64  df['col1'].sum() #sumatoria total de los elementos de la col1  10  (Sorting) Ordenar un DataFrame: df   .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; }  \n  col1 col2 col3     0 1 444 mama   1 2 555 papa   2 3 666 HIJO   3 4 444 HiJa     # ordenar el dataframe de menor a mayor basado en col2 df.sort_values(by='col2') #inplace=False por default   .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; }  \n  col1 col2 col3     0 1 444 mama   3 4 444 HiJa   1 2 555 papa   2 3 666 HIJO     df.sort_values(by='col2',ascending=False)   .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; }  \n  col1 col2 col3     2 3 666 HIJO   1 2 555 papa   0 1 444 mama   3 4 444 HiJa     Metodos de strings Los metodos de los strings se pueden usar en pandas de forma vectorizada\nhttps://pandas.pydata.org/pandas-docs/stable/getting_started/basics.html#vectorized-string-methods\nLos metodos vectorizados de los strings son:\nhttps://pandas.pydata.org/pandas-docs/stable/user_guide/text.html#text-string-methods\nNota: Recordar que la mayoria de los metodos NO son inplace\ndf   .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; }  \n  col1 col2 col3     0 1 444 mama   1 2 555 papa   2 3 666 HIJO   3 4 444 HiJa     # Estos Metodos solo funcionan por columna (Series) # se puede verificar el error #df.str.lower()  Algunos ejemplos de metodos en strings en las columnas\n# convertir strings en minuscula df['col3'].str.lower()  0 mama 1 papa 2 hijo 3 hija Name: col3, dtype: object  # convertir strings en Mayuscula df['col3'].str.upper()  0 MAMA 1 PAPA 2 HIJO 3 HIJA Name: col3, dtype: object  # Eliminar los espacios de los strings df['col3'].str.strip()  0 mama 1 papa 2 HIJO 3 HiJa Name: col3, dtype: object  Transformacion de Variables  Crear variables Dummy: convertir de categoría a númerica Discretización o Binning: convertir de número a categoría  Columnas Dummy Convertir variables categoricas a numericas\n# crear datos categoricos raw_data = {'first_name': ['Jason', 'Molly', 'Tina', 'Jake', 'Amy'], 'last_name': ['Miller', 'Jacobson', 'Ali', 'Milner', 'Cooze'], 'sex': ['male', 'female', 'male', 'female', 'female']} df = pd.DataFrame(raw_data, columns = ['first_name', 'last_name', 'sex']) df   .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; }  \n  first_name last_name sex     0 Jason Miller male   1 Molly Jacobson female   2 Tina Ali male   3 Jake Milner female   4 Amy Cooze female     # Crear un set de variables dummy para la columna sex df_sex = pd.get_dummies(df['sex']) df_sex   .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; }  \n  female male     0 0 1   1 1 0   2 0 1   3 1 0   4 1 0     # unir los dos dataframes df_new = df.join(df_sex) df_new   .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; }  \n  first_name last_name sex female male     0 Jason Miller male 0 1   1 Molly Jacobson female 1 0   2 Tina Ali male 0 1   3 Jake Milner female 1 0   4 Amy Cooze female 1 0     Discretización o Binning Conversion de Numerica a Categorica\nhttps://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.cut.html\n# dividir los datos en 3 rangos iguales (categorias) pd.cut(np.array([3.5, 2.8, 1, 5, 3, 4, 0, 4.4, 2, 3]), 3)  [(3.333, 5.0], (1.667, 3.333], (-0.005, 1.667], (3.333, 5.0], (1.667, 3.333], (3.333, 5.0], (-0.005, 1.667], (3.333, 5.0], (1.667, 3.333], (1.667, 3.333]] Categories (3, interval[float64]): [(-0.005, 1.667] \u0026lt; (1.667, 3.333] \u0026lt; (3.333, 5.0]]  # Asignar etiquetas ordenadas pd.cut(np.array([3.5, 2.8, 1, 5, 3, 4,0, 4.4, 2, 3]), 3, labels=[\u0026quot;Malo\u0026quot;, \u0026quot;Regular\u0026quot;, \u0026quot;Bien\u0026quot;])  [Bien, Regular, Malo, Bien, Regular, Bien, Malo, Bien, Regular, Regular] Categories (3, object): [Malo \u0026lt; Regular \u0026lt; Bien]  pd.cut(np.array([2, 4 , 10 , 35 , 25 , 60 , 23, 14]), 3, labels=[\u0026quot;Niño\u0026quot;, \u0026quot;Adolescente\u0026quot;, \u0026quot;Adulto\u0026quot;])  [Niño, Niño, Niño, Adolescente, Adolescente, Adulto, Adolescente, Niño] Categories (3, object): [Niño \u0026lt; Adolescente \u0026lt; Adulto]   Leer y guardar Datos (Data Input and Output) Pandas puede leer una variedad de tipos de archivos usando sus métodos pd.read_ mas informacion\n CSV Excel Json Html SQL  CSV File (comma-separated values) CSV Input # Leer archivos separados por comas, extension .csv df1 = pd.read_csv('./Data/supermarkets.csv') df1   .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; }  \n  ID Address City State Country Name Employees     0 1 3666 21st St San Francisco CA 94114 USA Madeira 8   1 2 735 Dolores St San Francisco CA 94119 USA Bready Shop 15   2 3 332 Hill St San Francisco California 94114 USA Super River 25   3 4 3995 23rd St San Francisco CA 94114 USA Ben's Shop 10   4 5 1056 Sanchez St San Francisco California USA Sanchez 12   5 6 551 Alvarado St San Francisco CA 94114 USA Richvalley 20     # dimensiones del dataframe df1.shape  (6, 7)  # conocer los tipos de datos de cada columna df1.dtypes  ID int64 Address object City object State object Country object Name object Employees int64 dtype: object  # Tambien se puede cambiar el valor de la columna index # definir la columna ID como el index df1.set_index(\u0026quot;ID\u0026quot;,inplace=True) # se debe especificar que sea inplace df1   .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; }  \n  Address City State Country Name Employees   ID           1 3666 21st St San Francisco CA 94114 USA Madeira 8   2 735 Dolores St San Francisco CA 94119 USA Bready Shop 15   3 332 Hill St San Francisco California 94114 USA Super River 25   4 3995 23rd St San Francisco CA 94114 USA Ben's Shop 10   5 1056 Sanchez St San Francisco California USA Sanchez 12   6 551 Alvarado St San Francisco CA 94114 USA Richvalley 20     # hare un cambio y luego lo grabare # multiplicar cada elemento de la columna employees por 2 df1['Employees']= df1['Employees'].apply(lambda x: x*2) df1   .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; }  \n  Address City State Country Name Employees   ID           1 3666 21st St San Francisco CA 94114 USA Madeira 16   2 735 Dolores St San Francisco CA 94119 USA Bready Shop 30   3 332 Hill St San Francisco California 94114 USA Super River 50   4 3995 23rd St San Francisco CA 94114 USA Ben's Shop 20   5 1056 Sanchez St San Francisco California USA Sanchez 24   6 551 Alvarado St San Francisco CA 94114 USA Richvalley 40     CSV Output # Grabar el dataframe como archivo separado por comas df1.to_csv('./Data/example_out.csv',index=False)  Archivo de texto separado por comas # Leer archivos separados por comas, extension .txt o sin extension df1 = pd.read_csv(\u0026quot;./Data/supermarkets-commas.txt\u0026quot;) df1   .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; }  \n  ID Address City State Country Name Employees     0 1 3666 21st St San Francisco CA 94114 USA Madeira 8   1 2 735 Dolores St San Francisco CA 94119 USA Bready Shop 15   2 3 332 Hill St San Francisco California 94114 USA Super River 25   3 4 3995 23rd St San Francisco CA 94114 USA Ben's Shop 10   4 5 1056 Sanchez St San Francisco California USA Sanchez 12   5 6 551 Alvarado St San Francisco CA 94114 USA Richvalley 20     Archivo de texto separado por otro caracter # este archivo los valores estan separados por ; df1 = pd.read_csv(\u0026quot;./Data/supermarkets-semi-colons.txt\u0026quot;,sep=';') df1   .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; }  \n  ID Address City State Country Name Employees     0 1 3666 21st St San Francisco CA 94114 USA Madeira 8   1 2 735 Dolores St San Francisco CA 94119 USA Bready Shop 15   2 3 332 Hill St San Francisco California 94114 USA Super River 25   3 4 3995 23rd St San Francisco CA 94114 USA Ben's Shop 10   4 5 1056 Sanchez St San Francisco California USA Sanchez 12   5 6 551 Alvarado St San Francisco CA 94114 USA Richvalley 20     Excel Pandas puede leer y escribir archivos de Excel, tenga en cuenta que esto solo importa datos. No fórmulas o imágenes, que tengan imágenes o macros pueden hacer que este método read_excel se bloquee.\nExcel Input # leer un archivo de excel df2 = pd.read_excel(\u0026quot;./Data/supermarkets.xlsx\u0026quot;,sheet_name=0) #leer la primera hoja del archivo df2   .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; }  \n  ID Address City State Country Supermarket Name Number of Employees     0 1 3666 21st St San Francisco CA 94114 USA Madeira 8   1 2 735 Dolores St San Francisco CA 94119 USA Bready Shop 15   2 3 332 Hill St San Francisco California 94114 USA Super River 25   3 4 3995 23rd St San Francisco CA 94114 USA Ben's Shop 10   4 5 1056 Sanchez St San Francisco California USA Sanchez 12   5 6 551 Alvarado St San Francisco CA 94114 USA Richvalley 20     # tambien podemos cambiar la columna del index df2.set_index('ID') df2   .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; }  \n  ID Address City State Country Supermarket Name Number of Employees     0 1 3666 21st St San Francisco CA 94114 USA Madeira 8   1 2 735 Dolores St San Francisco CA 94119 USA Bready Shop 15   2 3 332 Hill St San Francisco California 94114 USA Super River 25   3 4 3995 23rd St San Francisco CA 94114 USA Ben's Shop 10   4 5 1056 Sanchez St San Francisco California USA Sanchez 12   5 6 551 Alvarado St San Francisco CA 94114 USA Richvalley 20     Excel Output # sumar cada elemento del data frame por 4 df2['Number of Employees'] = df2['Number of Employees'].apply(lambda x: x+4) df2   .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; }  \n  ID Address City State Country Supermarket Name Number of Employees     0 1 3666 21st St San Francisco CA 94114 USA Madeira 12   1 2 735 Dolores St San Francisco CA 94119 USA Bready Shop 19   2 3 332 Hill St San Francisco California 94114 USA Super River 29   3 4 3995 23rd St San Francisco CA 94114 USA Ben's Shop 14   4 5 1056 Sanchez St San Francisco California USA Sanchez 16   5 6 551 Alvarado St San Francisco CA 94114 USA Richvalley 24     df2.to_excel('./Data/Excel_Sample_out.xlsx',sheet_name='Hoja1')  JSON JSON (JavaScript Object Notation - Notación de Objetos de JavaScript) es un formato ligero de intercambio de datos. Leerlo y escribirlo es simple para humanos, mientras que para las máquinas es simple interpretarlo y generarlo.\nJson Input # los archivos pueden estar en un link de internet df4 = pd.read_json(\u0026quot;http://pythonhow.com/supermarkets.json\u0026quot;) df4   .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; }  \n  Address City Country Employees ID Name State     0 3666 21st St San Francisco USA 8 1 Madeira CA 94114   1 735 Dolores St San Francisco USA 15 2 Bready Shop CA 94119   2 332 Hill St San Francisco USA 25 3 Super River California 94114   3 3995 23rd St San Francisco USA 10 4 Ben's Shop CA 94114   4 1056 Sanchez St San Francisco USA 12 5 Sanchez California   5 551 Alvarado St San Francisco USA 20 6 Richvalley CA 94114     Json output #Para grabar df4.to_json(\u0026quot;./Data/Salida.json\u0026quot;)  WEKA (arff) from scipy.io import arff # libreria para importar archivos de weka # principalmente importa datos numericos data = arff.loadarff('./Data/yeast-train.arff') df5 = pd.DataFrame(data[0]) df5.head()   .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; }  \n  Att1 Att2 Att3 Att4 Att5 Att6 Att7 Att8 Att9 Att10 ... Class5 Class6 Class7 Class8 Class9 Class10 Class11 Class12 Class13 Class14     0 0.093700 0.139771 0.062774 0.007698 0.083873 -0.119156 0.073305 0.005510 0.027523 0.043477 ... b'0' b'0' b'0' b'0' b'0' b'0' b'0' b'0' b'0' b'0'   1 -0.022711 -0.050504 -0.035691 -0.065434 -0.084316 -0.378560 0.038212 0.085770 0.182613 -0.055544 ... b'0' b'0' b'1' b'1' b'0' b'0' b'0' b'1' b'1' b'0'   2 -0.090407 0.021198 0.208712 0.102752 0.119315 0.041729 -0.021728 0.019603 -0.063853 -0.053756 ... b'0' b'0' b'0' b'0' b'0' b'0' b'0' b'1' b'1' b'0'   3 -0.085235 0.009540 -0.013228 0.094063 -0.013592 -0.030719 -0.116062 -0.131674 -0.165448 -0.123053 ... b'0' b'0' b'0' b'0' b'0' b'0' b'0' b'1' b'1' b'1'   4 -0.088765 -0.026743 0.002075 -0.043819 -0.005465 0.004306 -0.055865 -0.071484 -0.159025 -0.111348 ... b'0' b'0' b'0' b'0' b'0' b'0' b'0' b'0' b'0' b'0'    5 rows × 117 columns\n HTML Se debe instalar htmllib5,lxml y BeautifulSoup4. En la terminal escriba\nconda install lxml conda install html5lib conda install BeautifulSoup4  Luego reinicie el Jupyter Notebook. (o use pip install si no esta usando la distribucion de Anaconda)\nPandas puede leer tablas de html\nLa función pandas read_html leerá las tablas de una página web y devolverá una lista de objetos DataFrame:\ndata = pd.read_html('http://www.fdic.gov/bank/individual/failed/banklist.html')  data[0]   .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; }  \n  Bank Name City ST CERT Acquiring Institution Closing Date Updated Date     0 The Enloe State Bank Cooper TX 10716 Legend Bank, N. A. May 31, 2019 June 5, 2019   1 Washington Federal Bank for Savings Chicago IL 30570 Royal Savings Bank December 15, 2017 February 1, 2019   2 The Farmers and Merchants State Bank of Argonia Argonia KS 17719 Conway Bank October 13, 2017 February 21, 2018   3 Fayette County Bank Saint Elmo IL 1802 United Fidelity Bank, fsb May 26, 2017 January 29, 2019   4 Guaranty Bank, (d/b/a BestBank in Georgia \u0026amp; Mi... Milwaukee WI 30003 First-Citizens Bank \u0026amp; Trust Company May 5, 2017 March 22, 2018   5 First NBC Bank New Orleans LA 58302 Whitney Bank April 28, 2017 January 29, 2019   6 Proficio Bank Cottonwood Heights UT 35495 Cache Valley Bank March 3, 2017 January 29, 2019   7 Seaway Bank and Trust Company Chicago IL 19328 State Bank of Texas January 27, 2017 January 29, 2019   8 Harvest Community Bank Pennsville NJ 34951 First-Citizens Bank \u0026amp; Trust Company January 13, 2017 May 18, 2017   9 Allied Bank Mulberry AR 91 Today's Bank September 23, 2016 May 13, 2019   10 The Woodbury Banking Company Woodbury GA 11297 United Bank August 19, 2016 December 13, 2018   11 First CornerStone Bank King of Prussia PA 35312 First-Citizens Bank \u0026amp; Trust Company May 6, 2016 November 13, 2018   12 Trust Company Bank Memphis TN 9956 The Bank of Fayette County April 29, 2016 September 14, 2018   13 North Milwaukee State Bank Milwaukee WI 20364 First-Citizens Bank \u0026amp; Trust Company March 11, 2016 January 29, 2019   14 Hometown National Bank Longview WA 35156 Twin City Bank October 2, 2015 February 19, 2018   15 The Bank of Georgia Peachtree City GA 35259 Fidelity Bank October 2, 2015 July 9, 2018   16 Premier Bank Denver CO 34112 United Fidelity Bank, fsb July 10, 2015 February 20, 2018   17 Edgebrook Bank Chicago IL 57772 Republic Bank of Chicago May 8, 2015 January 29, 2019   18 Doral Bank En Español San Juan PR 32102 Banco Popular de Puerto Rico February 27, 2015 January 29, 2019   19 Capitol City Bank \u0026amp; Trust Company Atlanta GA 33938 First-Citizens Bank \u0026amp; Trust Company February 13, 2015 January 29, 2019   20 Highland Community Bank Chicago IL 20290 United Fidelity Bank, fsb January 23, 2015 November 15, 2017   21 First National Bank of Crestview Crestview FL 17557 First NBC Bank January 16, 2015 November 15, 2017   22 Northern Star Bank Mankato MN 34983 BankVista December 19, 2014 January 3, 2018   23 Frontier Bank, FSB D/B/A El Paseo Bank Palm Desert CA 34738 Bank of Southern California, N.A. November 7, 2014 November 10, 2016   24 The National Republic Bank of Chicago Chicago IL 916 State Bank of Texas October 24, 2014 January 6, 2016   25 NBRS Financial Rising Sun MD 4862 Howard Bank October 17, 2014 January 29, 2019   26 GreenChoice Bank, fsb Chicago IL 28462 Providence Bank, LLC July 25, 2014 December 12, 2016   27 Eastside Commercial Bank Conyers GA 58125 Community \u0026amp; Southern Bank July 18, 2014 October 6, 2017   28 The Freedom State Bank Freedom OK 12483 Alva State Bank \u0026amp; Trust Company June 27, 2014 February 21, 2018   29 Valley Bank Fort Lauderdale FL 21793 Landmark Bank, National Association June 20, 2014 January 29, 2019   ... ... ... ... ... ... ... ...   526 ANB Financial, NA Bentonville AR 33901 Pulaski Bank and Trust Company May 9, 2008 February 1, 2019   527 Hume Bank Hume MO 1971 Security Bank March 7, 2008 January 31, 2019   528 Douglass National Bank Kansas City MO 24660 Liberty Bank and Trust Company January 25, 2008 October 26, 2012   529 Miami Valley Bank Lakeview OH 16848 The Citizens Banking Company October 4, 2007 September 12, 2016   530 NetBank Alpharetta GA 32575 ING DIRECT September 28, 2007 January 31, 2019   531 Metropolitan Savings Bank Pittsburgh PA 35353 Allegheny Valley Bank of Pittsburgh February 2, 2007 October 27, 2010   532 Bank of Ephraim Ephraim UT 1249 Far West Bank June 25, 2004 April 9, 2008   533 Reliance Bank White Plains NY 26778 Union State Bank March 19, 2004 April 9, 2008   534 Guaranty National Bank of Tallahassee Tallahassee FL 26838 Hancock Bank of Florida March 12, 2004 April 17, 2018   535 Dollar Savings Bank Newark NJ 31330 No Acquirer February 14, 2004 April 9, 2008   536 Pulaski Savings Bank Philadelphia PA 27203 Earthstar Bank November 14, 2003 October 6, 2017   537 First National Bank of Blanchardville Blanchardville WI 11639 The Park Bank May 9, 2003 June 5, 2012   538 Southern Pacific Bank Torrance CA 27094 Beal Bank February 7, 2003 October 20, 2008   539 Farmers Bank of Cheneyville Cheneyville LA 16445 Sabine State Bank \u0026amp; Trust December 17, 2002 October 20, 2004   540 Bank of Alamo Alamo TN 9961 No Acquirer November 8, 2002 March 18, 2005   541 AmTrade International Bank En Español Atlanta GA 33784 No Acquirer September 30, 2002 September 11, 2006   542 Universal Federal Savings Bank Chicago IL 29355 Chicago Community Bank June 27, 2002 October 6, 2017   543 Connecticut Bank of Commerce Stamford CT 19183 Hudson United Bank June 26, 2002 February 14, 2012   544 New Century Bank Shelby Township MI 34979 No Acquirer March 28, 2002 March 18, 2005   545 Net 1st National Bank Boca Raton FL 26652 Bank Leumi USA March 1, 2002 April 9, 2008   546 NextBank, NA Phoenix AZ 22314 No Acquirer February 7, 2002 February 5, 2015   547 Oakwood Deposit Bank Co. Oakwood OH 8966 The State Bank \u0026amp; Trust Company February 1, 2002 October 25, 2012   548 Bank of Sierra Blanca Sierra Blanca TX 22002 The Security State Bank of Pecos January 18, 2002 November 6, 2003   549 Hamilton Bank, NA En Español Miami FL 24382 Israel Discount Bank of New York January 11, 2002 September 21, 2015   550 Sinclair National Bank Gravette AR 34248 Delta Trust \u0026amp; Bank September 7, 2001 October 6, 2017   551 Superior Bank, FSB Hinsdale IL 32646 Superior Federal, FSB July 27, 2001 August 19, 2014   552 Malta National Bank Malta OH 6629 North Valley Bank May 3, 2001 November 18, 2002   553 First Alliance Bank \u0026amp; Trust Co. Manchester NH 34264 Southern New Hampshire Bank \u0026amp; Trust February 2, 2001 February 18, 2003   554 National State Bank of Metropolis Metropolis IL 3815 Banterra Bank of Marion December 14, 2000 March 17, 2005   555 Bank of Honolulu Honolulu HI 21029 Bank of the Orient October 13, 2000 March 17, 2005    556 rows × 7 columns\n  SQL El módulo pandas.io.sql proporciona una colección de contenedores de consultas para facilitar la recuperación de datos y reducir la dependencia de la API específica de DB. La abstracción de la base de datos es proporcionada por SQLAlchemy si está instalado. Además, necesitará una biblioteca de controladores para su base de datos. Ejemplos de tales controladores son psycopg2 para PostgreSQL o pymysql para MySQL. Para SQLite esto está incluido en la biblioteca estándar de Python por defecto. Puede encontrar una descripción general de los controladores admitidos para cada lenguaje SQL en los documentos de SQLAlchemy.\nVea también algunos ejemplos de libros para algunas estrategias avanzadas.\nlas funciones claves son:\n read_sql_table(table_name, con[, schema, \u0026hellip;])  Read SQL database table into a DataFrame.   read_sql_query(sql, con[, index_col, \u0026hellip;])  Read SQL query into a DataFrame.   read_sql(sql, con[, index_col, \u0026hellip;])  Read SQL query or database table into a DataFrame.   DataFrame.to_sql(name, con[, flavor, \u0026hellip;])  Write records stored in a DataFrame to a SQL database.    # librerias para crear un proceso de sql sencillo from sqlalchemy import create_engine  # crear un proceso en memoria engine = create_engine('sqlite:///:memory:')  df4.to_sql('data', engine) # grabar el dataframe en formato sql  sql_df = pd.read_sql('data',con=engine) # definir la conexion  sql_df   .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; }  \n  index Address City Country Employees ID Name State     0 0 3666 21st St San Francisco USA 8 1 Madeira CA 94114   1 1 735 Dolores St San Francisco USA 15 2 Bready Shop CA 94119   2 2 332 Hill St San Francisco USA 25 3 Super River California 94114   3 3 3995 23rd St San Francisco USA 10 4 Ben's Shop CA 94114   4 4 1056 Sanchez St San Francisco USA 12 5 Sanchez California   5 5 551 Alvarado St San Francisco USA 20 6 Richvalley CA 94114     Referencias  Web Pandas 10 Minutes to Pandas Tutoriales oficiales de Pandas Pandas Basic cheat sheet Pandas cheat sheet Pandas Importing data cheat sheet Pandas vs R, SQL SAS y otras herramientas, Comparacion de comandos  Phd. Jose R. Zapata\n https://joserzapata.github.io/ https://twitter.com/joserzapata  ","date":1599609600,"expirydate":-62135596800,"kind":"section","lang":"en","lastmod":1599609600,"objectID":"e626929aefe561ac44cbdb7eb4cacb4b","permalink":"https://joserzapata.github.io/courses/python-ciencia-datos/pandas/","publishdate":"2020-09-09T00:00:00Z","relpermalink":"/courses/python-ciencia-datos/pandas/","section":"courses","summary":"Libreria de Manipulación y análisis de datos estructurados","tags":null,"title":"PANDAS - Manipulacion de Datos con Python","type":"book"},{"authors":null,"categories":null,"content":"Por Jose R. Zapata\n\nPython cuenta con varias librerias para visualizacion las principale son:\n  matplotlib para graficas sencillas: bars, pies, lines, scatter plots, etc.\n  Seaborn para visualizacion estadistica: Para crear mapas de calor o de alguna manera resumiendo los datos y aún desea mostrar la distribución de los datos.\n  Plotly y Bokeh para visualizacion interactiva: Si los datos son tan complejos (o no puede ver la informacion de sus datos), utilice plotly y Bokeh para crear visualizaciones interactivas que permitan a los usuarios explorar los datos mismos.\n  Importancia de la visualizacion El siguiente codigo demostrara El cuarteto de Anscombe demostracion realizada por el estadístico F. J. Anscombe.\nEstos datos que estan conformados por 4 dataset demuestra la importancia de la visualizacion de los datos para su analisis.\nimport pandas as pd # libreria manipulacion de datos import seaborn as sns # Libreria graficas import numpy as np %matplotlib inline  anscombe = pd.read_csv('https://github.com/mwaskom/seaborn-data/raw/master/anscombe.csv') anscombe    dataset x y     0 I 10.0 8.04   1 I 8.0 6.95   2 I 13.0 7.58   3 I 9.0 8.81   4 I 11.0 8.33   5 I 14.0 9.96   6 I 6.0 7.24   7 I 4.0 4.26   8 I 12.0 10.84   9 I 7.0 4.82   10 I 5.0 5.68   11 II 10.0 9.14   12 II 8.0 8.14   13 II 13.0 8.74   14 II 9.0 8.77   15 II 11.0 9.26   16 II 14.0 8.10   17 II 6.0 6.13   18 II 4.0 3.10   19 II 12.0 9.13   20 II 7.0 7.26   21 II 5.0 4.74   22 III 10.0 7.46   23 III 8.0 6.77   24 III 13.0 12.74   25 III 9.0 7.11   26 III 11.0 7.81   27 III 14.0 8.84   28 III 6.0 6.08   29 III 4.0 5.39   30 III 12.0 8.15   31 III 7.0 6.42   32 III 5.0 5.73   33 IV 8.0 6.58   34 IV 8.0 5.76   35 IV 8.0 7.71   36 IV 8.0 8.84   37 IV 8.0 8.47   38 IV 8.0 7.04   39 IV 8.0 5.25   40 IV 19.0 12.50   41 IV 8.0 5.56   42 IV 8.0 7.91   43 IV 8.0 6.89    Calcular los valores de la media y la varianza de cada dataset\nagg = anscombe.groupby('dataset').agg([np.mean, np.var]) agg     x y    mean var mean var   dataset         I 9.0 11.0 7.500909 4.127269   II 9.0 11.0 7.500909 4.127629   III 9.0 11.0 7.500000 4.122620   IV 9.0 11.0 7.500909 4.123249    Calcular la correlacion\ncorr = [g.corr()['x'][1] for _, g in anscombe.groupby('dataset')] corr  [0.81642051634484, 0.8162365060002428, 0.8162867394895981, 0.8165214368885028]  Graficar los datasets, haciendo un scatterplot y una regression lineal\n# Grafica Usando seaborn sns.set(style=\u0026quot;ticks\u0026quot;) sns.lmplot(x=\u0026quot;x\u0026quot;, y=\u0026quot;y\u0026quot;, col=\u0026quot;dataset\u0026quot;, hue=\u0026quot;dataset\u0026quot;, data=anscombe, col_wrap=2, ci=None, palette=\u0026quot;muted\u0026quot;, height=4, scatter_kws={\u0026quot;s\u0026quot;: 50, \u0026quot;alpha\u0026quot;: 1});  Calculo de los valores de la regresion lineal\nfits = [np.polyfit(g['x'], g['y'], 1) for _, g in anscombe.groupby('dataset')]  # Almacenar los valores calculados de las regresiones lineales en un dataframe val_reg = pd.DataFrame(fits,columns=['pendiente','intercepto'],index='I II II IV'.split()) val_reg.index.names = ['dataset'] val_reg    pendiente intercepto   dataset       I 0.500091 3.000091   II 0.500000 3.000909   II 0.499727 3.002455   IV 0.499909 3.001727    MATPLOTLIB Matplotlib es el \u0026ldquo;abuelo\u0026rdquo; de las librerias de visualización de datos con Python. Fue creado por John Hunter. Lo creó para tratar de replicar las capacidades de graficar de MatLab en Python.\nEs una excelente biblioteca de gráficos 2D y 3D para generar figuras científicas.\nAlgunos de los principales Pros de Matplotlib son:\n Generalmente es fácil comenzar por graficas simples Soporte para etiquetas personalizadas y textos Gran control de cada elemento en una figura Salida de alta calidad en muchos formatos Muy personalizable en general  Matplotlib le permite crear figuras reproducibles mediante programación. la página web oficial de Matplotlib: http://matplotlib.org/\nInstalacion Se debe instalar Matplotlib, pero si instalo Anaconda ya viene instalado, en caso de que no lo tenga se puede instalar asi:\npip install matplotlib o conda install matplotlib\nen Jupyter notebook !pip install matplotlib\nusar preferiblemente Conda.\nImportar la libreria Importar el modulo matplotlib.pyplot con el nombre de plt (esto es un estandar en la comunidad):\nimport matplotlib.pyplot as plt  Para ver las graficas directamente en este notebook se debe hacer con este comando:\n%matplotlib inline  Esa línea es solo para Jupyter notebooks, si está usando otro editor, usará: plt.show() al final de todos sus comandos de graficos para que aparezca la figura en otra ventana.\n# La mayoria de los datos son inventados para evitar los warnings por divisiones por cero # o valores igual a infinito, entonces apagare los warnings import warnings; warnings.simplefilter('ignore')  Comandos Basicos de Matplotlib Veamos un ejemplo muy simple usando dos arreglos numpy. También se pueden usar listas, pero lo más probable es usar arreglos Numpy o columnas de pandas (que esencialmente también se comportan como arreglos). Los datos que queremos graficar:\nimport numpy as np x = np.linspace(0,5, 11) y = x ** 2  x  array([0. , 0.5, 1. , 1.5, 2. , 2.5, 3. , 3.5, 4. , 4.5, 5. ])  y  array([ 0. , 0.25, 1. , 2.25, 4. , 6.25, 9. , 12.25, 16. , 20.25, 25. ])  Podemos crear un diagrama de líneas muy simple usando lo siguiente:\n# Metodo basico para graficar X vs Y plt.plot(x, y) # se grafica una linea de color azul plt.show() # Mostrar la grafica luego de que ya se definio todos los elementos # plt.show() no es necesario en jupyter notebook  Titulo plt.plot(x, y) # se grafica una linea de color azul plt.title('Titulo de la grafica'); # definir el titulo de la grafica  Nombres de los ejes plt.plot(x, y) # se grafica una linea de color azul plt.xlabel('Nombre del eje X') # definir el nombre del eje X plt.ylabel('Nombre del eje Y') # definir el nombre del eje Y plt.title('Titulo de la grafica'); # definir el titulo de la grafica  Legend Puede usar el argumento de palabra clave label = \u0026ldquo;texto de etiqueta\u0026rdquo; cuando se agreguen gráficas u otros objetos a la figura, y luego usar el método legend sin argumentos para agregar la leyenda a la figura:\nplt.plot(x, y, label=\u0026quot;x vs y\u0026quot;) # se grafica una linea de color azul # se pone en el atributo 'label' el textto deseado plt.xlabel('Nombre del eje X') # definir el nombre del eje X plt.ylabel('Nombre del eje Y') # definir el nombre del eje Y plt.title('Titulo de la grafica') # definir el titulo de la grafica plt.legend(); # agregar el legend al plot  ¡Observe cómo la leyenda se superpone con parte de la grafica!\nEL Metodo legend toma un argumento opcional de palabra clave loc que puede usarse para especificar en qué parte de la figura debe dibujarse la leyenda. Los valores permitidos de loc son códigos numéricos para los diversos lugares donde se puede dibujar la leyenda. Consulte la página de documentación para obtener detalles.\nCuadricula (Grid ) plt.plot(x, y, label=\u0026quot;x vs y\u0026quot;) # se grafica una linea de color azul # se pone en el atributo 'label' el textto deseado plt.xlabel('Nombre del eje X') # definir el nombre del eje X plt.ylabel('Nombre del eje Y') # definir el nombre del eje Y plt.title('Titulo de la grafica') # definir el titulo de la grafica plt.legend() # agregar el legend al plot plt.grid(True) # poner grid en la grafica  Tamaño de la Figura y DPI Matplotlib permite especificar la relación de aspecto, el DPI y el tamaño de la figura cuando se crea el objeto Figure. Puede usar los argumentos de las palabras clave figsize y dpi. No es necesario poner las dos.\n figsize es una tupla del ancho y alto de la figura en pulgadas dpi es el punto por pulgada (pixel por pulgada).  # se cambia el tamaño de la figura y el numero de puntos por pulgada plt.figure(figsize=(8,4), dpi=100) plt.plot(x, y) # se grafica una linea de color azul plt.xlabel('Nombre del eje X') # definir el nombre del eje X plt.ylabel('Nombre del eje Y') # definir el nombre del eje Y plt.title('Titulo de la grafica'); # definir el titulo de la grafica # agrego ; al final del ultimo comando para solo mostrar la grafica # plt.show() no es necesario en jupyter notebook  Parametros de las lineas: colores, ancho y tipos Matplotlib le brinda muchas opciones para personalizar colores, anchos de línea y tipos de línea.\nExiste la sintaxis básica que se puede consultar en:\nhttps://matplotlib.org/2.1.1/api/_as_gen/matplotlib.pyplot.plot.html\nColores Basicos Con matplotlib, podemos definir los colores de las líneas y otros elementos gráficos de varias maneras. En primer lugar, podemos usar la sintaxis similar a MATLAB donde 'b' significa azul,'g' significa verde, etc. También se admite la API MATLAB para seleccionar estilos de línea: donde, por ejemplo, \u0026lsquo;b.-\u0026lsquo;significa una línea azul con puntos:\n# Estilo MATLAB de estilo y color de linea plt.plot(x, x**2, 'b.-') # linea azul con puntos plt.plot(x, x**3, 'g--'); # Linea verde discontinua  Colores usando el parametro color También podemos definir colores por sus nombres o códigos hexadecimales RGB y, opcionalmente, proporcionar un valor alpha utilizando los argumentos de palabras clave color y alpha. Alpha indica opacidad.\nplt.plot(x, x, color=\u0026quot;red\u0026quot;) # Medio transparente plt.plot(x, x+1, color=\u0026quot;red\u0026quot;, alpha=0.5) # Medio transparente plt.plot(x, x+2, color=\u0026quot;#8B008B\u0026quot;) # RGB hex code plt.plot(x, x+3, color=\u0026quot;#F08C08\u0026quot;); # RGB hex code plt.grid(True) # poner grid en la grafica  Estilos de Lineas y marcadores Para cambiar el ancho de línea, podemos usar el argumento de la palabra clave linewidth o lw. El estilo de línea se puede seleccionar usando los argumentos de palabras clave linestyle o ls:\nplt.subplots(figsize=(12,6)) plt.plot(x, x+1, color=\u0026quot;red\u0026quot;, linewidth=0.25) plt.plot(x, x+2, color=\u0026quot;red\u0026quot;, linewidth=0.50) plt.plot(x, x+3, color=\u0026quot;red\u0026quot;, linewidth=1.00) plt.plot(x, x+4, color=\u0026quot;red\u0026quot;, linewidth=2.00) # posibles opciones linestype ‘-‘, ‘–’, ‘-.’, ‘:’, ‘steps’ plt.plot(x, x+5, color=\u0026quot;green\u0026quot;, lw=3, linestyle='-') plt.plot(x, x+6, color=\u0026quot;green\u0026quot;, lw=3, ls='-.') plt.plot(x, x+7, color=\u0026quot;green\u0026quot;, lw=3, ls=':') # lineas parametrizadas line, = plt.plot(x, x+8, color=\u0026quot;black\u0026quot;, lw=1.50) line.set_dashes([5, 10, 15, 10]) # formato: longitud de linea, longitud de espacio, ... # posibles simbolos del marcas: marker = '+', 'o', '*', 's', ',', '.',bb '1', '2', '3', '4', ... plt.plot(x, x+ 9, color=\u0026quot;blue\u0026quot;, lw=3, ls='-', marker='+') plt.plot(x, x+10, color=\u0026quot;blue\u0026quot;, lw=3, ls='--', marker='o') plt.plot(x, x+11, color=\u0026quot;blue\u0026quot;, lw=3, ls='-', marker='s') plt.plot(x, x+12, color=\u0026quot;blue\u0026quot;, lw=3, ls='--', marker='1') # tamaño y color de la marca plt.plot(x, x+13, color=\u0026quot;purple\u0026quot;, lw=1, ls='-', marker='o', markersize=2) plt.plot(x, x+14, color=\u0026quot;purple\u0026quot;, lw=1, ls='-', marker='o', markersize=4) plt.plot(x, x+15, color=\u0026quot;purple\u0026quot;, lw=1, ls='-', marker='o', markersize=8, markerfacecolor=\u0026quot;red\u0026quot;) plt.plot(x, x+16, color=\u0026quot;purple\u0026quot;, lw=1, ls='-', marker='s', markersize=8, markerfacecolor=\u0026quot;yellow\u0026quot;, markeredgewidth=3, markeredgecolor=\u0026quot;green\u0026quot;);  Para mas informacion: https://matplotlib.org/2.1.1/api/_as_gen/matplotlib.pyplot.plot.html\nSubplots # la funcion es plt.subplot(nrows, ncols, plot_number) plt.subplot(1,2,1) # subplot fila=1 Col=2, grafica=1 plt.plot(x, y, 'r--') # r-- color rojo y linea discontinua plt.subplot(1,2,2) # subplot fila=1 Col=2, grafica=2 plt.plot(y, x, 'g*-'); # para no mostrar info de la funcion plt.tight_layout() # para que no se superpongan las graficas  Multiples subplots Crear subplot de diferentes tamaños se puede lograr con el metodo .subplot2grid() Mas informacion en el link: https://matplotlib.org/api/_as_gen/matplotlib.pyplot.subplot2grid.html\nplt.subplot2grid((3,3), (0,0), colspan=3) plt.subplot2grid((3,3), (1,0), colspan=2) plt.subplot2grid((3,3), (1,2), rowspan=2) plt.subplot2grid((3,3), (2,0)) plt.subplot2grid((3,3), (2,1)) plt.tight_layout() # para que no se superpongan las graficas  Rango del Plot Podemos configurar los rangos de los ejes usando los métodos ylim y xlim en el objeto del eje, o axis('tight') para obtener automáticamente rangos de ejes \u0026ldquo;tightly fitted\u0026rdquo;:\nplt.figure(figsize=(12, 4)) plt.subplot(1,3,1) plt.plot(x, x**2, x, x**3) plt.title(\u0026quot;Rango por defecto de los ejes\u0026quot;) plt.subplot(1,3,2) plt.plot(x, x**2, x, x**3) plt.axis('tight') plt.title(\u0026quot;Ejes apretados\u0026quot;) plt.subplot(1,3,3) plt.plot(x, x**2, x, x**3) plt.ylim([0, 60]) plt.xlim([2, 5]) plt.title(\u0026quot;ejes de rango personalizados\u0026quot;); plt.tight_layout() # para que no se superpongan las graficas  Escala Logaritmica plt.figure(figsize=(10,4)) plt.subplot(1,2,1) plt.plot(x, x**2, x, np.exp(x)) plt.title(\u0026quot;escala Normal\u0026quot;) plt.subplot(1,2,2) plt.plot(x, x**2, x, np.exp(x)) plt.yscale(\u0026quot;log\u0026quot;) plt.title(\u0026quot;Escala Logaritmica(y)\u0026quot;); plt.tight_layout() # para que no se superpongan las graficas  Anotaciones de texto Anotar texto en figuras matplotlib se puede hacer usando la función text. Es compatible con el formato LaTeX al igual que los textos y títulos de la etiqueta del eje:\n# Datos para graficar xx = np.linspace(-0.75, 1., 100) plt.plot(xx, xx**2, xx, xx**3) plt.title(\u0026quot;Plot con anotaciones\u0026quot;) # Anotacion 1 plt.text(0.15, 0.2, r\u0026quot;$y=x^2$\u0026quot;, fontsize=20, color=\u0026quot;blue\u0026quot;) #Anotacion 2 plt.text(0.65, 0.1, r\u0026quot;$y=x^3$\u0026quot;, fontsize=20, color=\u0026quot;green\u0026quot;);   Matplotlib Método orientado a objetos Lo demostrado hasta el momento es la forma basica de usar Matplotlib, pero la libreria se puede usar mediante la programacion orientada a objtetos con el Matplotlib\u0026rsquo;s Object Oriented API. Esto significa que se creara una instancia del objeto de figura y luego llamaremos a métodos o atributos de ese objeto. La idea principal al utilizar el método más formal orientado a objetos es crear objetos de figura y luego simplemente invocar métodos o atributos fuera de ese objeto. Este enfoque es más agradable cuando se trata de una figura que tiene múltiples graficos en él. Mas informacion: https://matplotlib.org/api/api_overview.html#the-object-oriented-api\nUn ejemplo de matplotlib orientado a objetos:\n# Se crea una figura y 2 subplots # cada subplt se accede por medio de los objetos axes fig, axes = plt.subplots(nrows=1, ncols=2) for ax in axes: ax.plot(x, y, 'g') ax.set_xlabel('x') ax.set_ylabel('y') ax.set_title('title') plt.tight_layout() # para que no se superpongan las graficas   Tipos Especiales de Plots Hay muchas Graficas especializadas que podemos crear, como barras, histogramas, diagramas de dispersión y mucho más. La mayoría de este tipo de tramas lo crearemos usando seaborn, una biblioteca de gráficos estadísticos para Python. Pero aquí hay algunos ejemplos de este tipo de graficos\nScatter Plot (Dispersion) #Grafica X vs Y # crear datos aleatorios N = 50 x = np.random.rand(N) y = np.random.rand(N) plt.scatter(x, y) plt.title(\u0026quot;Scatter plot Simple\u0026quot;); plt.show() # En jupyter notebook no es necesario este comando  Con las graficas de scatter o dispersion se pueden representar mas de 2 variables en una misma grafica, en el siguiente ejemplo se realizara la comparacion de x vs y el color de los puntos se representara con otra variable y el tamaño de los puntos sera otra variable\n# se creara otra variable que se representara con colores colors = np.random.rand(N) # usar colores aleatorios # se creara otra variable que se representara con el area de los puntos area = np.pi * (15 * np.random.rand(N))**2 # 0 to 15 point radio plt.scatter(x, y, s=area, c=colors, alpha=0.5) # el atributo alpha es para la transparencia plt.title(\u0026quot;Scatter plot de representacion de 4 variables\u0026quot;);  Histograma Un histograma es una representación gráfica de una variable en forma de barras, donde la superficie de cada barra es proporcional a la frecuencia de los valores representados. Sirven para obtener una \u0026ldquo;primera vista\u0026rdquo; general, o panorama, de la distribución de la población, o de la muestra, respecto a una característica, cuantitativa y continua\n# crear datos aleatorios from random import sample data = sample(range(1, 1000), 100) plt.hist(data,bins = 10) # bins el numero de divisiones del histograma plt.title(\u0026quot;Histograma\u0026quot;);  Boxplot Informacion sobre el boxplot -\u0026gt; https://es.wikipedia.org/wiki/Diagrama_de_caja\n Primer cuartil (Q1) como la mediana de la primera mitad de valores Segundo cuartil (Q2) como la propia mediana de la serie Tercer cuartil (Q3) como la mediana de la segunda mitad de valores.  La diferencia entre el tercer cuartil y el primero se conoce como rango intercuartíl\n#crear datos aleatorios data = [np.random.normal(0, std, 100) for std in range(1, 4)] # boxplot rectangular plt.boxplot(data,vert=True,patch_artist=True); plt.title(\u0026quot;Boxplot\u0026quot;);  Diagramas de Violin Permiten ver como es la distribucion de los datos\nall_data = [np.random.normal(0, std, 100) for std in range(6, 10)] # grafico de violin, se puede activar la visualizacion de la media y de la mediana plt.violinplot(all_data, showmeans=False, showmedians=True) plt.title('violin plot');  Diagramas de Violin vs Boxplot Se grafiara usando programacino orientada a objetos con Matplolib la comparacion entre las graficas de violin y las de boxplot\nfig, axes = plt.subplots(nrows=1, ncols=2, figsize=(12, 4)) # generar datos aleatorios all_data = [np.random.normal(0, std, 100) for std in range(6, 10)] # plot de violin axes[0].violinplot(all_data, showmeans=False, showmedians=True) axes[0].set_title('violin plot') # plot box plot axes[1].boxplot(all_data) axes[1].set_title('box plot') # agregando lineas horizontales for ax in axes: ax.yaxis.grid(True) ax.set_xticks([y+1 for y in range(len(all_data))]) ax.set_xlabel('xlabel') ax.set_ylabel('ylabel') # agragando los nombres a las divisiones del eje x (x-tick labels) plt.setp(axes, xticks=[y+1 for y in range(len(all_data))], xticklabels=['x1', 'x2', 'x3', 'x4']) fig.suptitle(\u0026quot;Violin vs Boxplot\u0026quot;,fontsize = 14) # titulo general de la grafica plt.show()  Diagramas de torta No usarlos, los humanos no somos buenos discriminando angulos\nlabels = 'Caballos', 'Cerdos', 'Perros', 'Vacas' sizes = [15, 30, 45, 10] explode = (0, 0.1, 0, 0) # solo \u0026quot;Saque\u0026quot; el 2do pedazo (ejem. 'cerdos') plt.pie(sizes, explode=explode, labels=labels, autopct='%1.1f%%', shadow=True, startangle=90) plt.axis('equal') #La relación de aspecto igual garantiza que el círculo sea homogeneo plt.show()  Diagramas de error # generacion de datos aleatorios x = np.arange(0.1, 4, 0.5) y = np.exp(-x) # Graficas de error plt.errorbar(x, y, xerr=0.2, yerr=0.4) plt.title(\u0026quot;Diagrama de error\u0026quot;) plt.grid() plt.show()   Guardando las figuras Matplotlib puede generar resultados de alta calidad en varios formatos, incluidos PNG, JPG, EPS, SVG, PGF y PDF. Para guardar una figura en un archivo, podemos usar el método savefig de la clase Figure:\nLo primero es antes de crear una grafica definir la clase Figure al principio de todo la grafica, Ejemplo:\nfig = plt.figure(figsize=(10,4)) plt.scatter(x, y) plt.title(\u0026quot;Scatter plot Simple\u0026quot;);\nfig.savefig(\u0026quot;figura.png\u0026quot;)  Aquí también podemos especificar opcionalmente el DPI y elegir entre diferentes formatos de salida (PNG, JPG, EPS, SVG, PGF y PDF):\nfig.savefig(\u0026quot;figura.pdf\u0026quot;, dpi=200)   VISUALIZACION CON PANDAS Pandas tiene funciones incorporadas para la visualización de datos. Está construido sobre matplotlib, pero se usa el formato de pandas para un uso más fácil. Mas informacion en: https://pandas.pydata.org/pandas-docs/stable/visualization.html\nLos parametros de las graficas se pueden modificar con matplotlib.\nImportar la libreria import pandas as pd import numpy as np %matplotlib inline  Datos para graficar:\ndf1 = pd.read_csv('https://github.com/mwaskom/seaborn-data/raw/master/mpg.csv') df1.head()    mpg cylinders displacement horsepower weight acceleration model_year origin name     0 18.0 8 307.0 130.0 3504 12.0 70 usa chevrolet chevelle malibu   1 15.0 8 350.0 165.0 3693 11.5 70 usa buick skylark 320   2 18.0 8 318.0 150.0 3436 11.0 70 usa plymouth satellite   3 16.0 8 304.0 150.0 3433 12.0 70 usa amc rebel sst   4 17.0 8 302.0 140.0 3449 10.5 70 usa ford torino    df2 = pd.read_csv('https://github.com/mwaskom/seaborn-data/raw/master/iris.csv') df2.head()    sepal_length sepal_width petal_length petal_width species     0 5.1 3.5 1.4 0.2 setosa   1 4.9 3.0 1.4 0.2 setosa   2 4.7 3.2 1.3 0.2 setosa   3 4.6 3.1 1.5 0.2 setosa   4 5.0 3.6 1.4 0.2 setosa    Hojas de estilo(Style Sheets) Matplotlib tien Hojas de estilo se pueden usar para hacer que las graficas se vean un poco mejor. Estas hojas de estilo incluyen plot_bmh, plot_fivethirtyeight, plot_ggplot y más. Básicamente, crean un conjunto de reglas de estilo que siguen las gráficas. Es Recomendable usarlos, pues hacen que todas las graficas tengan el mismo aspecto y se sientan más profesionales.\nAntes de usasr plt.style.use() las graficas se ven así:\ndf1['acceleration'].hist();  Usar el estilo ggplot\nimport matplotlib.pyplot as plt plt.style.use('ggplot')  Ahora las graficas se ven asi:\ndf1['acceleration'].hist();  plt.style.use('bmh') df1['acceleration'].hist();  plt.style.use('dark_background') df1['acceleration'].hist();  plt.style.use('fivethirtyeight') df1['acceleration'].hist();  # El estilo por defecto es plt.style.use('classic') df1['acceleration'].hist();  # Seguire usando el estilo ggplot para ver los tipos de grafica de pandas plt.style.use('ggplot')  Tipos de graficas en Pandas Hay varios tipos de plots integradas a pandas, la mayoría de estos plots sobn para estadística por naturaleza:\n df.plot.area df.plot.barh df.plot.density df.plot.hist df.plot.line df.plot.scatter df.plot.bar df.plot.box df.plot.hexbin df.plot.kde df.plot.pie  También se puede llamar a df.plot(kind = 'hist') o reemplazar ese argumento kind con cualquiera de los términos clave que se muestran en la lista anterior (por ejemplo, \u0026lsquo;box\u0026rsquo;, \u0026lsquo;barh\u0026rsquo;, etc.)\nArea # Se puede hacer de las siguiente manera #df2.plot(kind='area',alpha = 0.4) df2.plot.area(alpha=0.4);  Barplots # Visualizacion de datos df2.head()    sepal_length sepal_width petal_length petal_width species     0 5.1 3.5 1.4 0.2 setosa   1 4.9 3.0 1.4 0.2 setosa   2 4.7 3.2 1.3 0.2 setosa   3 4.6 3.1 1.5 0.2 setosa   4 5.0 3.6 1.4 0.2 setosa    # los nombres de cada columna equivalen a un color diferente # Solo se graficaran algunos datos df2.iloc[2:8].plot.bar();  df2.iloc[2:8].plot.bar(stacked=True);  Histogramas df1.head()    mpg cylinders displacement horsepower weight acceleration model_year origin name     0 18.0 8 307.0 130.0 3504 12.0 70 usa chevrolet chevelle malibu   1 15.0 8 350.0 165.0 3693 11.5 70 usa buick skylark 320   2 18.0 8 318.0 150.0 3436 11.0 70 usa plymouth satellite   3 16.0 8 304.0 150.0 3433 12.0 70 usa amc rebel sst   4 17.0 8 302.0 140.0 3449 10.5 70 usa ford torino    df1['acceleration'].plot.hist(bins=50);  Lineas # eje y = valores de la acceleracion # eje x = valores del index # atributo lw es el grosor de la linea df1.plot.line(y='acceleration',figsize=(12,3),lw=1);  Scatter Plots df1.plot.scatter(x='acceleration',y='mpg');  Se puede usar c para indicar el color de los valores de otra columna Con cmap se indica el mapa de colores que se usaran. Para ver los colormaps existente: http://matplotlib.org/users/colormaps.html\ndf1.plot.scatter(x='acceleration',y='mpg',c='model_year',cmap='coolwarm');  O se puede usar s para indicar el tamaño de los puntos. El parametro s debe ser un arreglo, no solo el nombre de una columna:\ndf1.plot.scatter(x='acceleration',y='mpg',s=df1['horsepower']*2);  BoxPlots df2.plot.box(); # Tambien se puede poner by= argumento para groupby  Diagrama de Torta serie = pd.Series(3 * np.random.rand(4), index=['a', 'b', 'c', 'd'], name='series') serie.plot.pie(figsize=(6, 6));  Hexagonal Util para datos de 2 variables, alternativa al scatterplot:\ndf1.plot.hexbin(x='acceleration',y='mpg',gridsize=25,cmap='Oranges');  Kernel Density Estimation Plot(KDE) df1['weight'].plot.kde();  df2.plot.density();  Scatter Matrix pd.plotting.scatter_matrix(df2, figsize=(8, 8));  Parametros de las graficas Graficar con Pandas es un método de hacer graficas mucho más fácil de usar que matplotlib, equilibra la facilidad de uso con control sobre la figura. Muchas de las llamadas a gráficos también aceptan argumentos adicionales de matplotlib plt.\ndf2.plot.density() # grafico de densidad con pandas plt.title('Grafica de densidad de varias variables') plt.grid(False) plt.xticks([]);# Para eliminar los numeros del eje  # La misma grafica pero adicionando los parametros en los argumentos df2.plot.density(title='Grafica de densidad de varias variables', grid=False, xticks = []);  PLOTLY: Libreria de Visualizacion Interactiva Plotly es una libreria de graficos interactivos de código abierto que admite más de 40 tipos de gráficos únicos que cubren una amplia gama de casos de uso estadísticos, financieros, geográficos, científicos y tridimensionales.\nAdemas de ser interactivo y obtener los valores en cada punto de la gráfica, se pueden mezclar datos numéricos y categóricos.\nInstalacion Plotly ´pip install plotly´\n´conda install -c plotly plotly´\nImportar Plotly express Plotly express es un modulo para usar de forma rapida y concisa de usar la visualización interactiva de plotly\nNota: Los datos siempre deben estar en un dataframe\nimport plotly.express as px  Datos integrados en Plotly Plotly viene con algunos data sets clasicos integrados para hacer pruebas:\n carshare election gapminder iris tips wind  Tambien se pueden encontar otros datasets clasicos de demostracion en formato .csv en: https://github.com/mwaskom/seaborn-data\ntips = px.data.tips() # Importar el dataset tips type(tips)  pandas.core.frame.DataFrame  print(px.data.tips.__doc__)   Each row represents a restaurant bill. https://vincentarelbundock.github.io/Rdatasets/doc/reshape2/tips.html Returns: A `pandas.DataFrame` with 244 rows and the following columns: `['total_bill', 'tip', 'sex', 'smoker', 'day', 'time', 'size']`.  tips.head() # ver los primeros 5 registros    total_bill tip sex smoker day time size     0 16.99 1.01 Female No Sun Dinner 2   1 10.34 1.66 Male No Sun Dinner 3   2 21.01 3.50 Male No Sun Dinner 3   3 23.68 3.31 Male No Sun Dinner 2   4 24.59 3.61 Female No Sun Dinner 4    tips.dtypes #tipos de datos en el dataframe  total_bill float64 tip float64 sex object smoker object day object time object size int64 dtype: object  tips.describe() #Resumen estadistico de los datos del data frame por columna    total_bill tip size     count 244.000000 244.000000 244.000000   mean 19.785943 2.998279 2.569672   std 8.902412 1.383638 0.951100   min 3.070000 1.000000 1.000000   25% 13.347500 2.000000 2.000000   50% 17.795000 2.900000 2.000000   75% 24.127500 3.562500 3.000000   max 50.810000 10.000000 6.000000    Tipos de Graficas con Plotly Lineas px.line(tips,y='total_bill',title='Valor Total de la Cuenta')    (function() { let a = setInterval( function() { if ( typeof window.Plotly === 'undefined' ) { return; } clearInterval( a ); Plotly.d3.json(\"./01.json\", function(chart) { Plotly.plot('chart-174359268', chart.data, chart.layout, {responsive: true}); }); }, 500 ); })();  Barras px.bar(tips, x=\u0026quot;sex\u0026quot;, y=\u0026quot;total_bill\u0026quot;)    (function() { let a = setInterval( function() { if ( typeof window.Plotly === 'undefined' ) { return; } clearInterval( a ); Plotly.d3.json(\"./02.json\", function(chart) { Plotly.plot('chart-182796543', chart.data, chart.layout, {responsive: true}); }); }, 500 ); })();  px.bar(tips, x=\u0026quot;sex\u0026quot;, y=\u0026quot;total_bill\u0026quot;, color='sex')    (function() { let a = setInterval( function() { if ( typeof window.Plotly === 'undefined' ) { return; } clearInterval( a ); Plotly.d3.json(\"./03.json\", function(chart) { Plotly.plot('chart-789361254', chart.data, chart.layout, {responsive: true}); }); }, 500 ); })();  Histograma px.histogram(tips,'total_bill',title='Histograma Valor Total de la Cuenta')    (function() { let a = setInterval( function() { if ( typeof window.Plotly === 'undefined' ) { return; } clearInterval( a ); Plotly.d3.json(\"./04.json\", function(chart) { Plotly.plot('chart-917358642', chart.data, chart.layout, {responsive: true}); }); }, 500 ); })();  px.histogram(tips,'sex',title='Histograma de Generos')    (function() { let a = setInterval( function() { if ( typeof window.Plotly === 'undefined' ) { return; } clearInterval( a ); Plotly.d3.json(\"./05.json\", function(chart) { Plotly.plot('chart-623854791', chart.data, chart.layout, {responsive: true}); }); }, 500 ); })();  px.histogram(tips,'day', category_orders= {'day': [\u0026quot;Thur\u0026quot;,\u0026quot;Fri\u0026quot;,\u0026quot;Sat\u0026quot;, \u0026quot;Sun\u0026quot;]}, title='Histograma de Dias')    (function() { let a = setInterval( function() { if ( typeof window.Plotly === 'undefined' ) { return; } clearInterval( a ); Plotly.d3.json(\"./06.json\", function(chart) { Plotly.plot('chart-342159687', chart.data, chart.layout, {responsive: true}); }); }, 500 ); })();  Boxplot px.box(tips,y='total_bill', title='Boxplot Valor Total de la Cuenta')    (function() { let a = setInterval( function() { if ( typeof window.Plotly === 'undefined' ) { return; } clearInterval( a ); Plotly.d3.json(\"./07.json\", function(chart) { Plotly.plot('chart-123964758', chart.data, chart.layout, {responsive: true}); }); }, 500 ); })();  px.box(tips,x = 'day',y='total_bill', color='day', title='Boxplots por dia del Valor Total de la Cuenta')    (function() { let a = setInterval( function() { if ( typeof window.Plotly === 'undefined' ) { return; } clearInterval( a ); Plotly.d3.json(\"./08.json\", function(chart) { Plotly.plot('chart-342968751', chart.data, chart.layout, {responsive: true}); }); }, 500 ); })();  px.box(tips,x = 'day',y='total_bill', title= 'Boxplot por dia con dias en orden', category_orders= {'day': [\u0026quot;Thur\u0026quot;,\u0026quot;Fri\u0026quot;,\u0026quot;Sat\u0026quot;, \u0026quot;Sun\u0026quot;]})    (function() { let a = setInterval( function() { if ( typeof window.Plotly === 'undefined' ) { return; } clearInterval( a ); Plotly.d3.json(\"./09.json\", function(chart) { Plotly.plot('chart-137496582', chart.data, chart.layout, {responsive: true}); }); }, 500 ); })();  px.box(tips,x = 'day',y='total_bill', color='smoker', category_orders= {'day': [\u0026quot;Thur\u0026quot;,\u0026quot;Fri\u0026quot;,\u0026quot;Sat\u0026quot;, \u0026quot;Sun\u0026quot;]})    (function() { let a = setInterval( function() { if ( typeof window.Plotly === 'undefined' ) { return; } clearInterval( a ); Plotly.d3.json(\"./10.json\", function(chart) { Plotly.plot('chart-769218435', chart.data, chart.layout, {responsive: true}); }); }, 500 ); })();  px.box(tips,x = 'day',y='total_bill', color='smoker', boxmode='overlay', title = 'Boxplots de cuenta total por dia, fumador o no , sobrepuestos ', category_orders= {'day': [\u0026quot;Thur\u0026quot;,\u0026quot;Fri\u0026quot;,\u0026quot;Sat\u0026quot;, \u0026quot;Sun\u0026quot;]})    (function() { let a = setInterval( function() { if ( typeof window.Plotly === 'undefined' ) { return; } clearInterval( a ); Plotly.d3.json(\"./11.json\", function(chart) { Plotly.plot('chart-673985421', chart.data, chart.layout, {responsive: true}); }); }, 500 ); })();  Violin Plot px.violin(tips,y='total_bill', title='Boxplot Valor Total de la Cuenta')    (function() { let a = setInterval( function() { if ( typeof window.Plotly === 'undefined' ) { return; } clearInterval( a ); Plotly.d3.json(\"./12.json\", function(chart) { Plotly.plot('chart-234786159', chart.data, chart.layout, {responsive: true}); }); }, 500 ); })();  px.violin(tips,x = 'day',y='total_bill', title='Violin por dia del Valor Total de la Cuenta')    (function() { let a = setInterval( function() { if ( typeof window.Plotly === 'undefined' ) { return; } clearInterval( a ); Plotly.d3.json(\"./13.json\", function(chart) { Plotly.plot('chart-742965138', chart.data, chart.layout, {responsive: true}); }); }, 500 ); })();  px.violin(tips,x = 'day',y='total_bill', color='day', title='Violin por dia del Valor Total de la Cuenta')    (function() { let a = setInterval( function() { if ( typeof window.Plotly === 'undefined' ) { return; } clearInterval( a ); Plotly.d3.json(\"./14.json\", function(chart) { Plotly.plot('chart-679528314', chart.data, chart.layout, {responsive: true}); }); }, 500 ); })();  px.violin(tips,x = 'day',y='total_bill', color='sex', title='Violin por dia del Valor Total de la Cuenta')    (function() { let a = setInterval( function() { if ( typeof window.Plotly === 'undefined' ) { return; } clearInterval( a ); Plotly.d3.json(\"./15.json\", function(chart) { Plotly.plot('chart-971265348', chart.data, chart.layout, {responsive: true}); }); }, 500 ); })();  px.violin(tips,x = 'day',y='total_bill', color='sex',violinmode='overlay', title='Violin por dia del Valor Total de la Cuenta, Hombres y Mujeres')    (function() { let a = setInterval( function() { if ( typeof window.Plotly === 'undefined' ) { return; } clearInterval( a ); Plotly.d3.json(\"./16.json\", function(chart) { Plotly.plot('chart-341268579', chart.data, chart.layout, {responsive: true}); }); }, 500 ); })();  StripPlot px.strip(tips, x=\u0026quot;day\u0026quot;, y=\u0026quot;total_bill\u0026quot;)    (function() { let a = setInterval( function() { if ( typeof window.Plotly === 'undefined' ) { return; } clearInterval( a ); Plotly.d3.json(\"./17.json\", function(chart) { Plotly.plot('chart-649283175', chart.data, chart.layout, {responsive: true}); }); }, 500 ); })();  px.strip(tips, x=\u0026quot;total_bill\u0026quot;, y=\u0026quot;time\u0026quot;, orientation=\u0026quot;h\u0026quot;, color=\u0026quot;smoker\u0026quot;)    (function() { let a = setInterval( function() { if ( typeof window.Plotly === 'undefined' ) { return; } clearInterval( a ); Plotly.d3.json(\"./18.json\", function(chart) { Plotly.plot('chart-264973185', chart.data, chart.layout, {responsive: true}); }); }, 500 ); })();  px.strip(tips, x=\u0026quot;day\u0026quot;, y=\u0026quot;total_bill\u0026quot;, color=\u0026quot;sex\u0026quot;, stripmode='overlay')    (function() { let a = setInterval( function() { if ( typeof window.Plotly === 'undefined' ) { return; } clearInterval( a ); Plotly.d3.json(\"./19.json\", function(chart) { Plotly.plot('chart-247396851', chart.data, chart.layout, {responsive: true}); }); }, 500 ); })();  Scatterplot gapminder = px.data.gapminder() gapminder2007 = gapminder.query(\u0026quot;year==2007\u0026quot;)  px.scatter(gapminder2007, x=\u0026quot;gdpPercap\u0026quot;, y=\u0026quot;lifeExp\u0026quot;)    (function() { let a = setInterval( function() { if ( typeof window.Plotly === 'undefined' ) { return; } clearInterval( a ); Plotly.d3.json(\"./20.json\", function(chart) { Plotly.plot('chart-748152963', chart.data, chart.layout, {responsive: true}); }); }, 500 ); })();  px.scatter(gapminder2007, x=\u0026quot;gdpPercap\u0026quot;, y=\u0026quot;lifeExp\u0026quot;, color=\u0026quot;continent\u0026quot;)    (function() { let a = setInterval( function() { if ( typeof window.Plotly === 'undefined' ) { return; } clearInterval( a ); Plotly.d3.json(\"./21.json\", function(chart) { Plotly.plot('chart-452913786', chart.data, chart.layout, {responsive: true}); }); }, 500 ); })();  px.scatter(gapminder2007, x=\u0026quot;gdpPercap\u0026quot;, y=\u0026quot;lifeExp\u0026quot;, size=\u0026quot;pop\u0026quot;, color=\u0026quot;continent\u0026quot;, size_max=60)    (function() { let a = setInterval( function() { if ( typeof window.Plotly === 'undefined' ) { return; } clearInterval( a ); Plotly.d3.json(\"./22.json\", function(chart) { Plotly.plot('chart-153974286', chart.data, chart.layout, {responsive: true}); }); }, 500 ); })();  px.scatter(gapminder2007, x=\u0026quot;gdpPercap\u0026quot;, y=\u0026quot;lifeExp\u0026quot;, size=\u0026quot;pop\u0026quot;, color=\u0026quot;continent\u0026quot;, hover_name=\u0026quot;country\u0026quot;, log_x=True, size_max=60)    (function() { let a = setInterval( function() { if ( typeof window.Plotly === 'undefined' ) { return; } clearInterval( a ); Plotly.d3.json(\"./23.json\", function(chart) { Plotly.plot('chart-275839416', chart.data, chart.layout, {responsive: true}); }); }, 500 ); })();  Regresion Lineal px.scatter(tips,x='total_bill',y='tip',trendline='ols')    (function() { let a = setInterval( function() { if ( typeof window.Plotly === 'undefined' ) { return; } clearInterval( a ); Plotly.d3.json(\"./24.json\", function(chart) { Plotly.plot('chart-594671382', chart.data, chart.layout, {responsive: true}); }); }, 500 ); })();  Matrix Plot px.scatter_matrix(tips)    (function() { let a = setInterval( function() { if ( typeof window.Plotly === 'undefined' ) { return; } clearInterval( a ); Plotly.d3.json(\"./25.json\", function(chart) { Plotly.plot('chart-918567342', chart.data, chart.layout, {responsive: true}); }); }, 500 ); })();  px.scatter_matrix(tips, dimensions=['total_bill','tip','size'])    (function() { let a = setInterval( function() { if ( typeof window.Plotly === 'undefined' ) { return; } clearInterval( a ); Plotly.d3.json(\"./26.json\", function(chart) { Plotly.plot('chart-795638412', chart.data, chart.layout, {responsive: true}); }); }, 500 ); })();  px.scatter_matrix(tips, dimensions=['total_bill','tip','size'], color='sex')    (function() { let a = setInterval( function() { if ( typeof window.Plotly === 'undefined' ) { return; } clearInterval( a ); Plotly.d3.json(\"./27.json\", function(chart) { Plotly.plot('chart-381795642', chart.data, chart.layout, {responsive: true}); }); }, 500 ); })();  HeatMap tips.head()    total_bill tip sex smoker day time size     0 16.99 1.01 Female No Sun Dinner 2   1 10.34 1.66 Male No Sun Dinner 3   2 21.01 3.50 Male No Sun Dinner 3   3 23.68 3.31 Male No Sun Dinner 2   4 24.59 3.61 Female No Sun Dinner 4    # Matriz de correlacion de los datos tips.corr()    total_bill tip size     total_bill 1.000000 0.675734 0.598315   tip 0.675734 1.000000 0.489299   size 0.598315 0.489299 1.000000    # Este grafico usa plotly de forma diferente import plotly.figure_factory as ff correlation = tips.corr().values # obtener los numeros de la correlacion names = list(tips.corr().columns.values) # obtener los nombres de las columnas transposed_corr = correlation[::-1] # es necesario transponer la matriz  ff.create_annotated_heatmap(transposed_corr, x = names,y = names[::-1], colorscale='Viridis')    (function() { let a = setInterval( function() { if ( typeof window.Plotly === 'undefined' ) { return; } clearInterval( a ); Plotly.d3.json(\"./28.json\", function(chart) { Plotly.plot('chart-371964825', chart.data, chart.layout, {responsive: true}); }); }, 500 ); })();  Animaciones con Plotly px.scatter(gapminder, x=\u0026quot;gdpPercap\u0026quot;, y=\u0026quot;lifeExp\u0026quot;, animation_frame=\u0026quot;year\u0026quot;, animation_group=\u0026quot;country\u0026quot;, size=\u0026quot;pop\u0026quot;, color=\u0026quot;continent\u0026quot;, hover_name=\u0026quot;country\u0026quot;, log_x=True, size_max=45, range_x=[100,100000], range_y=[25,90])    (function() { let a = setInterval( function() { if ( typeof window.Plotly === 'undefined' ) { return; } clearInterval( a ); Plotly.d3.json(\"./29.json\", function(chart) { Plotly.plot('chart-635298417', chart.data, chart.layout, {responsive: true}); }); }, 500 ); })();  Division de Columnas y filas por Categorias (Facet) px.scatter(gapminder2007, x=\u0026quot;gdpPercap\u0026quot;, y=\u0026quot;lifeExp\u0026quot;, size=\u0026quot;pop\u0026quot;, color=\u0026quot;continent\u0026quot;, hover_name=\u0026quot;country\u0026quot;, size_max=60, facet_col='continent', log_x=True)    (function() { let a = setInterval( function() { if ( typeof window.Plotly === 'undefined' ) { return; } clearInterval( a ); Plotly.d3.json(\"./30.json\", function(chart) { Plotly.plot('chart-268793541', chart.data, chart.layout, {responsive: true}); }); }, 500 ); })();  px.scatter(gapminder, x=\u0026quot;gdpPercap\u0026quot;, y=\u0026quot;lifeExp\u0026quot;, animation_frame=\u0026quot;year\u0026quot;, animation_group=\u0026quot;country\u0026quot;, size=\u0026quot;pop\u0026quot;, color=\u0026quot;continent\u0026quot;, hover_name=\u0026quot;country\u0026quot;, facet_col=\u0026quot;continent\u0026quot;, log_x=True, size_max=45, range_x=[100, 100000], range_y=[25, 90])    (function() { let a = setInterval( function() { if ( typeof window.Plotly === 'undefined' ) { return; } clearInterval( a ); Plotly.d3.json(\"./31.json\", function(chart) { Plotly.plot('chart-365912847', chart.data, chart.layout, {responsive: true}); }); }, 500 ); })();  px.histogram(tips,'total_bill', facet_col=\u0026quot;time\u0026quot;, facet_row=\u0026quot;smoker\u0026quot;)    (function() { let a = setInterval( function() { if ( typeof window.Plotly === 'undefined' ) { return; } clearInterval( a ); Plotly.d3.json(\"./32.json\", function(chart) { Plotly.plot('chart-712589346', chart.data, chart.layout, {responsive: true}); }); }, 500 ); })();  px.scatter(tips, x=\u0026quot;total_bill\u0026quot;, y=\u0026quot;tip\u0026quot;, facet_row=\u0026quot;smoker\u0026quot;, facet_col=\u0026quot;time\u0026quot;, color=\u0026quot;sex\u0026quot;)    (function() { let a = setInterval( function() { if ( typeof window.Plotly === 'undefined' ) { return; } clearInterval( a ); Plotly.d3.json(\"./33.json\", function(chart) { Plotly.plot('chart-753219864', chart.data, chart.layout, {responsive: true}); }); }, 500 ); })();  px.scatter(tips, x=\u0026quot;total_bill\u0026quot;, y=\u0026quot;tip\u0026quot;, facet_row=\u0026quot;time\u0026quot;, facet_col=\u0026quot;day\u0026quot;, color=\u0026quot;smoker\u0026quot;, category_orders={\u0026quot;day\u0026quot;: [\u0026quot;Thur\u0026quot;, \u0026quot;Fri\u0026quot;, \u0026quot;Sat\u0026quot;, \u0026quot;Sun\u0026quot;], \u0026quot;time\u0026quot;: [\u0026quot;Lunch\u0026quot;, \u0026quot;Dinner\u0026quot;]})    (function() { let a = setInterval( function() { if ( typeof window.Plotly === 'undefined' ) { return; } clearInterval( a ); Plotly.d3.json(\"./34.json\", function(chart) { Plotly.plot('chart-436987512', chart.data, chart.layout, {responsive: true}); }); }, 500 ); })();  Graficos en Margenes px.scatter(tips,x='total_bill',y='tip', marginal_x='histogram', marginal_y='histogram')    (function() { let a = setInterval( function() { if ( typeof window.Plotly === 'undefined' ) { return; } clearInterval( a ); Plotly.d3.json(\"./35.json\", function(chart) { Plotly.plot('chart-623471598', chart.data, chart.layout, {responsive: true}); }); }, 500 ); })();  px.scatter(tips,x='total_bill',y='tip', marginal_x='violin', marginal_y ='box')    (function() { let a = setInterval( function() { if ( typeof window.Plotly === 'undefined' ) { return; } clearInterval( a ); Plotly.d3.json(\"./36.json\", function(chart) { Plotly.plot('chart-297368154', chart.data, chart.layout, {responsive: true}); }); }, 500 ); })();  px.scatter(tips,x='total_bill',y='tip', marginal_x='violin', marginal_y ='box', color='sex')    (function() { let a = setInterval( function() { if ( typeof window.Plotly === 'undefined' ) { return; } clearInterval( a ); Plotly.d3.json(\"./37.json\", function(chart) { Plotly.plot('chart-287459316', chart.data, chart.layout, {responsive: true}); }); }, 500 ); })();  SEABORN: Libreria de visualización de datos estadísticos de Python Seaborn complementa a Matplotlib y se dirige específicamente a la visualización de datos estadísticos, funciona muy bien con pandas.\nInstalacion Seaborn Anaconda instala automaticamente Seaborn, en caso de no tenerlo instalarlo con el siguiente comando:\nconda install seaborn o pip install seaborn.\nImportar seaborn Se importa de forma estandar de la siguiente manera:\nimport seaborn as sns #para graficar dentro del jupyter notebook %matplotlib inline  Datos integrados en seaborn Seaborn viene con algunos data sets integrados, la lista competa se puede encontrar en: https://github.com/mwaskom/seaborn-data\ntips = sns.load_dataset('tips') # Importar el dataset tips type(tips)  pandas.core.frame.DataFrame  tips.head() # ver los primeros 5 registros    total_bill tip sex smoker day time size     0 16.99 1.01 Female No Sun Dinner 2   1 10.34 1.66 Male No Sun Dinner 3   2 21.01 3.50 Male No Sun Dinner 3   3 23.68 3.31 Male No Sun Dinner 2   4 24.59 3.61 Female No Sun Dinner 4    tips.dtypes #tipos de datos en el dataframe  total_bill float64 tip float64 sex category smoker category day category time category size int64 dtype: object  tips.describe() #Resumen estadistico de los datos del data frame por columna    total_bill tip size     count 244.000000 244.000000 244.000000   mean 19.785943 2.998279 2.569672   std 8.902412 1.383638 0.951100   min 3.070000 1.000000 1.000000   25% 13.347500 2.000000 2.000000   50% 17.795000 2.900000 2.000000   75% 24.127500 3.562500 3.000000   max 50.810000 10.000000 6.000000    Plots de Distribucion en Seaborn distplot El distplot muestra la distribución de un conjunto univariante de observaciones.\nsns.distplot(tips['total_bill']);  Si se quiere eliminar la grafica kde y solo tener el histograma entonces:\nsns.distplot(tips['total_bill'],kde=False,bins=30);  jointplot jointplot() le permite básicamente emparejar dos distplots para datos bivariados. Con su elección de que parámetro kind va comparar:\n “scatter” “reg” “resid” “kde” “hex”  # Histogramas y scatter plot sns.jointplot(x='total_bill',y='tip',data=tips,kind='scatter');  # Histogramas y hexagonal sns.jointplot(x='total_bill',y='tip',data=tips,kind='hex');  #Hystogramas con kde y scatter plot sns.jointplot(x='total_bill',y='tip',data=tips,kind='reg');  pairplot pairplot grafica relaciones por pares en un dataframe completo (para las columnas numéricas) y soporta un argumento de tono de color(Hue) (para columnas categóricas).\n#diagonal histogramas los demas son scatter plots sns.pairplot(tips); # datos numericos  # Datos categoricos # Diagonal KDE y los otros plots son scatter sns.pairplot(tips,hue='sex',palette='coolwarm'); # cambio de colormap  kdeplot kdeplots son Gráficos de Estimación de Densidad del Núcleo.\n# Variable 'total bill' sns.kdeplot(tips['total_bill']) #plot kde  \u0026lt;matplotlib.axes._subplots.AxesSubplot at 0x7f47c6f3c860\u0026gt;  #Variable 'tip' sns.kdeplot(tips['tip'])  \u0026lt;matplotlib.axes._subplots.AxesSubplot at 0x7f47c6964908\u0026gt;  Plots para datos categoricos  boxplot violinplot stripplot swarmplot barplot countplot  import seaborn as sns %matplotlib inline  barplot Es un gráfico general que le permite agregar los datos categóricos basados en alguna función, el valor predeterminado es la media:\nsns.barplot(x='sex',y='total_bill',data=tips);  Puede cambiar el objeto estimador a su propia función, que convierte un vector a escalar:\nimport numpy as np sns.barplot(x='sex',y='total_bill',data=tips,estimator=np.std); # la desviacion estandar como estimador  countplot Esto es esencialmente lo mismo que Barplot, excepto que el estimador está contando explícitamente el número de ocurrencias. Por eso solo pasamos el valor de x:\nsns.countplot(x='sex',data=tips);  boxplot los boxplots (diagrama de caja) y violin plots se utilizan para mostrar la distribución de datos categóricos. Un diagrama de caja (boxplots o gráfico de caja y bigotes) muestra la distribución de datos cuantitativos de una manera que facilita las comparaciones entre variables o entre niveles de una variable categórica. El cuadro muestra los cuartiles del conjunto de datos, mientras que los bigotes se extienden para mostrar el resto de la distribución, a excepción de los puntos que se determinan como \u0026ldquo;valores atípicos\u0026rdquo; utilizando un método que es una función del rango intercuartílico.\nsns.boxplot(x=\u0026quot;day\u0026quot;, y=\u0026quot;total_bill\u0026quot;, data=tips,palette='rainbow');  # se pueden graficar de forma horizontal sns.boxplot(data=tips,palette='rainbow',orient='h');  # cambiar el color y ver varias variables (hue) sns.boxplot(x=\u0026quot;day\u0026quot;, y=\u0026quot;total_bill\u0026quot;, hue=\u0026quot;smoker\u0026quot;,data=tips, palette=\u0026quot;coolwarm\u0026quot;);  violinplot Un plot de violín juega un papel similar a un box and whisker plot (diagrama de cajas y bigotes). Muestra la distribución de datos cuantitativos a través de varios niveles de una (o más) variables categóricas de modo que esas distribuciones se puedan comparar. A diferencia de un diagrama de caja, en el que todos los componentes de la gráfica corresponden a los puntos de datos reales, la gráfica del violín presenta una estimación de la densidad del núcleo de la distribución subyacente.\nsns.violinplot(x=\u0026quot;day\u0026quot;, y=\u0026quot;total_bill\u0026quot;, data=tips,palette='rainbow');  # Varias Variables sns.violinplot(x=\u0026quot;day\u0026quot;, y=\u0026quot;total_bill\u0026quot;, data=tips,hue='sex',palette='Set1');  # Varias variables sns.violinplot(x=\u0026quot;day\u0026quot;, y=\u0026quot;total_bill\u0026quot;, data=tips,hue='sex',split=True,palette='Set1');  stripplot El stripplot dibujará un diagrama de dispersión donde una variable es categórica. Un stripplot se puede dibujar por sí mismo, pero también es un buen complemento de una casilla o trama de violín en los casos en que desea mostrar todas las observaciones junto con alguna representación de la distribución subyacente.\nsns.stripplot(x=\u0026quot;day\u0026quot;, y=\u0026quot;total_bill\u0026quot;, data=tips);  sns.stripplot(x=\u0026quot;day\u0026quot;, y=\u0026quot;total_bill\u0026quot;, data=tips,jitter=True);  # Varias variables sns.stripplot(x=\u0026quot;day\u0026quot;, y=\u0026quot;total_bill\u0026quot;, data=tips,jitter=True,hue='sex',palette='Set1');  # Varias Variables sns.stripplot(x=\u0026quot;day\u0026quot;, y=\u0026quot;total_bill\u0026quot;, data=tips,jitter=True,hue='sex',palette='Set1',dodge=True);  swarmplot El swarmplot es similar a stripplot(), pero los puntos se ajustan (solo a lo largo del eje categórico) para que no se superpongan. Esto proporciona una mejor representación de la distribución de los valores, aunque no se ajusta a un gran número de observaciones (tanto en términos de la capacidad de mostrar todos los puntos como en términos del cálculo necesario para organizarlos).\nsns.swarmplot(x=\u0026quot;day\u0026quot;, y=\u0026quot;total_bill\u0026quot;, data=tips);  sns.swarmplot(x=\u0026quot;day\u0026quot;, y=\u0026quot;total_bill\u0026quot;,hue='sex',data=tips, palette=\u0026quot;Set1\u0026quot;, dodge=True);  Combininando Plots Categoricos sns.violinplot(x=\u0026quot;tip\u0026quot;, y=\u0026quot;day\u0026quot;, data=tips,palette='rainbow') sns.swarmplot(x=\u0026quot;tip\u0026quot;, y=\u0026quot;day\u0026quot;, data=tips,color='black',size=3);  Graficas de Matrices Los Plot de matriz permiten graficar los datos como matrices codificadas por colores y también se pueden usar para indicar clústeres dentro de los datos, algunos de los mas usados son el heatmap y el clustermap de seaborn:\nflights = sns.load_dataset('flights') # carga de datos  tips = sns.load_dataset('tips') # carga de datos  tips.head() # ver los primeros 5 elementos de la tabla    total_bill tip sex smoker day time size     0 16.99 1.01 Female No Sun Dinner 2   1 10.34 1.66 Male No Sun Dinner 3   2 21.01 3.50 Male No Sun Dinner 3   3 23.68 3.31 Male No Sun Dinner 2   4 24.59 3.61 Female No Sun Dinner 4    flights.head() # ver los primeros 5 elementos de la tabla    year month passengers     0 1949 January 112   1 1949 February 118   2 1949 March 132   3 1949 April 129   4 1949 May 121    Heatmap Para que un mapa de calor funcione correctamente, los datos ya deben estar en forma de matriz, la función de sns.heatmap básicamente los colorea. Por ejemplo:\ntips.head()    total_bill tip sex smoker day time size     0 16.99 1.01 Female No Sun Dinner 2   1 10.34 1.66 Male No Sun Dinner 3   2 21.01 3.50 Male No Sun Dinner 3   3 23.68 3.31 Male No Sun Dinner 2   4 24.59 3.61 Female No Sun Dinner 4    # Matriz de correlacion de los datos tips.corr()    total_bill tip size     total_bill 1.000000 0.675734 0.598315   tip 0.675734 1.000000 0.489299   size 0.598315 0.489299 1.000000    # Heatmap de la matriz de correlacion sns.heatmap(tips.corr());  # Cambiando el mapa de colres y agregando las anotaciones a la grafica sns.heatmap(tips.corr(),cmap='coolwarm',annot=True);  O para los datos de vuelos:\n# Definir una pivot table flights.pivot_table(values='passengers',index='month',columns='year')   year 1949 1950 1951 1952 1953 1954 1955 1956 1957 1958 1959 1960   month                 January 112 115 145 171 196 204 242 284 315 340 360 417   February 118 126 150 180 196 188 233 277 301 318 342 391   March 132 141 178 193 236 235 267 317 356 362 406 419   April 129 135 163 181 235 227 269 313 348 348 396 461   May 121 125 172 183 229 234 270 318 355 363 420 472   June 135 149 178 218 243 264 315 374 422 435 472 535   July 148 170 199 230 264 302 364 413 465 491 548 622   August 148 170 199 242 272 293 347 405 467 505 559 606   September 136 158 184 209 237 259 312 355 404 404 463 508   October 119 133 162 191 211 229 274 306 347 359 407 461   November 104 114 146 172 180 203 237 271 305 310 362 390   December 118 140 166 194 201 229 278 306 336 337 405 432    # Graficar la pivot table como un heatmap pvflights = flights.pivot_table(values='passengers',index='month',columns='year') sns.heatmap(pvflights);  # Cambiando los parametros del colormap y el ancho y color de las lineas d division sns.heatmap(pvflights,cmap='magma',linecolor='white',linewidths=1);  clustermap El mapa de clúster utiliza la agrupación jerárquica para producir una versión agrupada del mapa de calor. Por ejemplo:\n# Grafica Clustermap de la tabla pivot de los vuelos sns.clustermap(pvflights);  Observe ahora cómo los años y meses ya no están en orden, en su lugar se agrupan por similitud en el valor (recuento de pasajeros). Eso significa que podemos comenzar a inferir cosas de esta trama, como agosto y julio siendo similares (tiene sentido, ya que ambos son meses de viaje de verano)\n# Más opciones para obtener la información un poco más clara como la normalización # Cambiar el colormap sns.clustermap(pvflights,cmap='coolwarm',standard_scale=1);  Grids Las grids son tipos generales de plots que le permiten mapear tipos de plots en filas y columnas de una cuadrícula, esto le ayuda a crear plots similares separadas por características.\n# Importar librerias import seaborn as sns import matplotlib.pyplot as plt %matplotlib inline  iris = sns.load_dataset('iris') #Importar el dataset  iris.head() #Ver los primeros 5 elementos de la tabla    sepal_length sepal_width petal_length petal_width species     0 5.1 3.5 1.4 0.2 setosa   1 4.9 3.0 1.4 0.2 setosa   2 4.7 3.2 1.3 0.2 setosa   3 4.6 3.1 1.5 0.2 setosa   4 5.0 3.6 1.4 0.2 setosa    PairGrid Pairgrid es un subplot grid para graficar relaciones por pares en un conjunto de datos.\n# solo el Grid sns.PairGrid(iris);  # Ahora se mapea el grid g = sns.PairGrid(iris) g.map(plt.scatter);  # Mapear a arriba, abajo y diagonal g = sns.PairGrid(iris) # crear una cuadricula g.map_diag(plt.hist) #Histogramas en la diagonal g.map_upper(plt.scatter) # Scatter plots en la parte superior g.map_lower(sns.kdeplot); # Plots de densidad kde en la parte inferior  pairplot pairplot es una versión más simple de PairGrid (se usa con bastante frecuencia)\n# La diagonal es un histograma # las otras graficas son scatter plots sns.pairplot(iris);  # la diagonal son kde de los datos categoricos # las otars graficas son scatter plots sns.pairplot(iris,hue='species',palette='rainbow');  Facet Grid FacetGrid es la forma general de crear grids de plots basados en dos caracteristica:\ntips = sns.load_dataset('tips')  tips.head()    total_bill tip sex smoker day time size     0 16.99 1.01 Female No Sun Dinner 2   1 10.34 1.66 Male No Sun Dinner 3   2 21.01 3.50 Male No Sun Dinner 3   3 23.68 3.31 Male No Sun Dinner 2   4 24.59 3.61 Female No Sun Dinner 4    # Solo el Grid g = sns.FacetGrid(tips, col=\u0026quot;time\u0026quot;, row=\u0026quot;smoker\u0026quot;);  # histogramas entre las dos variables g = sns.FacetGrid(tips, col=\u0026quot;time\u0026quot;, row=\u0026quot;smoker\u0026quot;) g = g.map(plt.hist, \u0026quot;total_bill\u0026quot;)  # Scatterplots g = sns.FacetGrid(tips, col=\u0026quot;time\u0026quot;, row=\u0026quot;smoker\u0026quot;,hue='sex') # Observe como los argumentos vienen despues de llamar a plt.scatter g = g.map(plt.scatter, \u0026quot;total_bill\u0026quot;, \u0026quot;tip\u0026quot;).add_legend()  JointGrid JointGrid es la version general de jointplot()\n# Solo el grid g = sns.JointGrid(x=\u0026quot;total_bill\u0026quot;, y=\u0026quot;tip\u0026quot;, data=tips)  # Grafica de regresion y histograma con kde g = sns.JointGrid(x=\u0026quot;total_bill\u0026quot;, y=\u0026quot;tip\u0026quot;, data=tips) g = g.plot(sns.regplot, sns.distplot)  Plots de Regresion Seaborn tiene muchas capacidades integradas para trazados de regresión, lmplot le permite visualizar modelos lineales, pero también le permite dividir los gráficos en función de las características, así como también colorear el tono (hue) en función de las características.\n#Importar librerias import seaborn as sns %matplotlib inline  tips = sns.load_dataset('tips') # importar el dataset  tips.head() # ver los primeros datos del dataset    total_bill tip sex smoker day time size     0 16.99 1.01 Female No Sun Dinner 2   1 10.34 1.66 Male No Sun Dinner 3   2 21.01 3.50 Male No Sun Dinner 3   3 23.68 3.31 Male No Sun Dinner 2   4 24.59 3.61 Female No Sun Dinner 4    lmplot() #scatter plot mas la regresion lineal sns.lmplot(x='total_bill',y='tip',data=tips);  #scatter plot mas la regresion lineal basado en el genero sns.lmplot(x='total_bill',y='tip',data=tips,hue='sex');  # Cambio de paleta de colores sns.lmplot(x='total_bill',y='tip',data=tips,hue='sex',palette='coolwarm');  Usando Marcadores Los argumentos kwargs lmplot pasan a regplto que es una forma más general de lmplot(). regplot tiene un parámetro scatter_kws que se pasa a plt.scatter y puede modificar los parametros.\nMire siempre la documentacion http://matplotlib.org/api/markers_api.html\n# http://matplotlib.org/api/markers_api.html sns.lmplot(x='total_bill',y='tip',data=tips,hue='sex',palette='coolwarm', markers=['o','v'],scatter_kws={'s':100});  Usando un Grid Podemos agregar una separación más variable a través de columnas y filas con el uso de un grid. Simplemente indícandolo en los argumentos col o row:\nsns.lmplot(x='total_bill',y='tip',data=tips,col='sex'); #hace una division por el genero  # division por el genero y por tiempo de almuerzo o cena sns.lmplot(x=\u0026quot;total_bill\u0026quot;, y=\u0026quot;tip\u0026quot;, row=\u0026quot;sex\u0026quot;, col=\u0026quot;time\u0026quot;,data=tips);  # informacion del genero en HUE sns.lmplot(x='total_bill',y='tip',data=tips,col='day',hue='sex',palette='coolwarm');  Aspecto y Tamaño Las figuras de Seaborn se les puede ajustar su tamaño y relación de aspecto con los parámetros height y aspect:\nsns.lmplot(x='total_bill',y='tip',data=tips,col='day',hue='sex',palette='coolwarm', aspect=0.6,height=8);  Referencias  http://www.matplotlib.org http://matplotlib.org/gallery.html - Una gran galería que muestra varios tipos de graficos matplotlib. ¡Muy recomendable! Matplotlib cheat sheet http://www.loria.fr/~rougier/teaching/matplotlib - Un Buen tutorial de matplotlib. http://scipy-lectures.github.io/matplotlib/matplotlib.html - Otra buena referencia para matplotlib reference. https://medium.com/plotly/introducing-plotly-express-808df010143d https://plot.ly/python/plotly-express/ http://seaborn.pydata.org/ - Documentacion Seaborn otra libreria de graficas estadisticas http://matplotlib.org/api/markers_api.html - documentacion de marcadores Lista de colormaps http://www.scipy.org/Cookbook/Matplotlib/Show_colormaps  Phd. Jose R. Zapata\n https://joserzapata.github.io/ https://twitter.com/joserzapata  ","date":1599609600,"expirydate":-62135596800,"kind":"section","lang":"en","lastmod":1599609600,"objectID":"0fe6bd039844e4b2351e4b083a5e55c1","permalink":"https://joserzapata.github.io/courses/python-ciencia-datos/visualizacion/","publishdate":"2020-09-09T00:00:00Z","relpermalink":"/courses/python-ciencia-datos/visualizacion/","section":"courses","summary":"Librerias de visualizacion (matplotlib, plotly, seaborn)","tags":null,"title":"Visualización de Datos con PYTHON","type":"book"},{"authors":null,"categories":null,"content":"Curso Programacion Analitica\nMaestria TIC Linea Ciencia de Datos\nPor Jose R. Zapata\n\nScikit-learn es una biblioteca de código abierto de\tPython que implementa una gran variedad de algoritmos de aprendizaje automático, Preprocesamiento, validación cruzada y visualización.\nProceso de un Proyecto de Machine Learning En este link encuentran un listado de los pasos a realizar en un proyecto de machine Learning:\nhttps://medium.com/@joserzapata/paso-a-paso-en-un-proyecto-machine-learning-bcdd0939d387\n 1. Definir el problema y mirar el panorama general. 2. Obténer los datos 3. Explorar los datos para obtener información. 4. Preparación de los datos. 5. Exploración y selección de modelos 6. Afinar los modelos. 7. Presentacion de la solución. 8. Desplegar, monitorear y mantener el sistema.  Pasos para la construccion y seleccion de un Modelo de Machine Learning  Preparacion de los datos Division de los datos entre datos de Entrenamiento (Para seleccionar el modelo, 80%) y datos de Test (para probar el modelo final, 20%) Se selecciona la medida de Evaluacion segun la aplicacion Usando solo los datos de Entrenamiento se selecciona los mejores modelos utilizando cross validation y comparando sus resultados Se toman los mejores modelos (1, 2 o 3 dependiendo de su rendimiento) y se ajustan los Hiper Parametros Se toman los parametros del mejor modelo y se entrena con todos los datos de Entrenamiento y se evalua su rendimiento con los datos de TEST Si hay un buen resultado se pasa a la etapa de Implementacion (Deployment), en caso contrario segun sea el problema, se prueba con otros modelos o se devuelve a la etapa de Feature Engineering.  Ejemplo Basico con un solo modelo ## Importar Librerias from sklearn import neighbors, datasets, preprocessing from sklearn.model_selection import train_test_split from sklearn.metrics import accuracy_score ## Cargar Dataset iris = datasets.load_iris() ## Definir cual es la columna de salida X, y = iris.data[:, :2], iris.target ## Division del dataset en datos de entrenamiento y de prueba X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=33) ## Standarizacion de los valores scaler = preprocessing.StandardScaler().fit(X_train) X_train = scaler.transform(X_train) X_test = scaler.transform(X_test) ## Seleccion del algoritmo de machine learning knn = neighbors.KNeighborsClassifier(n_neighbors=5) ## Entrenamiento del Modelo knn.fit(X_train, y_train) ## Prediccion y_pred = knn.predict(X_test) # Evaluacion accuracy_score(y_test, y_pred)  0.631578947368421  Preprocesamiento de Datos Standardizacion from sklearn.preprocessing import StandardScaler scaler = StandardScaler().fit(X_train) standardized_X = scaler.transform(X_train) standardized_X_test = scaler.transform(X_test)  Normalizacion from sklearn.preprocessing import Normalizer scaler = Normalizer().fit(X_train) normalized_X = scaler.transform(X_train) normalized_X_test = scaler.transform(X_test)  Binarizacion from sklearn.preprocessing import Binarizer binarizer = Binarizer(threshold=0.0).fit(X) binary_X = binarizer.transform(X)  Encoding Categorical Features from sklearn.preprocessing import LabelEncoder enc = LabelEncoder() y = enc.fit_transform(y)  Imputacion de valores Faltantes from sklearn.preprocessing import Imputer imp = Imputer(missing_values=0, strategy='mean', axis=0) imp.fit_transform(X_train)  Generacion de Caracteristicas Polinomiales from sklearn.preprocessing import PolynomialFeatures poly = PolynomialFeatures(5) oly.fit_transform(X)  Division datos de Entrenamiento y Prueba from sklearn.model_selection import train_test_split X_train, X_test, y_train, y_test = train_test_split(X,y,random_state=0)  Creacion de Modelos Estimadores Aprendizaje Supervisado Regresion Regresion lineal from sklearn.linear_model import LinearRegression lr = LinearRegression(normalize=True) # los parametros son opcionales  Arboles de Regresion from sklearn.tree import DecisionTreeRegressor dtr = DecisionTreeRegressor(random_state = 0) # los parametros son opcionales  Regresion Random Forest from sklearn.ensemble import RandomForestRegressor rfr = RandomForestRegressor(n_estimators = 10, random_state = 0) # los parametros son opcionales  Clasificacion Regresion Logistica from sklearn.linear_model import LogisticRegression lrc = LogisticRegression(random_state = 0) # los parametros son opcionales  Arboles de decision from sklearn.tree import DecisionTreeClassifier dtc = DecisionTreeClassifier(criterion = 'entropy', random_state = 0) # los parametros son opcionales  Clasificador Random Forest from sklearn.ensemble import RandomForestClassifier rfc = RandomForestClassifier(n_estimators = 10, criterion = 'entropy', random_state = 0) # los parametros son opcionales  Support Vector Machines (SVM) from sklearn.svm import SVC svc = SVC(kernel='linear') # los parametros son opcionales  Kernel Support Vector Machines (SVM) from sklearn.svm import SVC svck = SVC(kernel = 'rbf', random_state = 0) # los parametros son opcionales  Naive Bayes from sklearn.naive_bayes import GaussianNB gnb = GaussianNB()  KNN from sklearn import neighbors knn = neighbors.KNeighborsClassifier(n_neighbors=5) # los parametros son opcionales  Estimadores de Aprendizaje No supervizado Principal Component Analysis (PCA) from sklearn.decomposition import PCA pca = PCA(n_components=0.95)  K Means from sklearn.cluster import KMeans k_means = KMeans(n_clusters=3, random_state=0)  Hierarchical from sklearn.cluster import AgglomerativeClustering hc = AgglomerativeClustering(n_clusters = 5, affinity = 'euclidean', linkage = 'ward') # los parametros son opcionales  Entrenamiento de Modelos Aprendizaje Supervisado # Regresion lr.fit(X_train, y_train) dtr.fit(X_train, y_train) rfr.fit(X_train, y_train) #Clasificacion lrc.fit(X_train, y_train) dtc.fit(X_train, y_train) rfc.fit(X_train, y_train) knn.fit(X_train, y_train) svc.fit(X_train, y_train) svck.fit(X_train, y_train) gnb.fit(X_train, y_train)  Aprendizaje No Supervisado k_means.fit(X_train) pca_model = pca.fit_transform(X_train)  Prediccion Estimacion Supervisada # Regresion y_pred = lr.predict(X_test) y_pred = dtr.predict(X_test) y_pred = rfr.predict(X_test) # Clasificacion y_pred = svc.predict(X_test) y_pred = svck.predict(X_test) y_pred = lrc.predict(X_test) y_pred = dtc.predict(X_test) y_pred = rfc.predict(X_test) y_pred = gnb.predict(X_test) y_pred = knn.predict_proba(X_test))  Estimacion No Supervisada y_pred = k_means.predict(X_test) y_hc = hc.fit_predict(X_train)  Evaluacion de los Modelos Metricas de Clasificacion Accuracy Score knn.score(X_test, y_test) from sklearn.metrics import accuracy_score accuracy_score(y_test, y_pred)  Reporte de Clasificacion from sklearn.metrics import classification_report print(classification_report(y_test, y_pred)))  Confusion Matrix from sklearn.metrics import confusion_matrix print(confusion_matrix(y_test, y_pred)))  Metricas de Regresion Error absoluto medio (Mean Absolute Error) from sklearn.metrics import mean_absolute_error y_true = [3, -0.5, 2]) mean_absolute_error(y_true, y_pred))  Error Cuadratico Medio (Mean Squared Error) from sklearn.metrics import mean_squared_error mean_squared_error(y_test, y_pred))  R2 Score from sklearn.metrics import r2_score r2_score(y_true, y_pred))  Metricas de Clustering Adjusted Rand Index from sklearn.metrics import adjusted_rand_score adjusted_rand_score(y_true, y_pred))  Homogeneidad from sklearn.metrics import homogeneity_score homogeneity_score(y_true, y_pred))  V-measure from sklearn.metrics import v_measure_score metrics.v_measure_score(y_true, y_pred))  Cross-Validation print(cross_val_score(knn, X_train, y_train, cv=4)) print(cross_val_score(lr, X, y, cv=2))  Sintonizacion del Modelo (Hyper - parametrizacion) Grid Search from sklearn.grid_search import GridSearchCV params = {\u0026quot;n_neighbors\u0026quot;: np.arange(1,3), \u0026quot;metric\u0026quot;: [\u0026quot;euclidean\u0026quot;, \u0026quot;cityblock\u0026quot;]} grid = GridSearchCV(estimator=knn,param_grid=params) grid.fit(X_train, y_train) print(grid.best_score_) print(grid.best_estimator_.n_neighbors)  Optimizacion Aleatoria de Parametros # Ejemplo from sklearn.grid_search import RandomizedSearchCV params = {\u0026quot;n_neighbors\u0026quot;: range(1,5), \u0026quot;weights\u0026quot;: [\u0026quot;uniform\u0026quot;, \u0026quot;distance\u0026quot;]} rsearch = RandomizedSearchCV(estimator=knn, param_distributions=params, cv=4, n_iter=8, random_state=5) rsearch.fit(X_train, y_train) print(rsearch.best_score_)  Referencias Tutorial Scikit Learn - https://scikit-learn.org/stable/tutorial/index.html\nCheatsheet scikitlearn - https://datacamp-community-prod.s3.amazonaws.com/5433fa18-9f43-44cc-b228-74672efcd116\nPhd. Jose R. Zapata\n https://joserzapata.github.io/ https://twitter.com/joserzapata  ","date":1599609600,"expirydate":-62135596800,"kind":"section","lang":"en","lastmod":1599609600,"objectID":"010a061e25586e745eb825ff4b2d8b6a","permalink":"https://joserzapata.github.io/courses/python-ciencia-datos/ml/","publishdate":"2020-09-09T00:00:00Z","relpermalink":"/courses/python-ciencia-datos/ml/","section":"courses","summary":"Libreria de machine Learning que incluye varios algoritmos de clasificación, regresión y análisis de grupos, ademas de herramientas para seleccion, evaluacion de modelos.","tags":null,"title":"Machine Learning con Scikit Learn","type":"book"},{"authors":null,"categories":null,"content":"Por Jose R. Zapata\n\nimport pandas as pd import numpy as np import matplotlib.pyplot as plt import seaborn as sns import datetime  Informacion de los datos automobile_df = pd.read_csv('auto-mpg.csv') # Ver 5 registros aleatorios automobile_df.sample(5)   .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; }  \n  mpg cylinders displacement horsepower weight acceleration model year origin car name     284 20.6 6 225.0 110 3360 16.6 79 1 dodge aspen 6   136 16.0 8 302.0 140 4141 14.0 74 1 ford gran torino   300 23.9 8 260.0 90 3420 22.2 79 1 oldsmobile cutlass salon brougham   182 28.0 4 107.0 86 2464 15.5 76 2 fiat 131   90 12.0 8 429.0 198 4952 11.5 73 1 mercury marquis brougham     #Tamaño del dataset automobile_df.shape  (398, 9)  automobile_df.info()  \u0026lt;class 'pandas.core.frame.DataFrame'\u0026gt; RangeIndex: 398 entries, 0 to 397 Data columns (total 9 columns): # Column Non-Null Count Dtype --- ------ -------------- ----- 0 mpg 398 non-null float64 1 cylinders 398 non-null int64 2 displacement 398 non-null float64 3 horsepower 398 non-null object 4 weight 398 non-null int64 5 acceleration 398 non-null float64 6 model year 398 non-null int64 7 origin 398 non-null int64 8 car name 398 non-null object dtypes: float64(3), int64(4), object(2) memory usage: 28.1+ KB  Preparacion de Datos automobile_df = automobile_df.replace('?', np.nan)  automobile_df = automobile_df.dropna()  automobile_df.shape  (392, 9)  Eliminar Columnas no necesarias automobile_df.drop(['origin', 'car name'], axis=1, inplace=True)  automobile_df.sample(5)   .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; }  \n  mpg cylinders displacement horsepower weight acceleration model year     164 21.0 6 231.0 110 3039 15.0 75   309 41.5 4 98.0 76 2144 14.7 80   224 15.0 8 302.0 130 4295 14.9 77   322 46.6 4 86.0 65 2110 17.9 80   371 29.0 4 135.0 84 2525 16.0 82     automobile_df.info()  \u0026lt;class 'pandas.core.frame.DataFrame'\u0026gt; Int64Index: 392 entries, 0 to 397 Data columns (total 7 columns): # Column Non-Null Count Dtype --- ------ -------------- ----- 0 mpg 392 non-null float64 1 cylinders 392 non-null int64 2 displacement 392 non-null float64 3 horsepower 392 non-null object 4 weight 392 non-null int64 5 acceleration 392 non-null float64 6 model year 392 non-null int64 dtypes: float64(3), int64(3), object(1) memory usage: 24.5+ KB  Convertir el formato de \u0026lsquo;model year\u0026rsquo; a año completo automobile_df['model year'] = '19' + automobile_df['model year'].astype(str)  automobile_df.sample(5)   .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; }  \n  mpg cylinders displacement horsepower weight acceleration model year     132 25.0 4 140.0 75 2542 17.0 1974   213 13.0 8 350.0 145 4055 12.0 1976   25 10.0 8 360.0 215 4615 14.0 1970   306 28.8 6 173.0 115 2595 11.3 1979   259 20.8 6 200.0 85 3070 16.7 1978     Agregar columna de los años del automobil automobile_df['age'] = datetime.datetime.now().year - pd.to_numeric(automobile_df['model year'])  automobile_df.drop(['model year'], axis=1, inplace=True)  automobile_df.sample(5)   .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; }  \n  mpg cylinders displacement horsepower weight acceleration age     298 23.0 8 350.0 125 3900 17.4 41   154 15.0 6 250.0 72 3432 21.0 45   1 15.0 8 350.0 165 3693 11.5 50   82 23.0 4 120.0 97 2506 14.5 48   199 20.0 6 225.0 100 3651 17.7 44     automobile_df.dtypes  mpg float64 cylinders int64 displacement float64 horsepower object weight int64 acceleration float64 age int64 dtype: object  automobile_df['horsepower'] = pd.to_numeric(automobile_df['horsepower'], errors='coerce')  automobile_df.describe()   .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; }  \n  mpg cylinders displacement horsepower weight acceleration age     count 392.000000 392.000000 392.000000 392.000000 392.000000 392.000000 392.000000   mean 23.445918 5.471939 194.411990 104.469388 2977.584184 15.541327 44.020408   std 7.805007 1.705783 104.644004 38.491160 849.402560 2.758864 3.683737   min 9.000000 3.000000 68.000000 46.000000 1613.000000 8.000000 38.000000   25% 17.000000 4.000000 105.000000 75.000000 2225.250000 13.775000 41.000000   50% 22.750000 4.000000 151.000000 93.500000 2803.500000 15.500000 44.000000   75% 29.000000 8.000000 275.750000 126.000000 3614.750000 17.025000 47.000000   max 46.600000 8.000000 455.000000 230.000000 5140.000000 24.800000 50.000000     Analisis Univariable Se debe hacer un analisis de cada una de las variables y describir sus caracteristicas\nAnalisis Bivariable Scatter Plots fig, ax = plt.subplots(figsize=(12, 8)) plt.scatter(automobile_df['age'], automobile_df['mpg']) plt.xlabel('Años') plt.ylabel('Millas por galon');  fig, ax = plt.subplots(figsize=(12, 8)) plt.scatter(automobile_df['acceleration'], automobile_df['mpg']) plt.xlabel('Aceleracion') plt.ylabel('Millas por galon');  fig, ax = plt.subplots(figsize=(12, 8)) plt.scatter(automobile_df['weight'], automobile_df['mpg']) plt.xlabel('Peso') plt.ylabel('Millas por galon');  fig, ax = plt.subplots(figsize=(12, 8)) plt.scatter(automobile_df['displacement'], automobile_df['mpg']) plt.xlabel('Desplazamiento') plt.ylabel('Millas por galon');  fig, ax = plt.subplots(figsize=(12, 8)) plt.scatter(automobile_df['horsepower'], automobile_df['mpg']) plt.xlabel('Caballos de fuerza') plt.ylabel('Millas por galon');  fig, ax = plt.subplots(figsize=(12, 8)) plt.scatter(automobile_df['cylinders'], automobile_df['mpg']) plt.xlabel('Cilindros') plt.ylabel('Millas por galon');  Correlacion automobile_corr = automobile_df.corr() automobile_corr   .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; }  \n  mpg cylinders displacement horsepower weight acceleration age     mpg 1.000000 -0.777618 -0.805127 -0.778427 -0.832244 0.423329 -0.580541   cylinders -0.777618 1.000000 0.950823 0.842983 0.897527 -0.504683 0.345647   displacement -0.805127 0.950823 1.000000 0.897257 0.932994 -0.543800 0.369855   horsepower -0.778427 0.842983 0.897257 1.000000 0.864538 -0.689196 0.416361   weight -0.832244 0.897527 0.932994 0.864538 1.000000 -0.416839 0.309120   acceleration 0.423329 -0.504683 -0.543800 -0.689196 -0.416839 1.000000 -0.290316   age -0.580541 0.345647 0.369855 0.416361 0.309120 -0.290316 1.000000     fig, ax = plt.subplots(figsize=(12, 10)) sns.heatmap(automobile_corr, annot=True);  automobile_df = automobile_df.sample(frac=1).reset_index(drop=True) automobile_df.head()   .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; }  \n  mpg cylinders displacement horsepower weight acceleration age     0 17.0 8 305.0 130 3840 15.4 41   1 30.0 4 98.0 68 2155 16.5 42   2 19.4 6 232.0 90 3210 17.2 42   3 31.8 4 85.0 65 2020 19.2 41   4 26.0 4 108.0 93 2391 15.5 46     automobile_df.to_csv('auto-mpg-processed.csv', index=False)  Regresion Lineal Regresion lineal con una caracteristica (horsepower) from sklearn.model_selection import train_test_split X = automobile_df[['horsepower']] Y = automobile_df['mpg'] x_train, x_test, y_train, y_test = train_test_split(X, Y, test_size=0.2)  x_train.sample(5)   .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; }  \n  horsepower     375 89   265 68   29 97   231 120   349 108     from sklearn.linear_model import LinearRegression linear_model = LinearRegression(normalize=True).fit(x_train, y_train)  print('Puntaje Entrenamiento: ', linear_model.score(x_train, y_train))  Puntaje Entrenamiento: 0.6323165116170613  y_pred = linear_model.predict(x_test)  from sklearn.metrics import r2_score print('Puntaje Testing: ', r2_score(y_test, y_pred))  Puntaje Testing: 0.31221638583688094  fig, ax = plt.subplots(figsize=(12, 8)) plt.scatter(x_test, y_test) plt.plot(x_test, y_pred, color='r') plt.xlabel('Caballos de Fuerza') plt.ylabel('Mpg') plt.show()  Regresion lineal con una caracteristica - age X = automobile_df[['age']] Y = automobile_df['mpg'] x_train, x_test, y_train, y_test = train_test_split(X, Y, test_size=0.2) linear_model = LinearRegression(normalize=True).fit(x_train, y_train) print('Puntaje de entrenamiento: ', linear_model.score(x_train, y_train)) y_pred = linear_model.predict(x_test) print('Puntaje de Testing: ', r2_score(y_test, y_pred))  Puntaje de entrenamiento: 0.36723602073481343 Puntaje de Testing: 0.20002514279066197  fig, ax = plt.subplots(figsize=(12, 8)) plt.scatter(x_test, y_test) plt.plot(x_test, y_pred, color='r') plt.xlabel('Age') plt.ylabel('Mpg') plt.show()  Regresion lineal con varias caracteristicas # X = automobile_df[['displacement', 'horsepower', 'weight', 'acceleration', 'cylinders']] X = automobile_df[['displacement', 'horsepower', 'weight']] Y = automobile_df['mpg'] x_train, x_test, y_train, y_test = train_test_split(X, Y, test_size=0.2)  linear_model = LinearRegression(normalize=True).fit(x_train, y_train)  print('Training score: ', linear_model.score(x_train, y_train))  Training score: 0.7017904639110666  predictors = x_train.columns coef = pd.Series(linear_model.coef_, predictors).sort_values() print(coef)  horsepower -0.047675 weight -0.005258 displacement -0.004028 dtype: float64  y_pred = linear_model.predict(x_test)  print('Puntaje Testing', r2_score(y_test, y_pred))  Puntaje Testing 0.7243525511175973  plt.figure(figsize = (20,10)) plt.plot(y_pred, label='Prediccion') plt.plot(y_test.values, label='Real') plt.ylabel('Mpg') plt.legend() plt.show()  Regresión con Múltiples Modelos import statsmodels.api as sm from sklearn.model_selection import train_test_split from sklearn.metrics import r2_score from sklearn.preprocessing import StandardScaler from sklearn.linear_model import LinearRegression from sklearn.linear_model import Lasso from sklearn.linear_model import Ridge from sklearn.linear_model import ElasticNet from sklearn.linear_model import Lars from sklearn.linear_model import SGDRegressor from sklearn.svm import SVR from sklearn.neighbors import KNeighborsRegressor from sklearn.tree import DecisionTreeRegressor import warnings warnings.filterwarnings(\u0026quot;ignore\u0026quot;)  automobile_df = pd.read_csv('auto-mpg-processed.csv') automobile_df.head()   .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; }  \n  mpg cylinders displacement horsepower weight acceleration age     0 17.0 8 305.0 130 3840 15.4 41   1 30.0 4 98.0 68 2155 16.5 42   2 19.4 6 232.0 90 3210 17.2 42   3 31.8 4 85.0 65 2020 19.2 41   4 26.0 4 108.0 93 2391 15.5 46     result_dict = {}  Funciones de ayuda def build_model(regression_fn, name_of_y_col, names_of_x_cols, dataset, test_frac=0.2, preprocess_fn=None, show_plot_Y=False, show_plot_scatter=False): \u0026quot;\u0026quot;\u0026quot;build_model Funcion para entrenar y evaluar un modelo \u0026quot;\u0026quot;\u0026quot; X = dataset[names_of_x_cols] Y = dataset[name_of_y_col] if preprocess_fn is not None: X = preprocess_fn(X) x_train, x_test, y_train, y_test = train_test_split(X, Y, test_size=test_frac) model = regression_fn(x_train, y_train) y_pred = model.predict(x_test) print(\u0026quot;Entrenamiento_score : \u0026quot; , model.score(x_train, y_train)) print(\u0026quot;Prueba_score : \u0026quot;, r2_score(y_test, y_pred)) if show_plot_Y == True: fig, ax = plt.subplots(figsize=(12, 8)) plt.plot(y_pred, label='Prediccion') plt.plot(y_test.values, label='Actual') plt.ylabel(name_of_y_col) plt.legend() plt.show() if show_plot_scatter == True: fig, ax = plt.subplots(figsize=(12, 8)) plt.scatter(x_test, y_test) plt.plot(x_test, y_pred, 'r') plt.legend(['Linea de Prediccion','Datos observados']) plt.show() return { 'Entrenamiento_score': model.score(x_train, y_train), 'Prueba_score': r2_score(y_test, y_pred) }  def compare_results(): for key in result_dict: print('Regresion: ', key) print('Entrenamiento score', result_dict[key]['Entrenamiento_score']) print('Prueba score', result_dict[key]['Prueba_score']) print()  Regresion lineal def linear_reg(x_train, y_train): model = LinearRegression(normalize=True) model.fit(x_train, y_train) return model  result_dict['mpg ~ single_linear'] = build_model(linear_reg, 'mpg', ['weight'], automobile_df, show_plot_Y=True)  Entrenamiento_score : 0.6940251922026459 Prueba_score : 0.6780272557552722  result_dict['mpg ~ kitchen_sink_linear'] = build_model(linear_reg, 'mpg', ['cylinders', 'displacement', 'horsepower', 'weight', 'acceleration'], automobile_df, show_plot_Y=True)  Entrenamiento_score : 0.7143524190779553 Prueba_score : 0.6496852460273488  result_dict['mpg ~ parsimonius_linear'] = build_model(linear_reg, 'mpg', ['horsepower', 'weight'], automobile_df, show_plot_Y=True)  Entrenamiento_score : 0.7069851597873933 Prueba_score : 0.7035037058069924  compare_results()  Regresion: mpg ~ single_linear Entrenamiento score 0.6940251922026459 Prueba score 0.6780272557552722 Regresion: mpg ~ kitchen_sink_linear Entrenamiento score 0.7143524190779553 Prueba score 0.6496852460273488 Regresion: mpg ~ parsimonius_linear Entrenamiento score 0.7069851597873933 Prueba score 0.7035037058069924  Lasso def lasso_reg(x_train, y_train, alpha=0.5): model = Lasso(alpha=alpha) model.fit(x_train, y_train) return model  result_dict['mpg ~ kitchen_sink_lasso'] = build_model(lasso_reg, 'mpg', ['cylinders', 'displacement', 'horsepower', 'weight', 'acceleration'], automobile_df, show_plot_Y=True)  Entrenamiento_score : 0.7151538580963827 Prueba_score : 0.6762521302280093  compare_results()  Regresion: mpg ~ single_linear Entrenamiento score 0.6940251922026459 Prueba score 0.6780272557552722 Regresion: mpg ~ kitchen_sink_linear Entrenamiento score 0.7143524190779553 Prueba score 0.6496852460273488 Regresion: mpg ~ parsimonius_linear Entrenamiento score 0.7069851597873933 Prueba score 0.7035037058069924 Regresion: mpg ~ kitchen_sink_lasso Entrenamiento score 0.7151538580963827 Prueba score 0.6762521302280093  Ridge def ridge_reg(x_train, y_train, alpha=0.5, normalize=True): model = Ridge(alpha=alpha, normalize=normalize) model.fit(x_train, y_train) return model  result_dict['mpg ~ kitchen_sink_ridge'] = build_model(ridge_reg, 'mpg', ['cylinders', 'displacement', 'horsepower', 'weight', 'acceleration'], automobile_df, show_plot_Y=True)  Entrenamiento_score : 0.6896708807553231 Prueba_score : 0.6556435118377518  compare_results()  Regresion: mpg ~ single_linear Entrenamiento score 0.6940251922026459 Prueba score 0.6780272557552722 Regresion: mpg ~ kitchen_sink_linear Entrenamiento score 0.7143524190779553 Prueba score 0.6496852460273488 Regresion: mpg ~ parsimonius_linear Entrenamiento score 0.7069851597873933 Prueba score 0.7035037058069924 Regresion: mpg ~ kitchen_sink_lasso Entrenamiento score 0.7151538580963827 Prueba score 0.6762521302280093 Regresion: mpg ~ kitchen_sink_ridge Entrenamiento score 0.6896708807553231 Prueba score 0.6556435118377518  Elasticnet def elastic_net_reg(x_train, y_train, alpha=1, l1_ratio=0.5, normalize=False, max_iter=100000, warm_start=True, equivalent_to=\u0026quot;Elastic Net\u0026quot;): print(\u0026quot;Equivalent to:\u0026quot;, equivalent_to) model = ElasticNet(alpha=alpha, l1_ratio=l1_ratio, normalize=normalize, max_iter=max_iter, warm_start=warm_start) model.fit(x_train, y_train) return model  from functools import partial ## This generates a warning which says will not converge result_dict['mpg ~ kitchen_sink_elastic_net_ols'] = build_model(partial(elastic_net_reg, alpha=0, equivalent_to=\u0026quot;OLS\u0026quot;), 'mpg', ['cylinders', 'displacement', 'horsepower', 'weight', 'acceleration'], automobile_df, show_plot_Y=True)  Equivalent to: OLS Entrenamiento_score : 0.7164509306550633 Prueba_score : 0.6549631422669479  result_dict['mpg ~ kitchen_sink_elastic_net_lasso'] = build_model(partial(elastic_net_reg, alpha=1, l1_ratio=0, equivalent_to=\u0026quot;Lasso\u0026quot;), 'mpg', ['cylinders', 'displacement', 'horsepower', 'weight', 'acceleration'], automobile_df, show_plot_Y=True)  Equivalent to: Lasso Entrenamiento_score : 0.7095244697290346 Prueba_score : 0.693312102583202  result_dict['mpg ~ kitchen_sink_elastic_net_ridge'] = build_model(partial(elastic_net_reg, alpha=1, l1_ratio=1, equivalent_to=\u0026quot;Ridge\u0026quot;), 'mpg', ['cylinders', 'displacement', 'horsepower', 'weight', 'acceleration'], automobile_df, show_plot_Y=True)  Equivalent to: Ridge Entrenamiento_score : 0.7062365688088987 Prueba_score : 0.7064404958207979  result_dict['mpg ~ kitchen_sink_elastic_net'] = build_model(partial(elastic_net_reg, alpha=1, l1_ratio=0.5), 'mpg', [ 'cylinders', 'displacement', 'horsepower', 'weight', 'acceleration'], automobile_df, show_plot_Y=True)  Equivalent to: Elastic Net Entrenamiento_score : 0.7143840512964182 Prueba_score : 0.6719883512351271  compare_results()  Regresion: mpg ~ single_linear Entrenamiento score 0.6940251922026459 Prueba score 0.6780272557552722 Regresion: mpg ~ kitchen_sink_linear Entrenamiento score 0.7143524190779553 Prueba score 0.6496852460273488 Regresion: mpg ~ parsimonius_linear Entrenamiento score 0.7069851597873933 Prueba score 0.7035037058069924 Regresion: mpg ~ kitchen_sink_lasso Entrenamiento score 0.7151538580963827 Prueba score 0.6762521302280093 Regresion: mpg ~ kitchen_sink_ridge Entrenamiento score 0.6896708807553231 Prueba score 0.6556435118377518 Regresion: mpg ~ kitchen_sink_elastic_net_ols Entrenamiento score 0.7164509306550633 Prueba score 0.6549631422669479 Regresion: mpg ~ kitchen_sink_elastic_net_lasso Entrenamiento score 0.7095244697290346 Prueba score 0.693312102583202 Regresion: mpg ~ kitchen_sink_elastic_net_ridge Entrenamiento score 0.7062365688088987 Prueba score 0.7064404958207979 Regresion: mpg ~ kitchen_sink_elastic_net Entrenamiento score 0.7143840512964182 Prueba score 0.6719883512351271  SVR For SVR regression with larger datasets this alternate implementations is preferred\nhttps://scikit-learn.org/stable/modules/generated/sklearn.svm.LinearSVR.html#sklearn.svm.LinearSVR\n Uses a different library for implementation More flexibility with choice of penalties Scales to larger datasets  def svr_reg(x_train, y_train, kernel='linear', epsilon=0.05, C=0.3): model = SVR(kernel=kernel, epsilon=epsilon, C=C) model.fit(x_train,y_train) return model  result_dict['mpg ~ kitchen_sink_svr'] = build_model(svr_reg, 'mpg', ['cylinders', 'displacement', 'horsepower', 'weight', 'acceleration'], automobile_df, show_plot_Y=True)  Entrenamiento_score : 0.6933523813332493 Prueba_score : 0.6864550093179962  compare_results()  Regresion: mpg ~ single_linear Entrenamiento score 0.6940251922026459 Prueba score 0.6780272557552722 Regresion: mpg ~ kitchen_sink_linear Entrenamiento score 0.7143524190779553 Prueba score 0.6496852460273488 Regresion: mpg ~ parsimonius_linear Entrenamiento score 0.7069851597873933 Prueba score 0.7035037058069924 Regresion: mpg ~ kitchen_sink_lasso Entrenamiento score 0.7151538580963827 Prueba score 0.6762521302280093 Regresion: mpg ~ kitchen_sink_ridge Entrenamiento score 0.6896708807553231 Prueba score 0.6556435118377518 Regresion: mpg ~ kitchen_sink_elastic_net_ols Entrenamiento score 0.7164509306550633 Prueba score 0.6549631422669479 Regresion: mpg ~ kitchen_sink_elastic_net_lasso Entrenamiento score 0.7095244697290346 Prueba score 0.693312102583202 Regresion: mpg ~ kitchen_sink_elastic_net_ridge Entrenamiento score 0.7062365688088987 Prueba score 0.7064404958207979 Regresion: mpg ~ kitchen_sink_elastic_net Entrenamiento score 0.7143840512964182 Prueba score 0.6719883512351271 Regresion: mpg ~ kitchen_sink_svr Entrenamiento score 0.6933523813332493 Prueba score 0.6864550093179962  KNR def kneighbors_reg(x_train, y_train, n_neighbors=10): model = KNeighborsRegressor(n_neighbors=n_neighbors) model.fit(x_train, y_train) return model  result_dict['mpg ~ kitchen_sink_kneighbors'] = build_model(kneighbors_reg, 'mpg', ['cylinders', 'displacement', 'horsepower', 'weight', 'acceleration'], automobile_df, show_plot_Y=True)  Entrenamiento_score : 0.7486049876709735 Prueba_score : 0.7499122640106735  compare_results()  Regresion: mpg ~ single_linear Entrenamiento score 0.6940251922026459 Prueba score 0.6780272557552722 Regresion: mpg ~ kitchen_sink_linear Entrenamiento score 0.7143524190779553 Prueba score 0.6496852460273488 Regresion: mpg ~ parsimonius_linear Entrenamiento score 0.7069851597873933 Prueba score 0.7035037058069924 Regresion: mpg ~ kitchen_sink_lasso Entrenamiento score 0.7151538580963827 Prueba score 0.6762521302280093 Regresion: mpg ~ kitchen_sink_ridge Entrenamiento score 0.6896708807553231 Prueba score 0.6556435118377518 Regresion: mpg ~ kitchen_sink_elastic_net_ols Entrenamiento score 0.7164509306550633 Prueba score 0.6549631422669479 Regresion: mpg ~ kitchen_sink_elastic_net_lasso Entrenamiento score 0.7095244697290346 Prueba score 0.693312102583202 Regresion: mpg ~ kitchen_sink_elastic_net_ridge Entrenamiento score 0.7062365688088987 Prueba score 0.7064404958207979 Regresion: mpg ~ kitchen_sink_elastic_net Entrenamiento score 0.7143840512964182 Prueba score 0.6719883512351271 Regresion: mpg ~ kitchen_sink_svr Entrenamiento score 0.6933523813332493 Prueba score 0.6864550093179962 Regresion: mpg ~ kitchen_sink_kneighbors Entrenamiento score 0.7486049876709735 Prueba score 0.7499122640106735  SGD def apply_standard_scaler(x): scaler = StandardScaler() scaler.fit(x) return scaler.transform(x)  def sgd_reg(x_train, y_train, max_iter=10000, tol=1e-3): model = SGDRegressor(max_iter=max_iter, tol=tol) model.fit(x_train, y_train) return model  result_dict['mpg ~ kitchen_sink_sgd'] = build_model(sgd_reg, 'mpg', ['cylinders', 'displacement', 'horsepower', 'weight', 'acceleration'], automobile_df, show_plot_Y=True, preprocess_fn=apply_standard_scaler)  Entrenamiento_score : 0.7161070391196642 Prueba_score : 0.6613614209196004  compare_results()  Regresion: mpg ~ single_linear Entrenamiento score 0.6940251922026459 Prueba score 0.6780272557552722 Regresion: mpg ~ kitchen_sink_linear Entrenamiento score 0.7143524190779553 Prueba score 0.6496852460273488 Regresion: mpg ~ parsimonius_linear Entrenamiento score 0.7069851597873933 Prueba score 0.7035037058069924 Regresion: mpg ~ kitchen_sink_lasso Entrenamiento score 0.7151538580963827 Prueba score 0.6762521302280093 Regresion: mpg ~ kitchen_sink_ridge Entrenamiento score 0.6896708807553231 Prueba score 0.6556435118377518 Regresion: mpg ~ kitchen_sink_elastic_net_ols Entrenamiento score 0.7164509306550633 Prueba score 0.6549631422669479 Regresion: mpg ~ kitchen_sink_elastic_net_lasso Entrenamiento score 0.7095244697290346 Prueba score 0.693312102583202 Regresion: mpg ~ kitchen_sink_elastic_net_ridge Entrenamiento score 0.7062365688088987 Prueba score 0.7064404958207979 Regresion: mpg ~ kitchen_sink_elastic_net Entrenamiento score 0.7143840512964182 Prueba score 0.6719883512351271 Regresion: mpg ~ kitchen_sink_svr Entrenamiento score 0.6933523813332493 Prueba score 0.6864550093179962 Regresion: mpg ~ kitchen_sink_kneighbors Entrenamiento score 0.7486049876709735 Prueba score 0.7499122640106735 Regresion: mpg ~ kitchen_sink_sgd Entrenamiento score 0.7161070391196642 Prueba score 0.6613614209196004  Decision Tree def decision_tree_reg(x_train, y_train, max_depth=2): model = DecisionTreeRegressor(max_depth=max_depth) model.fit(x_train, y_train) return model  result_dict['mpg ~ kitchen_sink_decision_tree'] = build_model(decision_tree_reg, 'mpg', ['cylinders', 'displacement', 'horsepower', 'weight', 'acceleration'], automobile_df, show_plot_Y=True)  Entrenamiento_score : 0.7309719034142717 Prueba_score : 0.7059004654965224  compare_results()  Regresion: mpg ~ single_linear Entrenamiento score 0.6940251922026459 Prueba score 0.6780272557552722 Regresion: mpg ~ kitchen_sink_linear Entrenamiento score 0.7143524190779553 Prueba score 0.6496852460273488 Regresion: mpg ~ parsimonius_linear Entrenamiento score 0.7069851597873933 Prueba score 0.7035037058069924 Regresion: mpg ~ kitchen_sink_lasso Entrenamiento score 0.7151538580963827 Prueba score 0.6762521302280093 Regresion: mpg ~ kitchen_sink_ridge Entrenamiento score 0.6896708807553231 Prueba score 0.6556435118377518 Regresion: mpg ~ kitchen_sink_elastic_net_ols Entrenamiento score 0.7164509306550633 Prueba score 0.6549631422669479 Regresion: mpg ~ kitchen_sink_elastic_net_lasso Entrenamiento score 0.7095244697290346 Prueba score 0.693312102583202 Regresion: mpg ~ kitchen_sink_elastic_net_ridge Entrenamiento score 0.7062365688088987 Prueba score 0.7064404958207979 Regresion: mpg ~ kitchen_sink_elastic_net Entrenamiento score 0.7143840512964182 Prueba score 0.6719883512351271 Regresion: mpg ~ kitchen_sink_svr Entrenamiento score 0.6933523813332493 Prueba score 0.6864550093179962 Regresion: mpg ~ kitchen_sink_kneighbors Entrenamiento score 0.7486049876709735 Prueba score 0.7499122640106735 Regresion: mpg ~ kitchen_sink_sgd Entrenamiento score 0.7161070391196642 Prueba score 0.6613614209196004 Regresion: mpg ~ kitchen_sink_decision_tree Entrenamiento score 0.7309719034142717 Prueba score 0.7059004654965224  Lars def lars_reg(x_train, y_train, n_nonzero_coefs=4): model = Lars(n_nonzero_coefs=n_nonzero_coefs) model.fit(x_train, y_train) return model  result_dict['mpg ~ kitchen_sink_lars'] = build_model(lars_reg, 'mpg', ['cylinders', 'displacement', 'horsepower', 'weight', 'acceleration'], automobile_df, show_plot_Y=True)  Entrenamiento_score : 0.7097904166855645 Prueba_score : 0.6827026729724508  compare_results()  Regresion: mpg ~ single_linear Entrenamiento score 0.6940251922026459 Prueba score 0.6780272557552722 Regresion: mpg ~ kitchen_sink_linear Entrenamiento score 0.7143524190779553 Prueba score 0.6496852460273488 Regresion: mpg ~ parsimonius_linear Entrenamiento score 0.7069851597873933 Prueba score 0.7035037058069924 Regresion: mpg ~ kitchen_sink_lasso Entrenamiento score 0.7151538580963827 Prueba score 0.6762521302280093 Regresion: mpg ~ kitchen_sink_ridge Entrenamiento score 0.6896708807553231 Prueba score 0.6556435118377518 Regresion: mpg ~ kitchen_sink_elastic_net_ols Entrenamiento score 0.7164509306550633 Prueba score 0.6549631422669479 Regresion: mpg ~ kitchen_sink_elastic_net_lasso Entrenamiento score 0.7095244697290346 Prueba score 0.693312102583202 Regresion: mpg ~ kitchen_sink_elastic_net_ridge Entrenamiento score 0.7062365688088987 Prueba score 0.7064404958207979 Regresion: mpg ~ kitchen_sink_elastic_net Entrenamiento score 0.7143840512964182 Prueba score 0.6719883512351271 Regresion: mpg ~ kitchen_sink_svr Entrenamiento score 0.6933523813332493 Prueba score 0.6864550093179962 Regresion: mpg ~ kitchen_sink_kneighbors Entrenamiento score 0.7486049876709735 Prueba score 0.7499122640106735 Regresion: mpg ~ kitchen_sink_sgd Entrenamiento score 0.7161070391196642 Prueba score 0.6613614209196004 Regresion: mpg ~ kitchen_sink_decision_tree Entrenamiento score 0.7309719034142717 Prueba score 0.7059004654965224 Regresion: mpg ~ kitchen_sink_lars Entrenamiento score 0.7097904166855645 Prueba score 0.6827026729724508  Hyper Parameter Tunning from sklearn.metrics import r2_score from sklearn.preprocessing import StandardScaler from sklearn.model_selection import GridSearchCV import warnings warnings.filterwarnings(\u0026quot;ignore\u0026quot;)  automobile_df = pd.read_csv('auto-mpg-processed.csv') automobile_df.head()   .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; }  \n  mpg cylinders displacement horsepower weight acceleration age     0 17.0 8 305.0 130 3840 15.4 41   1 30.0 4 98.0 68 2155 16.5 42   2 19.4 6 232.0 90 3210 17.2 42   3 31.8 4 85.0 65 2020 19.2 41   4 26.0 4 108.0 93 2391 15.5 46     X = automobile_df.drop(['mpg', 'age'], axis=1) Y = automobile_df['mpg'] x_train, x_test, y_train, y_test = train_test_split(X, Y, test_size=0.2)  Lasso regression parameters = {'alpha': [0.2, 0.4, 0.6, 0.7, 0.8, 0.9, 1.0]} grid_search = GridSearchCV(Lasso(), parameters, cv=3, return_train_score=True) grid_search.fit(x_train, y_train) grid_search.best_params_  {'alpha': 0.2}  for i in range(len(parameters['alpha'])): print('Parametros: ', grid_search.cv_results_['params'][i]) print('Promedio Score Prueba: ', grid_search.cv_results_['mean_test_score'][i]) print('Rank: ', grid_search.cv_results_['rank_test_score'][i])  Parametros: {'alpha': 0.2} Promedio Score Prueba: 0.6798292939123337 Rank: 1 Parametros: {'alpha': 0.4} Promedio Score Prueba: 0.6793066874613363 Rank: 2 Parametros: {'alpha': 0.6} Promedio Score Prueba: 0.6792520244406055 Rank: 3 Parametros: {'alpha': 0.7} Promedio Score Prueba: 0.6792212039982681 Rank: 4 Parametros: {'alpha': 0.8} Promedio Score Prueba: 0.6791902701986242 Rank: 5 Parametros: {'alpha': 0.9} Promedio Score Prueba: 0.6791585685202491 Rank: 6 Parametros: {'alpha': 1.0} Promedio Score Prueba: 0.6791274140429867 Rank: 7  lasso_model = Lasso(alpha=grid_search.best_params_['alpha']).fit(x_train, y_train)  y_pred = lasso_model.predict(x_test) print('Entrenamiento score: ', lasso_model.score(x_train, y_train)) print('Prueba score: ', r2_score(y_test, y_pred))  Entrenamiento score: 0.7039334256831417 Prueba score: 0.712881256878038  KNeighbors regression parameters = {'n_neighbors': [10, 12, 14, 18, 20, 25, 30, 35, 50]} grid_search = GridSearchCV(KNeighborsRegressor(), parameters, cv=3, return_train_score=True) grid_search.fit(x_train, y_train) grid_search.best_params_  {'n_neighbors': 20}  for i in range(len(parameters['n_neighbors'])): print('Parametros: ', grid_search.cv_results_['params'][i]) print('Promedio Score prueba: ', grid_search.cv_results_['mean_test_score'][i]) print('Rank: ', grid_search.cv_results_['rank_test_score'][i])  Parametros: {'n_neighbors': 10} Promedio Score prueba: 0.6731278997926874 Rank: 9 Parametros: {'n_neighbors': 12} Promedio Score prueba: 0.6743114600648372 Rank: 8 Parametros: {'n_neighbors': 14} Promedio Score prueba: 0.6818160912518643 Rank: 7 Parametros: {'n_neighbors': 18} Promedio Score prueba: 0.6926328790991523 Rank: 3 Parametros: {'n_neighbors': 20} Promedio Score prueba: 0.693608261494612 Rank: 1 Parametros: {'n_neighbors': 25} Promedio Score prueba: 0.6929003433058707 Rank: 2 Parametros: {'n_neighbors': 30} Promedio Score prueba: 0.6917246778395015 Rank: 4 Parametros: {'n_neighbors': 35} Promedio Score prueba: 0.6891030084152204 Rank: 5 Parametros: {'n_neighbors': 50} Promedio Score prueba: 0.68514355215919 Rank: 6  kneighbors_model = KNeighborsRegressor(n_neighbors=grid_search.best_params_['n_neighbors']).fit(x_train, y_train)  y_pred = kneighbors_model.predict(x_test) print('Entrenamiento score: ', kneighbors_model.score(x_train, y_train)) print('Prueba score: ', r2_score(y_test, y_pred))  Entrenamiento score: 0.7233999422504827 Prueba score: 0.7376145104222849  Decision Tree parameters = {'max_depth':[1, 2, 3, 4, 5, 7, 8]} grid_search = GridSearchCV(DecisionTreeRegressor(), parameters, cv=3, return_train_score=True) grid_search.fit(x_train, y_train) grid_search.best_params_  {'max_depth': 3}  decision_tree_model = DecisionTreeRegressor(max_depth=grid_search.best_params_['max_depth']).fit(x_train, y_train)  y_pred = decision_tree_model.predict(x_test) print('Entrenamiento score: ', decision_tree_model.score(x_train, y_train)) print('Prueba score: ', r2_score(y_test, y_pred))  Entrenamiento score: 0.7721490521354658 Prueba score: 0.7344373548362138  SVR parameters = {'epsilon': [0.05, 0.1, 0.2, 0.3], 'C': [0.2, 0.3]} grid_search = GridSearchCV(SVR(kernel='linear'), parameters, cv=3, return_train_score=True) grid_search.fit(x_train, y_train) grid_search.best_params_  {'C': 0.3, 'epsilon': 0.05}  svr_model = SVR(kernel='linear', epsilon=grid_search.best_params_['epsilon'], C=grid_search.best_params_['C']).fit(x_train, y_train)  y_pred = svr_model.predict(x_test) print('Entrenamiento score: ', svr_model.score(x_train, y_train)) print('Prueba score: ', r2_score(y_test, y_pred))  Entrenamiento score: 0.6879838340458595 Prueba score: 0.6978720556081329  Grabar el Modelo from joblib import dump# libreria de serializacion # grabar el modelo en un archivo dump(kneighbors_model, 'kneighbors_model-auto_mpg.joblib')  ['kneighbors_model-auto_mpg.joblib']  Usar el Modelo import pandas as pd from joblib import load  modelo = load('kneighbors_model-auto_mpg.joblib')  modelo  KNeighborsRegressor(algorithm='auto', leaf_size=30, metric='minkowski', metric_params=None, n_jobs=None, n_neighbors=20, p=2, weights='uniform')  datos = pd.read_csv('auto-mpg-processed.csv') datos.head()   .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; }  \n  mpg cylinders displacement horsepower weight acceleration age     0 17.0 8 305.0 130 3840 15.4 41   1 30.0 4 98.0 68 2155 16.5 42   2 19.4 6 232.0 90 3210 17.2 42   3 31.8 4 85.0 65 2020 19.2 41   4 26.0 4 108.0 93 2391 15.5 46     # tomar dos datos de entrada para realizar la prediccion datos_prueba = datos.iloc[2:4,1:6] datos_prueba   .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; }  \n  cylinders displacement horsepower weight acceleration     2 6 232.0 90 3210 17.2   3 4 85.0 65 2020 19.2     # realizar predcion ccon el modelo modelo.predict(datos_prueba)  array([19.57 , 34.105])  Referencias https://scikit-learn.org/stable/supervised_learning.html#supervised-learning\nCheatsheet scikitlearn https://datacamp-community-prod.s3.amazonaws.com/5433fa18-9f43-44cc-b228-74672efcd116\nPhd. Jose R. Zapata\n https://joserzapata.github.io/ https://twitter.com/joserzapata  ","date":1599609600,"expirydate":-62135596800,"kind":"section","lang":"en","lastmod":1599609600,"objectID":"d68409ae52b78f12c23fcf20fed0dff2","permalink":"https://joserzapata.github.io/courses/python-ciencia-datos/regresion/","publishdate":"2020-09-09T00:00:00Z","relpermalink":"/courses/python-ciencia-datos/regresion/","section":"courses","summary":"Ejemplo Regresion con Python","tags":null,"title":"Regresion con Scikit Learn","type":"book"},{"authors":null,"categories":null,"content":"Por Jose R. Zapata\n\nImportar librerias\nimport pandas as pd import numpy as np import matplotlib.pyplot as plt import seaborn as sns  Informacion de los Datos Titanic dataset\nFuente: https://www.kaggle.com/francksylla/titanic-machine-learning-from-disaster\ntitanic_df = pd.read_csv('datasets/titanic_train.csv') titanic_df.head(10)   .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; }  \n  PassengerId Survived Pclass Name Sex Age SibSp Parch Ticket Fare Cabin Embarked     0 1 0 3 Braund, Mr. Owen Harris male 22.0 1 0 A/5 21171 7.2500 NaN S   1 2 1 1 Cumings, Mrs. John Bradley (Florence Briggs Th... female 38.0 1 0 PC 17599 71.2833 C85 C   2 3 1 3 Heikkinen, Miss. Laina female 26.0 0 0 STON/O2. 3101282 7.9250 NaN S   3 4 1 1 Futrelle, Mrs. Jacques Heath (Lily May Peel) female 35.0 1 0 113803 53.1000 C123 S   4 5 0 3 Allen, Mr. William Henry male 35.0 0 0 373450 8.0500 NaN S   5 6 0 3 Moran, Mr. James male NaN 0 0 330877 8.4583 NaN Q   6 7 0 1 McCarthy, Mr. Timothy J male 54.0 0 0 17463 51.8625 E46 S   7 8 0 3 Palsson, Master. Gosta Leonard male 2.0 3 1 349909 21.0750 NaN S   8 9 1 3 Johnson, Mrs. Oscar W (Elisabeth Vilhelmina Berg) female 27.0 0 2 347742 11.1333 NaN S   9 10 1 2 Nasser, Mrs. Nicholas (Adele Achem) female 14.0 1 0 237736 30.0708 NaN C     titanic_df.shape  (891, 12)  titanic_df.info()  \u0026lt;class 'pandas.core.frame.DataFrame'\u0026gt; RangeIndex: 891 entries, 0 to 890 Data columns (total 12 columns): # Column Non-Null Count Dtype --- ------ -------------- ----- 0 PassengerId 891 non-null int64 1 Survived 891 non-null int64 2 Pclass 891 non-null int64 3 Name 891 non-null object 4 Sex 891 non-null object 5 Age 714 non-null float64 6 SibSp 891 non-null int64 7 Parch 891 non-null int64 8 Ticket 891 non-null object 9 Fare 891 non-null float64 10 Cabin 204 non-null object 11 Embarked 889 non-null object dtypes: float64(2), int64(5), object(5) memory usage: 83.7+ KB  Preparacion de datos Eliminar columnas no necesarias titanic_df.drop(['PassengerId', 'Name', 'Ticket', 'Cabin'], 'columns', inplace=True) titanic_df.head()   .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; }  \n  Survived Pclass Sex Age SibSp Parch Fare Embarked     0 0 3 male 22.0 1 0 7.2500 S   1 1 1 female 38.0 1 0 71.2833 C   2 1 3 female 26.0 0 0 7.9250 S   3 1 1 female 35.0 1 0 53.1000 S   4 0 3 male 35.0 0 0 8.0500 S     titanic_df.info()  \u0026lt;class 'pandas.core.frame.DataFrame'\u0026gt; RangeIndex: 891 entries, 0 to 890 Data columns (total 8 columns): # Column Non-Null Count Dtype --- ------ -------------- ----- 0 Survived 891 non-null int64 1 Pclass 891 non-null int64 2 Sex 891 non-null object 3 Age 714 non-null float64 4 SibSp 891 non-null int64 5 Parch 891 non-null int64 6 Fare 891 non-null float64 7 Embarked 889 non-null object dtypes: float64(2), int64(4), object(2) memory usage: 55.8+ KB  Tratamiento de datos nulos # Contar el numero de datos nulos titanic_df[titanic_df.isnull().any(axis=1)].count()  Survived 179 Pclass 179 Sex 179 Age 2 SibSp 179 Parch 179 Fare 179 Embarked 177 dtype: int64  # eliminar las finlas con datos nulos titanic_df = titanic_df.dropna()  titanic_df.shape  (712, 8)  titanic_df[titanic_df.isnull().any(axis=1)].count()  Survived 0 Pclass 0 Sex 0 Age 0 SibSp 0 Parch 0 Fare 0 Embarked 0 dtype: int64  Descripcion estadistica titanic_df.describe()   .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; }  \n  Survived Pclass Age SibSp Parch Fare     count 712.000000 712.000000 712.000000 712.000000 712.000000 712.000000   mean 0.404494 2.240169 29.642093 0.514045 0.432584 34.567251   std 0.491139 0.836854 14.492933 0.930692 0.854181 52.938648   min 0.000000 1.000000 0.420000 0.000000 0.000000 0.000000   25% 0.000000 1.000000 20.000000 0.000000 0.000000 8.050000   50% 0.000000 2.000000 28.000000 0.000000 0.000000 15.645850   75% 1.000000 3.000000 38.000000 1.000000 1.000000 33.000000   max 1.000000 3.000000 80.000000 5.000000 6.000000 512.329200     titanic_df.head()   .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; }  \n  Survived Pclass Sex Age SibSp Parch Fare Embarked     0 0 3 male 22.0 1 0 7.2500 S   1 1 1 female 38.0 1 0 71.2833 C   2 1 3 female 26.0 0 0 7.9250 S   3 1 1 female 35.0 1 0 53.1000 S   4 0 3 male 35.0 0 0 8.0500 S     Analisis Univariable Se debe hacer un analisis de cada una de las variables y describir sus caracteristicas\nAnalisis Bivariable Scatter Plots fig, ax = plt.subplots(figsize=(12, 8)) plt.scatter(titanic_df['Age'], titanic_df['Survived']) plt.xlabel('Age') plt.ylabel('Survived');  fig, ax = plt.subplots(figsize=(12, 8)) plt.scatter(titanic_df['Fare'], titanic_df['Survived']) plt.xlabel('Fare') plt.ylabel('Survived');  Correlacion pd.crosstab(titanic_df['Sex'], titanic_df['Survived'])   .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; }  \n Survived 0 1   Sex       female 64 195   male 360 93     pd.crosstab(titanic_df['Pclass'], titanic_df['Survived'])   .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; }  \n Survived 0 1   Pclass       1 64 120   2 90 83   3 270 85     titanic_data_corr = titanic_df.corr() titanic_data_corr   .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; }  \n  Survived Pclass Age SibSp Parch Fare     Survived 1.000000 -0.356462 -0.082446 -0.015523 0.095265 0.266100   Pclass -0.356462 1.000000 -0.365902 0.065187 0.023666 -0.552893   Age -0.082446 -0.365902 1.000000 -0.307351 -0.187896 0.093143   SibSp -0.015523 0.065187 -0.307351 1.000000 0.383338 0.139860   Parch 0.095265 0.023666 -0.187896 0.383338 1.000000 0.206624   Fare 0.266100 -0.552893 0.093143 0.139860 0.206624 1.000000     fig, ax = plt.subplots(figsize=(12, 10)) sns.heatmap(titanic_data_corr, annot=True);  Transformacion de Variables from sklearn import preprocessing label_encoding = preprocessing.LabelEncoder() titanic_df['Sex'] = label_encoding.fit_transform(titanic_df['Sex'].astype(str)) titanic_df.head()   .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; }  \n  Survived Pclass Sex Age SibSp Parch Fare Embarked     0 0 3 1 22.0 1 0 7.2500 S   1 1 1 0 38.0 1 0 71.2833 C   2 1 3 0 26.0 0 0 7.9250 S   3 1 1 0 35.0 1 0 53.1000 S   4 0 3 1 35.0 0 0 8.0500 S     label_encoding.classes_  array(['female', 'male'], dtype=object)  C = Cherbourg, Q = Queenstown, S = Southampton\ntitanic_df = pd.get_dummies(titanic_df, columns=['Embarked']) titanic_df.head()   .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; }  \n  Survived Pclass Sex Age SibSp Parch Fare Embarked_C Embarked_Q Embarked_S     0 0 3 1 22.0 1 0 7.2500 0 0 1   1 1 1 0 38.0 1 0 71.2833 1 0 0   2 1 3 0 26.0 0 0 7.9250 0 0 1   3 1 1 0 35.0 1 0 53.1000 0 0 1   4 0 3 1 35.0 0 0 8.0500 0 0 1     titanic_df = titanic_df.sample(frac=1).reset_index(drop=True) titanic_df.head()   .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; }  \n  Survived Pclass Sex Age SibSp Parch Fare Embarked_C Embarked_Q Embarked_S     0 1 3 1 25.0 1 0 7.7750 0 0 1   1 0 3 1 27.0 1 0 14.4542 1 0 0   2 0 3 1 51.0 0 0 7.0542 0 0 1   3 0 3 1 19.0 0 0 7.8958 0 0 1   4 1 2 1 34.0 0 0 13.0000 0 0 1     titanic_df.to_csv('datasets/titanic_processed.csv', index=False)  Clasificacion Binaria titanic_df = pd.read_csv('datasets/titanic_processed.csv') titanic_df.head()   .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; }  \n  Survived Pclass Sex Age SibSp Parch Fare Embarked_C Embarked_Q Embarked_S     0 1 3 1 25.0 1 0 7.7750 0 0 1   1 0 3 1 27.0 1 0 14.4542 1 0 0   2 0 3 1 51.0 0 0 7.0542 0 0 1   3 0 3 1 19.0 0 0 7.8958 0 0 1   4 1 2 1 34.0 0 0 13.0000 0 0 1     titanic_df.shape  (712, 10)  from sklearn.model_selection import train_test_split X = titanic_df.drop('Survived', axis=1) Y = titanic_df['Survived'] x_train, x_test, y_train, y_test = train_test_split(X, Y, test_size=0.2)  x_train.shape, y_train.shape  ((569, 9), (569,))  x_test.shape, y_test.shape  ((143, 9), (143,))  Regresion Logistica para Clasificacion https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html\nfrom sklearn.linear_model import LogisticRegression logistic_model = LogisticRegression(penalty='l2', C=1.0, solver='liblinear').fit(x_train, y_train)  y_pred = logistic_model.predict(x_test)  Matriz de confusion pred_results = pd.DataFrame({'y_test': y_test, 'y_pred': y_pred})  pred_results.head()   .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; }  \n  y_test y_pred     484 1 1   102 0 1   65 0 0   30 1 0   38 1 1     titanic_crosstab = pd.crosstab(pred_results.y_pred, pred_results.y_test) titanic_crosstab   .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; }  \n y_test 0 1   y_pred       0 75 16   1 16 36     Precision - recall https://scikit-learn.org/stable/modules/generated/sklearn.metrics.precision_score.html https://scikit-learn.org/stable/modules/generated/sklearn.metrics.recall_score.html\nfrom sklearn.metrics import accuracy_score from sklearn.metrics import precision_score from sklearn.metrics import recall_score  acc = accuracy_score(y_test, y_pred) prec = precision_score(y_test, y_pred) recall = recall_score(y_test, y_pred) print(\u0026quot;accuracy_score : \u0026quot;, acc) print(\u0026quot;precision_score : \u0026quot;, prec) print(\u0026quot;recall_score : \u0026quot;, recall)  accuracy_score : 0.7762237762237763 precision_score : 0.6923076923076923 recall_score : 0.6923076923076923  titanic_crosstab   .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; }  \n y_test 0 1   y_pred       0 75 16   1 16 36     TP = titanic_crosstab[1][1] TN = titanic_crosstab[0][0] FP = titanic_crosstab[0][1] FN = titanic_crosstab[1][0]  accuracy_score_verified = (TP + TN) / (TP + FP + TN + FN) accuracy_score_verified  0.7762237762237763  precision_score_survived = TP / (TP + FP) precision_score_survived  0.6923076923076923  recall_score_survived = TP / (TP + FN) recall_score_survived  0.6923076923076923  Clasificacion con Multiples Modelos from sklearn.model_selection import train_test_split from sklearn.metrics import accuracy_score from sklearn.metrics import precision_score from sklearn.metrics import recall_score from sklearn.linear_model import LogisticRegression from sklearn.discriminant_analysis import LinearDiscriminantAnalysis from sklearn.discriminant_analysis import QuadraticDiscriminantAnalysis from sklearn.linear_model import SGDClassifier from sklearn.svm import LinearSVC from sklearn.neighbors import RadiusNeighborsClassifier from sklearn.naive_bayes import GaussianNB from sklearn.tree import DecisionTreeClassifier  titanic_df = pd.read_csv('datasets/titanic_processed.csv') titanic_df.head()   .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; }  \n  Survived Pclass Sex Age SibSp Parch Fare Embarked_C Embarked_Q Embarked_S     0 1 3 1 25.0 1 0 7.7750 0 0 1   1 0 3 1 27.0 1 0 14.4542 1 0 0   2 0 3 1 51.0 0 0 7.0542 0 0 1   3 0 3 1 19.0 0 0 7.8958 0 0 1   4 1 2 1 34.0 0 0 13.0000 0 0 1     FEATURES = list(titanic_df.columns[1:]) FEATURES  ['Pclass', 'Sex', 'Age', 'SibSp', 'Parch', 'Fare', 'Embarked_C', 'Embarked_Q', 'Embarked_S']  result_dict = {}  def summarize_classification(y_test, y_pred): acc = accuracy_score(y_test, y_pred, normalize=True) num_acc = accuracy_score(y_test, y_pred, normalize=False) prec = precision_score(y_test, y_pred) recall = recall_score(y_test, y_pred) return {'accuracy': acc, 'precision': prec, 'recall':recall, 'accuracy_count':num_acc}  def build_model(classifier_fn, name_of_y_col, names_of_x_cols, dataset, test_frac=0.2): X = dataset[names_of_x_cols] Y = dataset[name_of_y_col] x_train, x_test, y_train, y_test = train_test_split(X, Y, test_size=test_frac) model = classifier_fn(x_train, y_train) y_pred = model.predict(x_test) y_pred_train = model.predict(x_train) train_summary = summarize_classification(y_train, y_pred_train) test_summary = summarize_classification(y_test, y_pred) pred_results = pd.DataFrame({'y_test': y_test, 'y_pred': y_pred}) model_crosstab = pd.crosstab(pred_results.y_pred, pred_results.y_test) return {'training': train_summary, 'test': test_summary, 'confusion_matrix': model_crosstab}  def compare_results(): for key in result_dict: print('Classification: ', key) print() print('Training data') for score in result_dict[key]['training']: print(score, result_dict[key]['training'][score]) print() print('Test data') for score in result_dict[key]['test']: print(score, result_dict[key]['test'][score]) print()  Regresion logistica def logistic_fn(x_train, y_train): model = LogisticRegression(solver='liblinear') model.fit(x_train, y_train) return model  result_dict['survived ~ logistic'] = build_model(logistic_fn, 'Survived', FEATURES, titanic_df) compare_results()  Classification: survived ~ logistic Training data accuracy 0.8101933216168717 precision 0.8020304568527918 recall 0.6960352422907489 accuracy_count 461 Test data accuracy 0.7692307692307693 precision 0.7692307692307693 recall 0.6557377049180327 accuracy_count 110  Lineal Discriminant Analysis def linear_discriminant_fn(x_train, y_train, solver='svd'): model = LinearDiscriminantAnalysis(solver=solver) model.fit(x_train, y_train) return model  result_dict['survived ~ linear_discriminant_analysis'] = build_model(linear_discriminant_fn, 'Survived', FEATURES, titanic_df) compare_results()  Classification: survived ~ logistic Training data accuracy 0.8101933216168717 precision 0.8020304568527918 recall 0.6960352422907489 accuracy_count 461 Test data accuracy 0.7692307692307693 precision 0.7692307692307693 recall 0.6557377049180327 accuracy_count 110 Classification: survived ~ linear_discriminant_analysis Training data accuracy 0.8014059753954306 precision 0.7782805429864253 recall 0.7288135593220338 accuracy_count 456 Test data accuracy 0.7482517482517482 precision 0.6739130434782609 recall 0.5961538461538461 accuracy_count 107  result_dict['survived ~ linear_discriminant_analysis'] = build_model(linear_discriminant_fn, 'Survived', FEATURES[0:-1], titanic_df) compare_results()  Classification: survived ~ logistic Training data accuracy 0.8101933216168717 precision 0.8020304568527918 recall 0.6960352422907489 accuracy_count 461 Test data accuracy 0.7692307692307693 precision 0.7692307692307693 recall 0.6557377049180327 accuracy_count 110 Classification: survived ~ linear_discriminant_analysis Training data accuracy 0.8101933216168717 precision 0.7887323943661971 recall 0.7272727272727273 accuracy_count 461 Test data accuracy 0.7622377622377622 precision 0.7555555555555555 recall 0.5964912280701754 accuracy_count 109  Quadratic Discriminant Analysis def quadratic_discriminant_fn(x_train, y_train): model = QuadraticDiscriminantAnalysis() model.fit(x_train, y_train) return model  result_dict['survived ~ quadratic_discriminant_analysis'] = build_model(quadratic_discriminant_fn, 'Survived', FEATURES[0:-1], titanic_df) compare_results()  Classification: survived ~ logistic Training data accuracy 0.8101933216168717 precision 0.8020304568527918 recall 0.6960352422907489 accuracy_count 461 Test data accuracy 0.7692307692307693 precision 0.7692307692307693 recall 0.6557377049180327 accuracy_count 110 Classification: survived ~ linear_discriminant_analysis Training data accuracy 0.8101933216168717 precision 0.7887323943661971 recall 0.7272727272727273 accuracy_count 461 Test data accuracy 0.7622377622377622 precision 0.7555555555555555 recall 0.5964912280701754 accuracy_count 109 Classification: survived ~ quadratic_discriminant_analysis Training data accuracy 0.7943760984182777 precision 0.7594339622641509 recall 0.7092511013215859 accuracy_count 452 Test data accuracy 0.8251748251748252 precision 0.8103448275862069 recall 0.7704918032786885 accuracy_count 118  SGD def sgd_fn(x_train, y_train, max_iter=1000, tol=1e-3): model = SGDClassifier(max_iter=max_iter, tol=tol) model.fit(x_train, y_train) return model  result_dict['survived ~ sgd'] = build_model(sgd_fn, 'Survived', FEATURES, titanic_df) compare_results()  Classification: survived ~ logistic Training data accuracy 0.8101933216168717 precision 0.8020304568527918 recall 0.6960352422907489 accuracy_count 461 Test data accuracy 0.7692307692307693 precision 0.7692307692307693 recall 0.6557377049180327 accuracy_count 110 Classification: survived ~ linear_discriminant_analysis Training data accuracy 0.8101933216168717 precision 0.7887323943661971 recall 0.7272727272727273 accuracy_count 461 Test data accuracy 0.7622377622377622 precision 0.7555555555555555 recall 0.5964912280701754 accuracy_count 109 Classification: survived ~ quadratic_discriminant_analysis Training data accuracy 0.7943760984182777 precision 0.7594339622641509 recall 0.7092511013215859 accuracy_count 452 Test data accuracy 0.8251748251748252 precision 0.8103448275862069 recall 0.7704918032786885 accuracy_count 118 Classification: survived ~ sgd Training data accuracy 0.7504393673110721 precision 0.6554054054054054 recall 0.8290598290598291 accuracy_count 427 Test data accuracy 0.7272727272727273 precision 0.6 recall 0.8333333333333334 accuracy_count 104  SVC Lineal https://scikit-learn.org/stable/modules/generated/sklearn.svm.LinearSVC.html\n SVC con kernel lineal dual=False cuando el numero de muestras \u0026gt; numero de caracteristicas  def linear_svc_fn(x_train, y_train, C=1.0, max_iter=1000, tol=1e-3): model = LinearSVC(C=C, max_iter=max_iter, tol=tol, dual=False) model.fit(x_train, y_train) return model  result_dict['survived ~ linear_svc'] = build_model(linear_svc_fn, 'Survived', FEATURES, titanic_df) compare_results()  Classification: survived ~ logistic Training data accuracy 0.8101933216168717 precision 0.8020304568527918 recall 0.6960352422907489 accuracy_count 461 Test data accuracy 0.7692307692307693 precision 0.7692307692307693 recall 0.6557377049180327 accuracy_count 110 Classification: survived ~ linear_discriminant_analysis Training data accuracy 0.8101933216168717 precision 0.7887323943661971 recall 0.7272727272727273 accuracy_count 461 Test data accuracy 0.7622377622377622 precision 0.7555555555555555 recall 0.5964912280701754 accuracy_count 109 Classification: survived ~ quadratic_discriminant_analysis Training data accuracy 0.7943760984182777 precision 0.7594339622641509 recall 0.7092511013215859 accuracy_count 452 Test data accuracy 0.8251748251748252 precision 0.8103448275862069 recall 0.7704918032786885 accuracy_count 118 Classification: survived ~ sgd Training data accuracy 0.7504393673110721 precision 0.6554054054054054 recall 0.8290598290598291 accuracy_count 427 Test data accuracy 0.7272727272727273 precision 0.6 recall 0.8333333333333334 accuracy_count 104 Classification: survived ~ linear_svc Training data accuracy 0.7961335676625659 precision 0.7692307692307693 recall 0.7017543859649122 accuracy_count 453 Test data accuracy 0.7762237762237763 precision 0.7333333333333333 recall 0.7333333333333333 accuracy_count 111  Radius Neighbors Classifier def radius_neighbor_fn(x_train, y_train, radius=40.0): model = RadiusNeighborsClassifier(radius=radius) model.fit(x_train, y_train) return model  result_dict['survived ~ radius_neighbors'] = build_model(radius_neighbor_fn, 'Survived', FEATURES, titanic_df) compare_results()  Classification: survived ~ logistic Training data accuracy 0.8101933216168717 precision 0.8020304568527918 recall 0.6960352422907489 accuracy_count 461 Test data accuracy 0.7692307692307693 precision 0.7692307692307693 recall 0.6557377049180327 accuracy_count 110 Classification: survived ~ linear_discriminant_analysis Training data accuracy 0.8101933216168717 precision 0.7887323943661971 recall 0.7272727272727273 accuracy_count 461 Test data accuracy 0.7622377622377622 precision 0.7555555555555555 recall 0.5964912280701754 accuracy_count 109 Classification: survived ~ quadratic_discriminant_analysis Training data accuracy 0.7943760984182777 precision 0.7594339622641509 recall 0.7092511013215859 accuracy_count 452 Test data accuracy 0.8251748251748252 precision 0.8103448275862069 recall 0.7704918032786885 accuracy_count 118 Classification: survived ~ sgd Training data accuracy 0.7504393673110721 precision 0.6554054054054054 recall 0.8290598290598291 accuracy_count 427 Test data accuracy 0.7272727272727273 precision 0.6 recall 0.8333333333333334 accuracy_count 104 Classification: survived ~ linear_svc Training data accuracy 0.7961335676625659 precision 0.7692307692307693 recall 0.7017543859649122 accuracy_count 453 Test data accuracy 0.7762237762237763 precision 0.7333333333333333 recall 0.7333333333333333 accuracy_count 111 Classification: survived ~ radius_neighbors Training data accuracy 0.671353251318102 precision 0.7157894736842105 recall 0.2982456140350877 accuracy_count 382 Test data accuracy 0.6433566433566433 precision 0.7142857142857143 recall 0.25 accuracy_count 92  Decision Tree classifier max_depth = None [ If None, then nodes are expanded until all leaves are pure or until all leaves contain less than min_samples_split samples ]\nmax_features = None [None \u0026ndash; max_features=n_features, auto \u0026ndash; then max_features=sqrt(n_features), sqrt \u0026ndash; then max_features=sqrt(n_features), log2 \u0026ndash; then max_features=log2(n_features)]\ndef decision_tree_fn(x_train, y_train, max_depth=None, max_features=None): model = DecisionTreeClassifier(max_depth=max_depth, max_features=max_features) model.fit(x_train, y_train) return model  result_dict['survived ~ decision_tree'] = build_model(decision_tree_fn, 'Survived', FEATURES, titanic_df) compare_results()  Classification: survived ~ logistic Training data accuracy 0.8101933216168717 precision 0.8020304568527918 recall 0.6960352422907489 accuracy_count 461 Test data accuracy 0.7692307692307693 precision 0.7692307692307693 recall 0.6557377049180327 accuracy_count 110 Classification: survived ~ linear_discriminant_analysis Training data accuracy 0.8101933216168717 precision 0.7887323943661971 recall 0.7272727272727273 accuracy_count 461 Test data accuracy 0.7622377622377622 precision 0.7555555555555555 recall 0.5964912280701754 accuracy_count 109 Classification: survived ~ quadratic_discriminant_analysis Training data accuracy 0.7943760984182777 precision 0.7594339622641509 recall 0.7092511013215859 accuracy_count 452 Test data accuracy 0.8251748251748252 precision 0.8103448275862069 recall 0.7704918032786885 accuracy_count 118 Classification: survived ~ sgd Training data accuracy 0.7504393673110721 precision 0.6554054054054054 recall 0.8290598290598291 accuracy_count 427 Test data accuracy 0.7272727272727273 precision 0.6 recall 0.8333333333333334 accuracy_count 104 Classification: survived ~ linear_svc Training data accuracy 0.7961335676625659 precision 0.7692307692307693 recall 0.7017543859649122 accuracy_count 453 Test data accuracy 0.7762237762237763 precision 0.7333333333333333 recall 0.7333333333333333 accuracy_count 111 Classification: survived ~ radius_neighbors Training data accuracy 0.671353251318102 precision 0.7157894736842105 recall 0.2982456140350877 accuracy_count 382 Test data accuracy 0.6433566433566433 precision 0.7142857142857143 recall 0.25 accuracy_count 92 Classification: survived ~ decision_tree Training data accuracy 0.9894551845342706 precision 1.0 recall 0.9737991266375546 accuracy_count 563 Test data accuracy 0.6993006993006993 precision 0.6538461538461539 recall 0.576271186440678 accuracy_count 100  Naive Bayes def naive_bayes_fn(x_train,y_train, priors=None): model = GaussianNB(priors=priors) model.fit(x_train, y_train) return model  result_dict['survived ~ naive_bayes'] = build_model(naive_bayes_fn, 'Survived', FEATURES, titanic_df) compare_results()  Classification: survived ~ logistic Training data accuracy 0.8101933216168717 precision 0.8020304568527918 recall 0.6960352422907489 accuracy_count 461 Test data accuracy 0.7692307692307693 precision 0.7692307692307693 recall 0.6557377049180327 accuracy_count 110 Classification: survived ~ linear_discriminant_analysis Training data accuracy 0.8101933216168717 precision 0.7887323943661971 recall 0.7272727272727273 accuracy_count 461 Test data accuracy 0.7622377622377622 precision 0.7555555555555555 recall 0.5964912280701754 accuracy_count 109 Classification: survived ~ quadratic_discriminant_analysis Training data accuracy 0.7943760984182777 precision 0.7594339622641509 recall 0.7092511013215859 accuracy_count 452 Test data accuracy 0.8251748251748252 precision 0.8103448275862069 recall 0.7704918032786885 accuracy_count 118 Classification: survived ~ sgd Training data accuracy 0.7504393673110721 precision 0.6554054054054054 recall 0.8290598290598291 accuracy_count 427 Test data accuracy 0.7272727272727273 precision 0.6 recall 0.8333333333333334 accuracy_count 104 Classification: survived ~ linear_svc Training data accuracy 0.7961335676625659 precision 0.7692307692307693 recall 0.7017543859649122 accuracy_count 453 Test data accuracy 0.7762237762237763 precision 0.7333333333333333 recall 0.7333333333333333 accuracy_count 111 Classification: survived ~ radius_neighbors Training data accuracy 0.671353251318102 precision 0.7157894736842105 recall 0.2982456140350877 accuracy_count 382 Test data accuracy 0.6433566433566433 precision 0.7142857142857143 recall 0.25 accuracy_count 92 Classification: survived ~ decision_tree Training data accuracy 0.9894551845342706 precision 1.0 recall 0.9737991266375546 accuracy_count 563 Test data accuracy 0.6993006993006993 precision 0.6538461538461539 recall 0.576271186440678 accuracy_count 100 Classification: survived ~ naive_bayes Training data accuracy 0.7644991212653779 precision 0.7032520325203252 recall 0.7393162393162394 accuracy_count 435 Test data accuracy 0.7202797202797203 precision 0.6206896551724138 recall 0.6666666666666666 accuracy_count 103  Hyperparameter tunning (Optimizacion de hiperparametros) titanic_df = pd.read_csv('datasets/titanic_processed.csv') titanic_df.head()   .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; }  \n  Survived Pclass Sex Age SibSp Parch Fare Embarked_C Embarked_Q Embarked_S     0 1 3 1 25.0 1 0 7.7750 0 0 1   1 0 3 1 27.0 1 0 14.4542 1 0 0   2 0 3 1 51.0 0 0 7.0542 0 0 1   3 0 3 1 19.0 0 0 7.8958 0 0 1   4 1 2 1 34.0 0 0 13.0000 0 0 1     X = titanic_df.drop('Survived', axis=1) Y = titanic_df['Survived'] x_train, x_test, y_train, y_test = train_test_split(X, Y, test_size=0.2)  def summarize_classification(y_test, y_pred): acc = accuracy_score(y_test, y_pred, normalize=True) num_acc = accuracy_score(y_test, y_pred, normalize=False) prec = precision_score(y_test, y_pred) recall = recall_score(y_test, y_pred) print(\u0026quot;Test data count: \u0026quot;,len(y_test)) print(\u0026quot;accuracy_count : \u0026quot; , num_acc) print(\u0026quot;accuracy_score : \u0026quot; , acc) print(\u0026quot;precision_score : \u0026quot; , prec) print(\u0026quot;recall_score : \u0026quot;, recall) print()  Decision Tree from sklearn.model_selection import GridSearchCV parameters = {'max_depth': [2, 4, 5, 7, 9, 10]} grid_search = GridSearchCV(DecisionTreeClassifier(), parameters, cv=3, return_train_score=True) grid_search.fit(x_train, y_train) grid_search.best_params_  {'max_depth': 7}  for i in range(6): print('Parameters: ', grid_search.cv_results_['params'][i]) print('Mean Test Score: ', grid_search.cv_results_['mean_test_score'][i]) print('Rank: ', grid_search.cv_results_['rank_test_score'][i])  Parameters: {'max_depth': 2} Mean Test Score: 0.797930010210712 Rank: 2 Parameters: {'max_depth': 4} Mean Test Score: 0.7978371855564838 Rank: 3 Parameters: {'max_depth': 5} Mean Test Score: 0.7855657662675206 Rank: 4 Parameters: {'max_depth': 7} Mean Test Score: 0.8102014294996751 Rank: 1 Parameters: {'max_depth': 9} Mean Test Score: 0.7768309663046504 Rank: 5 Parameters: {'max_depth': 10} Mean Test Score: 0.7733036294439802 Rank: 6  decision_tree_model = DecisionTreeClassifier( \\ max_depth = grid_search.best_params_['max_depth']).fit(x_train, y_train)  y_pred = decision_tree_model.predict(x_test)  summarize_classification(y_test, y_pred)  Test data count: 143 accuracy_count : 112 accuracy_score : 0.7832167832167832 precision_score : 0.8604651162790697 recall_score : 0.5967741935483871  Regresion logistica parameters = {'penalty': ['l1', 'l2'], 'C': [0.1, 0.4, 0.8, 1, 2, 5]} grid_search = GridSearchCV(LogisticRegression(solver='liblinear'), parameters, cv=3, return_train_score=True) grid_search.fit(x_train, y_train) grid_search.best_params_  {'C': 2, 'penalty': 'l1'}  for i in range(12): print('Parameters: ', grid_search.cv_results_['params'][i]) print('Mean Test Score: ', grid_search.cv_results_['mean_test_score'][i]) print('Rank: ', grid_search.cv_results_['rank_test_score'][i])  Parameters: {'C': 0.1, 'penalty': 'l1'} Mean Test Score: 0.7803304557690524 Rank: 12 Parameters: {'C': 0.1, 'penalty': 'l2'} Mean Test Score: 0.780339738234475 Rank: 11 Parameters: {'C': 0.4, 'penalty': 'l1'} Mean Test Score: 0.7943562610229277 Rank: 7 Parameters: {'C': 0.4, 'penalty': 'l2'} Mean Test Score: 0.7855843311983662 Rank: 10 Parameters: {'C': 0.8, 'penalty': 'l1'} Mean Test Score: 0.7978557504873294 Rank: 4 Parameters: {'C': 0.8, 'penalty': 'l2'} Mean Test Score: 0.7961199294532628 Rank: 6 Parameters: {'C': 1, 'penalty': 'l1'} Mean Test Score: 0.7978557504873294 Rank: 4 Parameters: {'C': 1, 'penalty': 'l2'} Mean Test Score: 0.7996287013830874 Rank: 3 Parameters: {'C': 2, 'penalty': 'l1'} Mean Test Score: 0.8048825768124014 Rank: 1 Parameters: {'C': 2, 'penalty': 'l2'} Mean Test Score: 0.7943376960920822 Rank: 8 Parameters: {'C': 5, 'penalty': 'l1'} Mean Test Score: 0.8031096259166435 Rank: 2 Parameters: {'C': 5, 'penalty': 'l2'} Mean Test Score: 0.7943376960920822 Rank: 8  logistic_model = LogisticRegression(solver='liblinear', \\ penalty=grid_search.best_params_['penalty'], C=grid_search.best_params_['C']). \\ fit(x_train, y_train)  y_pred = logistic_model.predict(x_test)  summarize_classification(y_test, y_pred)  Test data count: 143 accuracy_count : 110 accuracy_score : 0.7692307692307693 precision_score : 0.7543859649122807 recall_score : 0.6935483870967742  Grabar el Modelo from joblib import dump, load # libreria de serializacion # grabar el modelo en un archivo dump(logistic_model, 'logistic_model-titanic.joblib')  ['logistic_model-titanic.joblib']  Referencias https://scikit-learn.org/stable/supervised_learning.html#supervised-learning\nCheatsheet scikitlearn https://datacamp-community-prod.s3.amazonaws.com/5433fa18-9f43-44cc-b228-74672efcd116\nPhd. Jose R. Zapata\n https://joserzapata.github.io/ https://twitter.com/joserzapata  ","date":1599609600,"expirydate":-62135596800,"kind":"section","lang":"en","lastmod":1599609600,"objectID":"b1978251a73229b908ac3c3369cce89b","permalink":"https://joserzapata.github.io/courses/python-ciencia-datos/clasificacion/","publishdate":"2020-09-09T00:00:00Z","relpermalink":"/courses/python-ciencia-datos/clasificacion/","section":"courses","summary":"Ejemplo Regresion con Python","tags":null,"title":"Clasificacion con Scikit Learn","type":"book"},{"authors":null,"categories":null,"content":"Por Jose R. Zapata\n\nImportar librerias\nimport pandas as pd import matplotlib import matplotlib.pyplot as plt import numpy as np  from sklearn import metrics from sklearn.cluster import KMeans import warnings warnings.filterwarnings(\u0026quot;ignore\u0026quot;)  Informacion de los datos iris_df = pd.read_csv('datasets/iris.csv', skiprows=1, names = ['sepal-length', 'sepal-width', 'petal-length', 'petal-width', 'class']) iris_df.head()   .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; }  \n  sepal-length sepal-width petal-length petal-width class     0 5.1 3.5 1.4 0.2 Iris-setosa   1 4.9 3.0 1.4 0.2 Iris-setosa   2 4.7 3.2 1.3 0.2 Iris-setosa   3 4.6 3.1 1.5 0.2 Iris-setosa   4 5.0 3.6 1.4 0.2 Iris-setosa     Mezclar la base de datos\niris_df = iris_df.sample(frac=1).reset_index(drop=True) iris_df.head()   .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; }  \n  sepal-length sepal-width petal-length petal-width class     0 6.5 2.8 4.6 1.5 Iris-versicolor   1 6.8 3.2 5.9 2.3 Iris-virginica   2 5.4 3.9 1.3 0.4 Iris-setosa   3 6.7 3.0 5.0 1.7 Iris-versicolor   4 7.2 3.2 6.0 1.8 Iris-virginica     iris_df.shape  (150, 5)  iris_df[iris_df.isnull().any(axis=1)]   .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; }  \n  sepal-length sepal-width petal-length petal-width class       iris_df.describe()   .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; }  \n  sepal-length sepal-width petal-length petal-width     count 150.000000 150.000000 150.000000 150.000000   mean 5.843333 3.054000 3.758667 1.198667   std 0.828066 0.433594 1.764420 0.763161   min 4.300000 2.000000 1.000000 0.100000   25% 5.100000 2.800000 1.600000 0.300000   50% 5.800000 3.000000 4.350000 1.300000   75% 6.400000 3.300000 5.100000 1.800000   max 7.900000 4.400000 6.900000 2.500000     iris_df['class'].unique()  array(['Iris-versicolor', 'Iris-virginica', 'Iris-setosa'], dtype=object)  Preprocesamiento de datos from sklearn import preprocessing label_encoding = preprocessing.LabelEncoder() iris_df['class'] = label_encoding.fit_transform(iris_df['class'].astype(str)) iris_df.head()   .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; }  \n  sepal-length sepal-width petal-length petal-width class     0 6.5 2.8 4.6 1.5 1   1 6.8 3.2 5.9 2.3 2   2 5.4 3.9 1.3 0.4 0   3 6.7 3.0 5.0 1.7 1   4 7.2 3.2 6.0 1.8 2     iris_df.describe()   .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; }  \n  sepal-length sepal-width petal-length petal-width class     count 150.000000 150.000000 150.000000 150.000000 150.000000   mean 5.843333 3.054000 3.758667 1.198667 1.000000   std 0.828066 0.433594 1.764420 0.763161 0.819232   min 4.300000 2.000000 1.000000 0.100000 0.000000   25% 5.100000 2.800000 1.600000 0.300000 0.000000   50% 5.800000 3.000000 4.350000 1.300000 1.000000   75% 6.400000 3.300000 5.100000 1.800000 2.000000   max 7.900000 4.400000 6.900000 2.500000 2.000000     Analisis Univariable Se debe hacer un analisis de cada una de las variables y describir sus caracteristicas\nAnalisis Bivariable Scatter plot fig, ax = plt.subplots(figsize=(12, 8)) plt.scatter(iris_df['sepal-length'], iris_df['sepal-width'], s=250) plt.xlabel('sepal-length') plt.ylabel('sepal-width') plt.show()  fig, ax = plt.subplots(figsize=(12, 8)) plt.scatter(iris_df['petal-width'], iris_df['petal-length'], s=250) plt.xlabel('petal-width') plt.ylabel('petal-length') plt.show()  fig, ax = plt.subplots(figsize=(12, 8)) plt.scatter(iris_df['sepal-length'], iris_df['petal-length'], s=250) plt.xlabel('sepal-length') plt.ylabel('petal-length') plt.show()  KMeans SIMPLE iris_2D = iris_df[['sepal-length', 'petal-length']] iris_2D.sample(5)   .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; }  \n  sepal-length petal-length     133 6.4 5.3   124 5.6 3.6   111 5.8 5.1   75 4.7 1.3   31 6.7 5.2     iris_2D.shape  (150, 2)  iris_2D = np.array(iris_2D)  kmeans_model_2D = KMeans(n_clusters=3, max_iter=1000).fit(iris_2D)  kmeans_model_2D.labels_  array([2, 0, 1, 0, 0, 2, 0, 1, 2, 0, 2, 1, 1, 2, 1, 1, 0, 2, 1, 2, 2, 0, 2, 2, 2, 1, 0, 2, 1, 0, 1, 0, 1, 2, 2, 1, 0, 2, 1, 0, 2, 1, 0, 2, 0, 2, 1, 1, 2, 1, 1, 0, 0, 0, 2, 1, 2, 0, 1, 1, 1, 2, 2, 1, 2, 0, 2, 0, 1, 0, 2, 2, 1, 2, 2, 1, 0, 0, 1, 0, 1, 0, 2, 2, 0, 1, 2, 2, 1, 2, 0, 2, 1, 0, 1, 1, 1, 2, 2, 1, 0, 2, 2, 0, 2, 1, 0, 1, 2, 2, 1, 2, 1, 1, 1, 2, 0, 1, 2, 0, 1, 1, 1, 2, 2, 2, 2, 1, 0, 1, 2, 2, 2, 0, 0, 2, 2, 0, 2, 0, 1, 2, 1, 0, 1, 2, 0, 0, 1, 2], dtype=int32)  centroids_2D = kmeans_model_2D.cluster_centers_ centroids_2D  array([[6.83902439, 5.67804878], [5.00784314, 1.49411765], [5.87413793, 4.39310345]])  fig, ax = plt.subplots(figsize=(12, 8)) plt.scatter(centroids_2D[:,0], centroids_2D[:,1], c='r', s=250, marker='s') for i in range(len(centroids_2D)): plt.annotate(i, (centroids_2D[i][0], centroids_2D[i][1]), fontsize=30)  iris_labels = iris_df['class']  print(\u0026quot;Homogeneity_score: \u0026quot;, metrics.homogeneity_score(iris_labels, kmeans_model_2D.labels_)) print(\u0026quot;Completeness_score: \u0026quot;, metrics.completeness_score(iris_labels, kmeans_model_2D.labels_)) print(\u0026quot;v_measure_score: \u0026quot;, metrics.v_measure_score(iris_labels, kmeans_model_2D.labels_)) print(\u0026quot;Adjusted_rand_score: \u0026quot;, metrics.adjusted_rand_score(iris_labels, kmeans_model_2D.labels_)) print(\u0026quot;Adjusted_mutual_info_score: \u0026quot;, metrics.adjusted_mutual_info_score(iris_labels, kmeans_model_2D.labels_)) print(\u0026quot;Silhouette_score: \u0026quot;, metrics.silhouette_score(iris_2D, kmeans_model_2D.labels_))  Homogeneity_score: 0.7033177646052958 Completeness_score: 0.7096993707802843 v_measure_score: 0.706494157075837 Adjusted_rand_score: 0.6988627672348092 Adjusted_mutual_info_score: 0.7028024531409133 Silhouette_score: 0.5890612473759281  colors = ['yellow','blue','green'] plt.figure(figsize=(12, 8)) plt.scatter(iris_df['sepal-length'], iris_df['petal-length'], c=iris_df['class'], s=200, cmap=matplotlib.colors.ListedColormap(colors), alpha=0.5) plt.scatter(centroids_2D[:,0], centroids_2D[:,1], c='r', s=250, marker='s') for i in range(len(centroids_2D)): plt.annotate( i, (centroids_2D[i][0], centroids_2D[i][1]), fontsize=30)  iris_features = iris_df.drop('class', axis=1) iris_features.head()   .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; }  \n  sepal-length sepal-width petal-length petal-width     0 6.5 2.8 4.6 1.5   1 6.8 3.2 5.9 2.3   2 5.4 3.9 1.3 0.4   3 6.7 3.0 5.0 1.7   4 7.2 3.2 6.0 1.8     iris_labels = iris_df['class'] iris_labels.sample(5)  81 2 136 1 86 1 19 1 61 1 Name: class, dtype: int64  kmeans_model = KMeans(n_clusters=3).fit(iris_features)  kmeans_model.labels_  array([2, 0, 1, 0, 0, 2, 0, 1, 2, 0, 2, 1, 1, 2, 1, 1, 0, 2, 1, 2, 2, 2, 2, 2, 2, 1, 0, 2, 1, 0, 1, 0, 1, 2, 2, 1, 0, 2, 1, 0, 2, 1, 0, 2, 0, 2, 1, 1, 2, 1, 1, 2, 2, 0, 2, 1, 2, 0, 1, 1, 1, 2, 2, 1, 2, 0, 2, 0, 1, 0, 2, 2, 1, 2, 2, 1, 0, 0, 1, 0, 1, 0, 2, 2, 0, 1, 2, 2, 1, 2, 0, 2, 1, 0, 1, 1, 1, 2, 2, 1, 0, 2, 2, 0, 2, 1, 0, 1, 2, 2, 1, 2, 1, 1, 1, 2, 0, 1, 2, 0, 2, 1, 1, 2, 2, 2, 2, 1, 0, 1, 2, 2, 2, 0, 0, 2, 2, 0, 2, 0, 1, 2, 1, 0, 1, 2, 0, 0, 1, 2], dtype=int32)  kmeans_model.cluster_centers_  array([[6.85 , 3.07368421, 5.74210526, 2.07105263], [5.006 , 3.418 , 1.464 , 0.244 ], [5.9016129 , 2.7483871 , 4.39354839, 1.43387097]])  print(\u0026quot;Homogeneity_score: \u0026quot;, metrics.homogeneity_score(iris_labels, kmeans_model.labels_)) print(\u0026quot;Completeness_score: \u0026quot;, metrics.completeness_score(iris_labels, kmeans_model.labels_)) print(\u0026quot;v_measure_score: \u0026quot;, metrics.v_measure_score(iris_labels, kmeans_model.labels_)) print(\u0026quot;Adjusted_rand_score: \u0026quot;, metrics.adjusted_rand_score(iris_labels, kmeans_model.labels_)) print(\u0026quot;Adjusted_mutual_info_score: \u0026quot;, metrics.adjusted_mutual_info_score(iris_labels, kmeans_model.labels_)) print(\u0026quot;Silhouette_score: \u0026quot;, metrics.silhouette_score(iris_features, kmeans_model.labels_))  Homogeneity_score: 0.7514854021988338 Completeness_score: 0.7649861514489815 v_measure_score: 0.7581756800057784 Adjusted_rand_score: 0.7302382722834697 Adjusted_mutual_info_score: 0.7551191675800484 Silhouette_score: 0.5525919445499757  Clustering con varios modelos from sklearn import metrics from sklearn.cluster import KMeans from sklearn.cluster import AgglomerativeClustering from sklearn.cluster import DBSCAN from sklearn.cluster import MeanShift from sklearn.cluster import Birch from sklearn.cluster import AffinityPropagation from sklearn.cluster import MiniBatchKMeans import warnings warnings.filterwarnings(\u0026quot;ignore\u0026quot;)  def build_model(clustering_model, data, labels): model = clustering_model(data) print('homo\\tcompl\\tv-meas\\tARI\\tAMI\\tsilhouette') print(50 * '-') print('%.3f\\t%.3f\\t%.3f\\t%.3f\\t%.3f\\t%.3f' %(metrics.homogeneity_score(labels, model.labels_), metrics.completeness_score(labels, model.labels_), metrics.v_measure_score(labels, model.labels_), metrics.adjusted_rand_score(labels, model.labels_), metrics.adjusted_mutual_info_score(labels, model.labels_), metrics.silhouette_score(data, model.labels_)))  Kmeans def k_means(data, n_clusters=3, max_iter=1000): model = KMeans(n_clusters=n_clusters, max_iter=max_iter).fit(data) return model  build_model(k_means, iris_features, iris_labels)  homo\tcompl\tv-meas\tARI\tAMI\tsilhouette -------------------------------------------------- 0.751\t0.765\t0.758\t0.730\t0.755\t0.553  Agglomerative def agglomerative_fn(data, n_clusters=3): model = AgglomerativeClustering(n_clusters = n_clusters).fit(data) return model  build_model(agglomerative_fn, iris_features, iris_labels)  homo\tcompl\tv-meas\tARI\tAMI\tsilhouette -------------------------------------------------- 0.761\t0.780\t0.770\t0.731\t0.767\t0.554  Dbscan def dbscan_fn(data, eps=0.45, min_samples=4): model = DBSCAN(eps=eps, min_samples=min_samples).fit(data) return model  build_model(dbscan_fn, iris_features, iris_labels)  homo\tcompl\tv-meas\tARI\tAMI\tsilhouette -------------------------------------------------- 0.577\t0.609\t0.593\t0.508\t0.584\t0.372  Mean Shift def mean_shift_fn(data, bandwidth=0.85): model = MeanShift(bandwidth=bandwidth).fit(data) return model  build_model(mean_shift_fn, iris_features, iris_labels)  homo\tcompl\tv-meas\tARI\tAMI\tsilhouette -------------------------------------------------- 0.760\t0.772\t0.766\t0.744\t0.763\t0.551  Birch def birch_fn(data, n_clusters=3): model = Birch(n_clusters=n_clusters).fit(data) return model  build_model(birch_fn, iris_features, iris_labels)  homo\tcompl\tv-meas\tARI\tAMI\tsilhouette -------------------------------------------------- 0.635\t0.792\t0.705\t0.566\t0.700\t0.534  Affinity Propagation def affinity_propagation_fn(data, damping=0.6, max_iter=1000): model = AffinityPropagation(damping=damping, max_iter=max_iter).fit(data) return model  build_model(affinity_propagation_fn, iris_features, iris_labels)  homo\tcompl\tv-meas\tARI\tAMI\tsilhouette -------------------------------------------------- 0.851\t0.492\t0.623\t0.437\t0.612\t0.349  Mini Batch Kmeans def mini_batch_kmeans_fn(data, n_clusters=3, max_iter=1000): model = MiniBatchKMeans(n_clusters=n_clusters, max_iter=max_iter, batch_size=20).fit(data) return model  build_model(mini_batch_kmeans_fn, iris_features, iris_labels)  homo\tcompl\tv-meas\tARI\tAMI\tsilhouette -------------------------------------------------- 0.745\t0.754\t0.750\t0.729\t0.746\t0.549  Hyperparameter Tuning (optimizacion de Hiperparametros) El ejemplo se desarrollara con un dataset diferente con menos variables\nfrom sklearn import metrics from sklearn.metrics import silhouette_score from sklearn.model_selection import ParameterGrid from sklearn.cluster import KMeans from sklearn.cluster import DBSCAN from sklearn.cluster import MeanShift  drivers_df = pd.read_csv('datasets/driver_details.csv') drivers_df.head()   .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; }  \n  Driver_ID Distance_Feature Speeding_Feature     0 3423311935 71.24 28   1 3423313212 52.53 25   2 3423313724 64.54 27   3 3423311373 55.69 22   4 3423310999 54.58 25     drivers_df.shape  (4000, 3)  drivers_df[drivers_df.isnull().any(axis=1)]   .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; }  \n  Driver_ID Distance_Feature Speeding_Feature       drivers_df.describe()   .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; }  \n  Driver_ID Distance_Feature Speeding_Feature     count 4.000000e+03 4000.000000 4000.000000   mean 3.423312e+09 76.041523 10.721000   std 1.154845e+03 53.469563 13.708543   min 3.423310e+09 15.520000 0.000000   25% 3.423311e+09 45.247500 4.000000   50% 3.423312e+09 53.330000 6.000000   75% 3.423313e+09 65.632500 9.000000   max 3.423314e+09 244.790000 100.000000     drivers_features = drivers_df.drop('Driver_ID', axis=1)  KMeans clustering parameters = {'n_clusters': [2, 3, 4, 5, 10, 20, 30]} parameter_grid = ParameterGrid(parameters)  list(parameter_grid)  [{'n_clusters': 2}, {'n_clusters': 3}, {'n_clusters': 4}, {'n_clusters': 5}, {'n_clusters': 10}, {'n_clusters': 20}, {'n_clusters': 30}]  best_score = -1 model = KMeans()  for g in parameter_grid: model.set_params(**g) model.fit(drivers_features) ss = metrics.silhouette_score(drivers_features, model.labels_) print('Parametro: ', g, 'Score: ', ss) if ss \u0026gt; best_score: best_score = ss best_grid = g  Parametro: {'n_clusters': 2} Score: 0.8490223286225532 Parametro: {'n_clusters': 3} Score: 0.8231396834167266 Parametro: {'n_clusters': 4} Score: 0.5911323766293183 Parametro: {'n_clusters': 5} Score: 0.5128161654454148 Parametro: {'n_clusters': 10} Score: 0.41891069100637685 Parametro: {'n_clusters': 20} Score: 0.3639487789194615 Parametro: {'n_clusters': 30} Score: 0.3540375785241541  best_grid  {'n_clusters': 2}  fig, ax = plt.subplots(figsize=(12, 8)) plt.scatter(drivers_features['Distance_Feature'], drivers_features['Speeding_Feature'], s=250) plt.xlabel('Distance_Feature') plt.ylabel('Speeding_Feature') plt.show()  DBSCAN clustering parameters = {'eps': [0.9, 1.0, 5.0, 10.0, 12.0, 14.0, 20.0], 'min_samples': [5, 7, 10, 12]} parameter_grid = ParameterGrid(parameters) list(parameter_grid)  [{'eps': 0.9, 'min_samples': 5}, {'eps': 0.9, 'min_samples': 7}, {'eps': 0.9, 'min_samples': 10}, {'eps': 0.9, 'min_samples': 12}, {'eps': 1.0, 'min_samples': 5}, {'eps': 1.0, 'min_samples': 7}, {'eps': 1.0, 'min_samples': 10}, {'eps': 1.0, 'min_samples': 12}, {'eps': 5.0, 'min_samples': 5}, {'eps': 5.0, 'min_samples': 7}, {'eps': 5.0, 'min_samples': 10}, {'eps': 5.0, 'min_samples': 12}, {'eps': 10.0, 'min_samples': 5}, {'eps': 10.0, 'min_samples': 7}, {'eps': 10.0, 'min_samples': 10}, {'eps': 10.0, 'min_samples': 12}, {'eps': 12.0, 'min_samples': 5}, {'eps': 12.0, 'min_samples': 7}, {'eps': 12.0, 'min_samples': 10}, {'eps': 12.0, 'min_samples': 12}, {'eps': 14.0, 'min_samples': 5}, {'eps': 14.0, 'min_samples': 7}, {'eps': 14.0, 'min_samples': 10}, {'eps': 14.0, 'min_samples': 12}, {'eps': 20.0, 'min_samples': 5}, {'eps': 20.0, 'min_samples': 7}, {'eps': 20.0, 'min_samples': 10}, {'eps': 20.0, 'min_samples': 12}]  model = DBSCAN() best_score = -1  for g in parameter_grid: model.set_params(**g) model.fit(drivers_features) ss = metrics.silhouette_score(drivers_features, model.labels_) print('Parametro: ', g, 'Score: ', ss) if ss \u0026gt; best_score: best_score = ss best_grid = g  Parametro: {'eps': 0.9, 'min_samples': 5} Score: -0.6057173612292268 Parametro: {'eps': 0.9, 'min_samples': 7} Score: -0.4265046999507063 Parametro: {'eps': 0.9, 'min_samples': 10} Score: -0.39254168253371013 Parametro: {'eps': 0.9, 'min_samples': 12} Score: -0.4286838741223884 Parametro: {'eps': 1.0, 'min_samples': 5} Score: -0.6155746493060738 Parametro: {'eps': 1.0, 'min_samples': 7} Score: -0.41637001640330673 Parametro: {'eps': 1.0, 'min_samples': 10} Score: -0.3837814631696031 Parametro: {'eps': 1.0, 'min_samples': 12} Score: -0.38648235283744914 Parametro: {'eps': 5.0, 'min_samples': 5} Score: 0.31011275260225 Parametro: {'eps': 5.0, 'min_samples': 7} Score: 0.7820011223700856 Parametro: {'eps': 5.0, 'min_samples': 10} Score: 0.7974222681120255 Parametro: {'eps': 5.0, 'min_samples': 12} Score: 0.7914367881923341 Parametro: {'eps': 10.0, 'min_samples': 5} Score: 0.7598056658175874 Parametro: {'eps': 10.0, 'min_samples': 7} Score: 0.8157570071704705 Parametro: {'eps': 10.0, 'min_samples': 10} Score: 0.8107405850782263 Parametro: {'eps': 10.0, 'min_samples': 12} Score: 0.7826641175724478 Parametro: {'eps': 12.0, 'min_samples': 5} Score: 0.8082887021398691 Parametro: {'eps': 12.0, 'min_samples': 7} Score: 0.8006933163754029 Parametro: {'eps': 12.0, 'min_samples': 10} Score: 0.8177778536465214 Parametro: {'eps': 12.0, 'min_samples': 12} Score: 0.8155661587264617 Parametro: {'eps': 14.0, 'min_samples': 5} Score: 0.8111072866552332 Parametro: {'eps': 14.0, 'min_samples': 7} Score: 0.8121719747215577 Parametro: {'eps': 14.0, 'min_samples': 10} Score: 0.8029471072047811 Parametro: {'eps': 14.0, 'min_samples': 12} Score: 0.8178938395610874 Parametro: {'eps': 20.0, 'min_samples': 5} Score: 0.8490223286225532 Parametro: {'eps': 20.0, 'min_samples': 7} Score: 0.8490223286225532 Parametro: {'eps': 20.0, 'min_samples': 10} Score: 0.8192119040131286 Parametro: {'eps': 20.0, 'min_samples': 12} Score: 0.8156567891999053  best_grid  {'eps': 20.0, 'min_samples': 5}  model.set_params(**best_grid) model.fit(drivers_features)  DBSCAN(algorithm='auto', eps=20.0, leaf_size=30, metric='euclidean', metric_params=None, min_samples=5, n_jobs=None, p=None)  len(model.labels_)  4000  n_clusters = len(set(model.labels_)) - (1 if -1 in model.labels_ else 0) n_clusters  2  n_noise = list(model.labels_).count(-1) n_noise  0  another_grid = {'eps': 5.0, 'min_samples': 5} model.set_params(**another_grid) model.fit(drivers_features) n_clusters = len(set(model.labels_)) - (1 if -1 in model.labels_ else 0) print('Numero de clusters: ', n_clusters) n_noise = list(model.labels_).count(-1) print('Puntos ruido: ', n_noise)  Numero de clusters: 7 Puntos ruido: 117  another_grid = {'eps': 5.0, 'min_samples': 7} model.set_params(**another_grid) model.fit(drivers_features) n_clusters = len(set(model.labels_)) - (1 if -1 in model.labels_ else 0) print('Numero de Clusters: ', n_clusters) n_noise = list(model.labels_).count(-1) print('Puntos Ruido: ', n_noise)  Numero de Clusters: 3 Puntos Ruido: 157  MeanShift clustering https://github.com/scikit-learn/scikit-learn/blob/master/sklearn/cluster/mean_shift_.py#L4\nhttps://stats.stackexchange.com/questions/86324/name-of-algorithm-or-paper-that-scikit-learn-cluster-estimate-bandwidth-func\nfrom sklearn.cluster import estimate_bandwidth estimate_bandwidth(drivers_features)  33.960524729584314  model = MeanShift(bandwidth=estimate_bandwidth(drivers_features)).fit(drivers_features) metrics.silhouette_score(drivers_features, model.labels_)  0.8231396834167266  Referencias https://scikit-learn.org/stable/modules/clustering.html#clustering\nCheatsheet scikitlearn https://datacamp-community-prod.s3.amazonaws.com/5433fa18-9f43-44cc-b228-74672efcd116\nPhd. Jose R. Zapata\n https://joserzapata.github.io/ https://twitter.com/joserzapata  ","date":1599609600,"expirydate":-62135596800,"kind":"section","lang":"en","lastmod":1599609600,"objectID":"b33c757fc8ea826371a9b9f181547d0d","permalink":"https://joserzapata.github.io/courses/python-ciencia-datos/clustering/","publishdate":"2020-09-09T00:00:00Z","relpermalink":"/courses/python-ciencia-datos/clustering/","section":"courses","summary":"Ejemplo Clustering con Python","tags":null,"title":"Clustering con Scikit Learn","type":"book"},{"authors":["Estefanía Cano","Fernando Mora-Ángel","Gustavo López Gil","Jose R. Zapata","Antonio Escamilla","Juan Fernando Alzate Londoño","Moisés Betancur Peláez"],"categories":[],"content":"","date":1601308662,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1601308662,"objectID":"5882c0cabb77caebb7927c8cdec2fbcb","permalink":"https://joserzapata.github.io/publication/ismir2020/","publishdate":"2020-09-28T10:57:42-05:00","relpermalink":"/publication/ismir2020/","section":"publication","summary":"","tags":["Computational Musicology","Colombian Music","Beat-tracking","Music-information-retrieval","Python","ISMIR"],"title":"Sesquialtera in the Colombian Bambuco: Perception and Estimation of Beat and Meter","type":"publication"},{"authors":["Jose R. Zapata"],"categories":["Data-Science"],"content":" Esta es una traduccion propia al español del Apendice B (Machine Learning Project Checklist) del libro Hands-On Machine Learning with Scikit-Learn, Keras, and TensorFlow: Concepts, Tools, and Techniques to Build Intelligent Systems 2nd Edition de Aurélien Géron con algunos pasos propios agregados o que he encontrado en otros libros o cursos que he realizado.\nEste Libro me ha gustado mucho, para mi es el libro practico mas completo sobre machine learning con python que he leido, tiene una excelente estructura, codigo en python muy bien explicado, ademas muchos tips y sugerencias para realizar un proyecto de machine learning.\nEl libro esta acompañado por un repositorio con Jupyter Notebooks: https://github.com/ageron/handson-ml2\n\nTable of Contents  1. Definir el problema y mirar el panorama general. 2. Obténer los datos 3. Explorar los datos para obtener información. 4. Preparación de los datos. 5. Exploración y selección de modelos 6. Afinar los modelos. 7. Presentacion de la solución. 8. Desplegar, monitorear y mantener el sistema. Referencias    Esta lista de verificación puede ser una guia paso a paso para proyectos de Machine Learning.\n1. Definir el problema y mirar el panorama general.  Definir el objetivo en términos del negocio. ¿Cómo se usará su solución? ¿Cuáles son las soluciones actuales (si las hay)? ¿Cómo se debe enmarcar este problema (supervisado / no supervisado, en línea / fuera de línea, etc.) ¿Cómo se debe medir el desempeño o el rendimiento de la solucion? ¿La medida de desempeño está alineada con el objetivo del negocio? ¿Cuál sería el desempeño o rendimiento mínimo necesario para alcanzar el objetivo del negocio? ¿Cuáles son los problemas parecidos? ¿Se puede reutilizar experiencias o herramientas ya creadas? ¿Hay experiencia del problema disponible? ¿Cómo se puede resolver el problema manualmente? Hacer un listado de los supuestos que hay hasta este momento. Verificar los supuestos si es posible.  2. Obténer los datos  Nota: automatizar tanto como sea posible este proceso para que pueda obtener fácilmente datos nuevos.    Enumere los datos que necesita y la cantidad que necesita. Busque y documente dónde se pueden obtener los datos. Compruebe cuánto espacio de almacenamiento ocuparán los datos. Verifique las limitaciones legales y obtener autorización a los datos si es necesario. Obtener autorizaciones de acceso a los datos. Reservar suficiente espacio de almacenamiento para el proyecto. Obtener los datos. Convertir los datos a un formato que se pueda manipular fácilmente (sin cambiar los datos en sí). Asegúrarse de que la información confidencial se elimine o se proteja (por ejemplo, anonimizar los datos). Verificar el tamaño y el tipo de datos (series de tiempo, muestra de datos, geoposicionamiento, etc.). Separar un conjunto de datos prueba, dejarlos a un lado y nunca mirarlos.  3. Explorar los datos para obtener información.  Nota: intente obtener información de un experto en el tema para estos pasos.    Crear una copia de los datos para explorarlos (muestreándolos a un tamaño manejable si necesario). Crar un Jupyter Notebook para mantener un registro de la exploración de los datos. Estudiar cada atributo y sus características (Analisis Univariable):  Nombre Tipo de dato (categórico, int / float, acotado / no acotado, texto, estructurado, etc.) porcentaje (%) de valores faltantes. Ruido y tipo de ruido (estocástico, valores atípicos, errores de redondeo, etc.) ¿Son posiblemente útiles para el proyecto? Tipo de distribución (gaussiana, uniforme, logarítmica, etc.)  Para los proyectos de aprendizaje supervisado, identifique los atributos objetivo (target). Visualizacion de los datos. Estudiar las correlaciones entre atributos (Analisis Bivariable). Estudiar cómo resolver el problema manualmente. Identificar las transformaciones que tal vez se puedan aplicar. Identificar datos adicionales que pueden ser útiles. Documentar lo que ha aprendido.  4. Preparación de los datos. Para exponer mejor los patrones de los datos y usarlos con los algoritmos de Machine Learning.\n Notas:\n Trabaje en copias de los datos (mantenga intacto el conjunto de datos original). Escriba funciones para todas las transformaciones de datos que realice, por cinco razones:  Para que pueda preparar fácilmente los datos la próxima vez que obtenga un conjunto de datos nuevo Para que pueda aplicar estas transformaciones en proyectos futuros Para limpiar y preparar el set de datos de prueba Para limpiar y preparar nuevas instancias de datos una vez que su solución esté activa (produccion) Para que sea fácil probar diferentes formas de preparación de datos como hiperparámetros       Limpieza de datos:  Eliminar registros datos duplicados (disminuir el numero de datos) Corregir o eliminar valores atípicos (opcional). Los valores atípicos pueden separarse del dataset dependiendo del problema del proyecto (por ejemplo, deteccion de anomalias). Completar los valores faltantes (por ejemplo, con cero, media, mediana \u0026hellip;) o eliminar las filas (o columnas).  Selección de atributos (Feature Selection) (opcional): Descartar los atributos que no proporcionan información útil para el proyecto. Eliminar registros duplicados (al eliminar atributos pueden quedar registros iguales) Ingeniería de atributos (Feature Engineering), cuando sea apropiado: Discretizar las atributos continuas. Descomponer en partes los atributos (p. Ej., Categóricas, fecha / hora, etc.). Agregar transformaciones prometedoras de las atributos (por ejemplo, log(x), sqrt(x), x^2, etc.). Aplicar funciones a los datos para agregar nuevos atributos. Escalado de atributos (Feature Scaling).: estandarizar o normalizar atributos.  5. Exploración y selección de modelos  Notas:\n Si se tiene una gran cantidad de datos, es posible que desee hacer un muestreo de los datos para tener conjuntos de entrenamiento más pequeños, de esta forma se pueden entrenar varios modelos diferentes en un tiempo razonable (se debe tener en cuenta que esto penaliza modelos complejos como redes neuronales grandes o Random Forest). Una vez más, intentar automatizar estos pasos tanto como sea posible.     Entrenar muchos modelos rápidos y utilizando parámetros estándar de diferentes categorías (p. Ej., Lineales, Naive Bayes, SVM, Random Forest, redes neuronales, etc.). Medir y comparar su desempeño. Para cada modelo, utilice la validación cruzada (Cross validation) de N subconjuntos y calcule la media y la desviación estándar de la medida de rendimiento en las N evaluaciones. Analice las variables más significativas para cada algoritmo. Analice los tipos de errores que cometen los modelos. ¿Qué datos habría utilizado un humano para evitar estos errores? Realizar rapidamente una selección de atributos e ingeniería de atributos (Feature selection, Feature Engineering). Realice una o dos iteraciones rápidas más de los cinco pasos anteriores. Hacer una lista corta de los tres a cinco modelos más prometedores, prefiriendo seleccionar modelos que cometan diferentes tipos de errores (diversidad de los errores).  6. Afinar los modelos.  Notas:\n Se deberá utilizar la mayor cantidad de datos posible para este paso, especialmente a medida que avanza hacia el final del ajuste fino del modelo. Como siempre, automatizar lo que se pueda.     Ajuste los hiperparámetros (hyperparameter tunning) mediante validación cruzada (cross validation). Tratar las elecciones de transformación de datos como hiperparámetros, especialmente cuando no esta seguro de ellos (por ejemplo, ¿debería reemplazar los valores faltantes con cero o con el valor medio? ¿O simplemente dejar eliminar las filas?). A menos que haya muy pocos valores de hiperparámetros para explorar, prefiera la búsqueda aleatoria (random search) a la búsqueda de cuadrícula (grid search). Si el entrenamiento es muy largo, es posible que prefiera un enfoque de optimización bayesiano (por ejemplo, utilizando procesos previos gaussianos, como lo describen Jasper Snoek, Hugo Larochelle y Ryan Adams1. Pruebe los métodos de Ensamble (ensemble methods). La combinación de sus mejores modelos a menudo tendrá un mejor rendimiento que se ejecutan individualmente (hay mejor desempeño si hay diversidad de errores entre los modelos). Una vez que esté seguro de su modelo final, mida su rendimiento en el conjunto de prueba (test set, separado al inicio) para estimar el error de generalización.   No modifique su modelo después de medir el error de generalización: simplemente comenzaría a sobreajustar el conjunto de prueba.   7. Presentacion de la solución.  Documentar lo que ha hecho. Crear una buena presentación. Asegúrese de resaltar el panorama general del proyecto o del problema primero. Explicar por qué la solución encontrada logra el objetivo buscado. No olvidar presentar puntos interesantes que se notaron en el camino. Describir qué funcionó y qué no. Enumerar los supuestos y las limitaciones del sistema. Asegúrarse de que los hallazgos clave se comuniquen a través de hermosas visualizaciones o declaraciones fáciles de recordar (por ejemplo, \u0026ldquo;el ingreso medio es el predictor número uno de los precios de la vivienda\u0026rdquo;).  8. Desplegar, monitorear y mantener el sistema.  Preparar la solución para producción (conectar las entradas de datos de producción, escribir pruebas unitarias (unit test), etc.). Escribir código de monitoreo para verificar el rendimiento en tiempo real del sistema a intervalos regulares y activar alertas cuando se caiga o falle. Tener cuidado con la lenta degradación: los modelos tienden a \u0026ldquo;pudrirse\u0026rdquo; a medida que los datos evolucionan, el modelo va perdiendo validez en el tiempo. La medición del rendimiento puede requerir supervision humana (por ejemplo, a través de un servicio de crowdsourcing). Controlar la calidad de los datos de entrada (por ejemplo, Un sensor que funciona mal y que envía valores aleatorios, o la salida de datos de otro equipo se vuelve obsoleta). Esto es particularmente importante para los sistemas de aprendizaje en línea (online learning). Vuelva a entrenar sus modelos de forma regular con datos nuevos (automatizar lo más posible).  Referencias  Hands-On Machine Learning with Scikit-Learn, Keras, and TensorFlow: Concepts, Tools, and Techniques to Build Intelligent Systems https://github.com/ageron/handson-ml2 https://machinelearningmastery.com/machine-learning-in-python-step-by-step/ https://medium.com/@mcintyreshiv/how-to-master-python-for-machine-learning-from-scratch-a-step-by-step-tutorial-8c6569895cb0   “Practical Bayesian Optimization of Machine Learning Algorithms,” J. Snoek, H. Larochelle, R. Adams (2012) ^   ","date":1586433284,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1586433284,"objectID":"1d594254846e534bf97169f0445002f4","permalink":"https://joserzapata.github.io/post/lista-proyecto-machine-learning/","publishdate":"2020-04-09T06:54:44-05:00","relpermalink":"/post/lista-proyecto-machine-learning/","section":"post","summary":"Checklist y preguntas para realizar un proyecto de machine learning","tags":["Data-Science"],"title":"Paso a paso en un Proyecto Machine Learning","type":"post"},{"authors":["Jose R. Zapata"],"categories":["Data-Science"],"content":"                 He visto en las redes sociales varias visualizaciones de los datos del COVID 19 y queria realizarlos en Python para tener la actualizacion de las graficas actualizadas cada dia, y ademas practicar el uso de plotly para visualizacion interactiva.\nPrincipalmente los datos que se tienen es del numero de personas contegiadas y personas muertas quiero visualizar los datos de personas recuperadas y casos activos.\nPueden interactuar con las graficas con el mouse y las Graficas se actualizaran diariamente con los nuevos datos!\nInformacion extraida de 2019 Novel Coronavirus COVID-19 (2019-nCoV) Data Repository by Johns Hopkins CSSE\nhttps://github.com/CSSEGISandData/COVID-19\nActualizaciones:\n 27/May/2020 Se Agregar los datos de las personas recuperadas y se calculan los casos Activos 29/May/2020 Se agrega Bar chart race 25/Sep/2020 Mapa Mundial de Confirmados por Pais con choropleth  \nTable of Contents  Paquetes de Python y Datos  Paquetes de Python Importar datos Datos CSSEGISandData/COVID-19  Eliminar Ubicacion Casos Activos Consolidar datos Datos Mundiales   Covid19 en el Mundo  Evolucion Animada de Casos Activos por Pais Visualizacion con Plotly Valores Mundiales de Casos Confirmados, Activos, Recuperados y Muertos Mapa Mundial de Confirmados por Pais Confirmados vs Muertos por pais Progresion Mundial en el Tiempo de Confirmados y Muertos Total Casos Confirmados de COVID 19 por Pais Total Casos Confirmados de COVID 19 por Pais (Excluyendo los 8 mas infectados) Animacion del Mapa de Evolucion Temporal del Codiv 19  Covid 19 en Colombia  Numero de Casos COVID 19 en Colombia  Actualizacion de las Graficas cada 24 Horas Codigo Fuente Jupyter notebook Refencias    Paquetes de Python y Datos Paquetes de Python !pip install chart_studio -q  import pandas as pd import plotly.express as px import numpy as np import chart_studio  Para subir las graficas interactivas de plotly a chart studio\n#chart-studio api username = '' # your username api_key = '' # your api api_key chart_studio.tools.set_credentials_file(username=username, api_key=api_key) import chart_studio.plotly as py  Importar datos confirmed = pd.read_csv('https://github.com/CSSEGISandData/COVID-19/raw/master/csse_covid_19_data/csse_covid_19_time_series/time_series_covid19_confirmed_global.csv') death = pd.read_csv('https://github.com/CSSEGISandData/COVID-19/raw/master/csse_covid_19_data/csse_covid_19_time_series/time_series_covid19_deaths_global.csv') recovered = pd.read_csv('https://github.com/CSSEGISandData/COVID-19/raw/master/csse_covid_19_data/csse_covid_19_time_series/time_series_covid19_recovered_global.csv')  Datos CSSEGISandData/COVID-19 Descripcion de los datos en ingles\nProvince/State: China - province name; US/Canada/Australia/ - city name, state/province name; Others - name of the event (e.g., \u0026ldquo;Diamond Princess\u0026rdquo; cruise ship); other countries - blank.\nCountry/Region: country/region name conforming to WHO (will be updated).\nLast Update: MM/DD/YYYY HH:mm (24 hour format, in UTC).\nConfirmed: the number of confirmed cases. For Hubei Province: from Feb 13 (GMT +8), we report both clinically diagnosed and lab-confirmed cases. For lab-confirmed cases only (Before Feb 17), please refer to who_covid_19_situation_reports. For Italy, diagnosis standard might be changed since Feb 27 to \u0026ldquo;slow the growth of new case numbers.\u0026rdquo;\nDeaths: the number of deaths.\nRecovered: the number of recovered cases.\nEliminar Ubicacion Se va realizar un analisis general de los datos y No se van a tomar los datos geograficos de latitud, longitud y los datos de Province/State estan incompletos.\nSolo se realizara un analisis por pais entonces se eliminaran las columnas mencionadas anteriormente\nconfirmed = confirmed.drop(columns=['Lat', 'Long','Province/State']) death = death.drop(columns=['Lat', 'Long','Province/State']) recovered = recovered.drop(columns=['Lat', 'Long','Province/State'])  Casos Activos Se calcula a partir del número de personas confirmadas - muertos - recuperados\nactive =confirmed.copy() active.iloc[:,1:] = active.iloc[:,1:] - death.iloc[:,1:] - recovered.iloc[:,1:]  Consolidar datos confirmed_group = confirmed.groupby(by='Country/Region').aggregate(np.sum).T confirmed_group.index.name = 'date' confirmed_group = confirmed_group.reset_index()  recovered_group = recovered.groupby(by='Country/Region').aggregate(np.sum).T recovered_group.index.name = 'date' recovered_group = recovered_group.reset_index()  active_group = active.groupby(by='Country/Region').aggregate(np.sum).T active_group.index.name = 'date' active_group = active_group.reset_index()  death_group = death.groupby(by='Country/Region').aggregate(np.sum).T death_group.index.name = 'date' death_group = death_group.reset_index()  confirmed_melt = confirmed_group.melt(id_vars=\u0026quot;date\u0026quot;).copy() confirmed_melt.rename(columns = {'value':'Confirmados', 'date':'Fecha'}, inplace = True) death_melt = death_group.melt(id_vars=\u0026quot;date\u0026quot;) death_melt.rename(columns = {'value':'Muertos', 'date':'Fecha'}, inplace = True)  Datos Mundiales # Numero de Casos confirmados por dia en el mundo column_names = [\u0026quot;Fecha\u0026quot;, \u0026quot;Confirmados\u0026quot;, \u0026quot;Recuperados\u0026quot;,\u0026quot;Muertos\u0026quot;] world = pd.DataFrame(columns = column_names) world['Fecha'] = confirmed_group['date'].copy() world['Confirmados'] = confirmed_group.iloc[:,1:].sum(1) world['Muertos'] = death_group.iloc[:,1:].sum(1) world['Recuperados'] = recovered_group.iloc[:,1:].sum(1) world['Activos'] = active_group.iloc[:,1:].sum(1)  Covid19 en el Mundo Evolucion Animada de Casos Activos por Pais La gráfica animada de la evolución temporal de los casos activos por país, la he creado con la libreria Pandas alive y Bar Chart Race.\n  import pandas_alive active_evol = active_group.set_index('date') active_evol.index = pd.to_datetime(active_evol.index) active_evol.plot_animated(filename='evolucion_casos_activos.mp4', n_bars=8,n_visible=8, title='Evolución en el tiempo de Casos Activos COVID-19 por pais \\n https://joserzapata.github.io/', perpendicular_bar_func='mean', dpi=300, period_label={'x': .99, 'y': .25, 'ha': 'right', 'va': 'center'}, period_fmt='%B %d, %Y', period_summary_func=lambda v: {'x': .99, 'y': .18, 's': f'Total Activos: {v.nlargest(8).sum():,.0f}', 'ha': 'right', 'size': 9, 'family': 'Courier New'})  Visualizacion con Plotly Valores Mundiales de Casos Confirmados, Activos, Recuperados y Muertos  temp = pd.DataFrame(world.iloc[-1,:]).T tm = temp.melt(id_vars=\u0026quot;Fecha\u0026quot;, value_vars=[ \u0026quot;Confirmados\u0026quot;,\u0026quot;Activos\u0026quot;,\u0026quot;Recuperados\u0026quot;,\u0026quot;Muertos\u0026quot;]) fig = px.bar(tm, x=\u0026quot;variable\u0026quot; , y=\u0026quot;value\u0026quot;, color= 'variable', text='value', color_discrete_sequence=[\u0026quot;teal\u0026quot;,\u0026quot;navy\u0026quot;,\u0026quot;green\u0026quot;, \u0026quot;coral\u0026quot;], height=500, width=600, title= f'Total de Casos Mundiales de COVID 19 - {str(world.iloc[-1,0])}') fig.update_traces(textposition='outside')#poner los valores de las barras fuera fig.layout.update(showlegend=False, yaxis = {\u0026quot;title\u0026quot;: {\u0026quot;text\u0026quot;: \u0026quot;Numero de Personas\u0026quot;}}, # Cambiar texto eje y xaxis = {\u0026quot;title\u0026quot;: {\u0026quot;text\u0026quot;: \u0026quot;\u0026quot;}} #Esconder nombre eje x ) # grabar grafica en chart-studio si se proporciona el api-key if api_key: py.plot(fig, filename = 'total_casos_general', auto_open=False) fig.show()  Mapa Mundial de Confirmados por Pais Mover el Mouse sobre el mapa para ver la informacion de cada pais \nconfirmed_melt['Fecha'] = pd.to_datetime(confirmed_melt['Fecha']) confirmed_melt['Fecha'] = confirmed_melt['Fecha'].dt.strftime('%m/%d/%Y') max_Fecha = confirmed_melt['Fecha'].iloc[-1] conf_max = confirmed_melt[confirmed_melt['Fecha']== max_Fecha] conf_max.dropna(inplace=True) #eliminar filas con valores faltantes fig = px.choropleth(conf_max, locations=\u0026quot;Country/Region\u0026quot;, locationmode='country names', color=np.log10(conf_max[\u0026quot;Confirmados\u0026quot;]), hover_name=\u0026quot;Country/Region\u0026quot;, hover_data = [\u0026quot;Confirmados\u0026quot;], projection=\u0026quot;natural earth\u0026quot;, width=900, color_continuous_scale = px.colors.sequential.Jet, title='Mapa de Confirmados COVID 19 por Pais') fig.update(layout_coloraxis_showscale=False) #py.plot(fig, filename = 'mapa_confirmados_pais', auto_open=False) fig.show()  Confirmados vs Muertos por pais  death_melt['Fecha'] = pd.to_datetime(death_melt['Fecha']) death_melt['Fecha'] = death_melt['Fecha'].dt.strftime('%m/%d/%Y') max_Fecha = death_melt['Fecha'].iloc[-1] death_max = death_melt[death_melt['Fecha']== max_Fecha].copy() death_max.dropna(inplace=True) #eliminar filas con valores faltantes fig = px.scatter(full_melt_max.sort_values('Muertos', ascending=False).iloc[:15, :], x='Confirmados', y='Muertos', color='Country/Region', size='Confirmados', height=500, text='Country/Region', log_x=True, log_y=True, title= f'Muertos vs Confirmados - {max_Fecha} - (15 Paises)') fig.update_traces(textposition='top center') fig.layout.update(showlegend = False) #py.plot(fig, filename = 'scatter_muertos_confirmados', auto_open=False) fig.show()  Progresion Mundial en el Tiempo de Confirmados y Muertos  world_melt = world.melt(id_vars='Fecha', value_vars= list(world.columns)[1:], var_name=None) fig = px.line(world_melt, x=\u0026quot;Fecha\u0026quot;, y= 'value', color='variable', color_discrete_sequence=[\u0026quot;teal\u0026quot;,\u0026quot;green\u0026quot;,\u0026quot;coral\u0026quot;, \u0026quot;navy\u0026quot;], title=f'Total de Casos en el tiempo de COVID 19 - {world.iloc[-1,0]}') for n in list(world.columns)[1:]: fig.add_annotation(x=world.iloc[-1,0], y=world.loc[world.index[-1],n], text=n, xref=\u0026quot;x\u0026quot;,yref=\u0026quot;y\u0026quot;, showarrow=True, ax=-50, ay=-20) # Indicador de numero total de confirmados fig.add_indicator( title= {'text':'Confirmados', 'font':{'color':'teal'}}, value = world['Confirmados'].iloc[-1], mode = \u0026quot;number+delta\u0026quot;, delta = {\u0026quot;reference\u0026quot;: world['Confirmados' ].iloc[-2], 'relative': True },domain = {'x': [0, 0.25], 'y': [0.15, .4]}) #Indicador numero total de Activos fig.add_indicator(title={'text':'Activos', 'font':{'color':'navy'}}, value = world['Activos'].iloc[-1], mode = \u0026quot;number+delta\u0026quot;, delta = {\u0026quot;reference\u0026quot;: world['Activos' ].iloc[-2], 'relative': True },domain = {'x': [0, 0.25], 'y': [0.6, .85]}) #Indicador numero total de Recuperados fig.add_indicator(title={'text':'Recuperados', 'font':{'color':'green'}}, value = world['Recuperados'].iloc[-1], mode = \u0026quot;number+delta\u0026quot;, delta = {\u0026quot;reference\u0026quot;: world['Recuperados' ].iloc[-2], 'relative': True },domain = {'x': [0.25, 0.50], 'y': [0.6, .85]}) #Indicador numero total de muertos fig.add_indicator(title={'text':'Muertos', 'font':{'color':'coral'}}, value = world['Muertos'].iloc[-1], mode = \u0026quot;number+delta\u0026quot;, delta = {\u0026quot;reference\u0026quot;: world['Muertos' ].iloc[-2], 'relative': True },domain = {'x': [0.25, 0.5], 'y': [0.15, .4]}) fig.layout.update(showlegend = False, yaxis = {\u0026quot;title\u0026quot;: {\u0026quot;text\u0026quot;: \u0026quot;Numero de Personas\u0026quot;}}, # Cambiar texto eje y ) # grabar grafica en chart-studio si se proporciona el api-key if api_key: py.plot(fig, filename = 'total_casos_serie', auto_open=False) fig.show()  Total Casos Confirmados de COVID 19 por Pais  df1 = confirmed_group.copy() # Cambiar el nombre de la columna df1.rename(columns = {'date':'Fecha'}, inplace = True) df_melt = df1.melt(id_vars='Fecha', value_vars= list(df1.columns)[1:], var_name=None) fig = px.line(df_melt, x='Fecha' , y='value', color='Country/Region', color_discrete_sequence=px.colors.qualitative.G10, title=f'Total Casos Confirmados de COVID 19 por Pais (Excluyendo China) - {world.iloc[-1,0]}') # 8 paises mas infectados fecha = df1['Fecha'].iloc[-1] #obtener la fecha del ultimo dato paises = df1.iloc[-1,1:] #obtener la serie sin el primer dato, fecha paises.sort_values(ascending=False, inplace=True) mas_infectados=[] for n in range(8): fig.add_annotation(x=fecha, y=paises[n], text=paises.index[n], showarrow=True, ax=+30, ay=0) mas_infectados.append(paises.index[n]) fig.layout.update(showlegend=False, yaxis = {\u0026quot;title\u0026quot;: {\u0026quot;text\u0026quot;: \u0026quot;Numero de Personas\u0026quot;}}, # Cambiar texto eje y ) # grabar grafica en chart-studio #py.plot(fig, filename = 'total_casos_no_china', auto_open=False) fig.show()  Total Casos Confirmados de COVID 19 por Pais (Excluyendo los 8 mas infectados)  df2 = confirmed_group.drop(columns= mas_infectados).copy() # Cambiar el nombre de la columna df2.rename(columns = {'date':'Fecha'}, inplace = True) df_melt2 = df2.melt(id_vars='Fecha', value_vars= list(df2.columns)[1:], var_name=None) fig = px.line(df_melt2, x='Fecha' , y='value', color='Country/Region', color_discrete_sequence=px.colors.qualitative.G10, title=f'Total Casos Confirmados de COVID 19 por Pais (Excluyendo los 8 mas infectados) - {world.iloc[-1,0]}') fecha = df2['Fecha'].iloc[-1] #obtener la fecha del ultimo dato paises = df2.iloc[-1,1:] #obtener la serie sin el primer dato, fecha paises.sort_values(ascending=False, inplace=True) for n in range(8): fig.add_annotation(x=fecha, y=paises[n], text=paises.index[n], showarrow=True, ax=+30, ay=0) fig.layout.update(showlegend=False, yaxis = {\u0026quot;title\u0026quot;: {\u0026quot;text\u0026quot;: \u0026quot;Numero de Personas\u0026quot;}}, # Cambiar texto eje y ) #py.plot(fig, filename = 'total_casos_no_8_infectados', auto_open=False) fig.show()  Animacion del Mapa de Evolucion Temporal del Codiv 19 Mover el Mouse sobre el mapa para ver la informacion de cada pais. Presionar el boton de play para ver la animacion.\n confirmed_melt['Fecha'] = pd.to_datetime(confirmed_melt['Fecha']) confirmed_melt['Fecha'] = confirmed_melt['Fecha'].dt.strftime('%m/%d/%Y') confirmed_melt['size'] = confirmed_melt['Confirmados'].pow(0.3) confirmed_melt.dropna(inplace=True) #eliminar filas con valores faltantes fig = px.scatter_geo(confirmed_melt, locations=\u0026quot;Country/Region\u0026quot;, locationmode='country names', color=\u0026quot;Confirmados\u0026quot;, size='size', hover_name=\u0026quot;Country/Region\u0026quot;, range_color= [0, max(confirmed_melt['Confirmados'])+2], projection=\u0026quot;natural earth\u0026quot;, animation_frame=\u0026quot;Fecha\u0026quot;, title='Contagiados COVID 19 en el Tiempo') fig.update(layout_coloraxis_showscale=False) #py.plot(fig, filename = 'mapa_evolucion_temporal', auto_open=False) fig.show()  Covid 19 en Colombia Numero de Casos COVID 19 en Colombia  column_names = [\u0026quot;Fecha\u0026quot;, \u0026quot;Confirmados\u0026quot;, \u0026quot;Recuperados\u0026quot;,\u0026quot;Muertos\u0026quot;, \u0026quot;Activos\u0026quot;] colombia = pd.DataFrame(columns = column_names) colombia['Fecha'] = confirmed_group['Fecha'] colombia['Confirmados'] = confirmed_group['Colombia'] colombia['Muertos'] = death_group['Colombia'] colombia['Recuperados'] = recovered_group['Colombia'] colombia['Activos'] = active_group['Colombia'] df_melt3 = colombia.melt(id_vars='Fecha', value_vars= list(colombia.columns)[1:], var_name=None) fig = px.line(df_melt3, x='Fecha' , y='value', color='variable', color_discrete_sequence=[\u0026quot;teal\u0026quot;,\u0026quot;green\u0026quot;,\u0026quot;coral\u0026quot;, \u0026quot;navy\u0026quot;], title=f'Corona virus (COVID 19) en Colombia - {colombia.iloc[-1,0]}') # Indicador de numero total de confirmados fig.add_indicator( title= {'text':'Confirmados', 'font':{'color':'teal'}}, value = colombia['Confirmados'].iloc[-1], mode = \u0026quot;number+delta\u0026quot;, delta = {\u0026quot;reference\u0026quot;: colombia['Confirmados' ].iloc[-2], 'relative': True },domain = {'x': [0, 0.25], 'y': [0.15, .4]}) #Indicador numero total de Activos fig.add_indicator(title={'text':'Activos', 'font':{'color':'navy'}}, value = colombia['Activos'].iloc[-1], mode = \u0026quot;number+delta\u0026quot;, delta = {\u0026quot;reference\u0026quot;: colombia['Activos' ].iloc[-2], 'relative': True },domain = {'x': [0, 0.25], 'y': [0.6, .85]}) #Indicador numero total de Recuperados fig.add_indicator(title={'text':'Recuperados', 'font':{'color':'green'}}, value = colombia['Recuperados'].iloc[-1], mode = \u0026quot;number+delta\u0026quot;, delta = {\u0026quot;reference\u0026quot;: colombia['Recuperados' ].iloc[-2], 'relative': True },domain = {'x': [0.25, 0.50], 'y': [0.6, .85]}) #Indicador numero total de muertos fig.add_indicator(title={'text':'Muertos', 'font':{'color':'coral'}}, value = colombia['Muertos'].iloc[-1], mode = \u0026quot;number+delta\u0026quot;, delta = {\u0026quot;reference\u0026quot;: colombia['Muertos' ].iloc[-2], 'relative': True },domain = {'x': [0.25, 0.5], 'y': [0.15, .4]}) fig.layout.update(showlegend=False, yaxis = {\u0026quot;title\u0026quot;: {\u0026quot;text\u0026quot;: \u0026quot;Numero de Personas\u0026quot;}}, # Cambiar texto eje y xaxis = {\u0026quot;title\u0026quot;: {\u0026quot;text\u0026quot;: \u0026quot;Fecha\u0026quot;}}) # grabar grafica en chart-studio si se proporciona el api-key if api_key: py.plot(fig, filename = 'Colombia_general', auto_open=False) fig.show()  Actualizacion de las Graficas cada 24 Horas Las graficas creadas con plotly son enviadas a chart-studio y cargadas en la pagina web mediante el tag iframe de html. Las gráficas se actualizan cada 24 horas usando Github Actions\nCodigo Fuente Jupyter notebook    Google Colaboratory My binder NBviewver           Refencias Fuentes de datos, visualizaciones y análisis de datos.\n https://github.com/CSSEGISandData/COVID-19 https://www.kaggle.com/imdevskp/covid-19-analysis-viz-prediction-comparisons https://junye0798.com/post/build-a-dashboard-to-track-the-spread-of-coronavirus-using-dash/ https://github.com/Perishleaf/data-visualisation-scripts/tree/master/dash-2019-coronavirus https://medium.com/tomas-pueyo/coronavirus-por-qu%C3%A9-debemos-actuar-ya-93079c61e200 https://github.com/features/actions  ","date":1584482637,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1590789837,"objectID":"46e5195e6508837960f45a896c05991e","permalink":"https://joserzapata.github.io/post/covid19-visualizacion/","publishdate":"2020-03-17T17:03:57-05:00","relpermalink":"/post/covid19-visualizacion/","section":"post","summary":"Visualizaciones con Python y Plotly de los datos mundiales del corona virus COVID19","tags":["Python","Data-Science","Jupyter-notebook"],"title":"Visualizacion Datos Coronavirus (COVID19) Mundial con Plotly","type":"post"},{"authors":["Jose R. Zapata"],"categories":["Data-Science"],"content":"PySpark en Google Colab Automatico  Instalacion Marzo/2020 Intalacion Automatica  Instalacion Java Instalacion de Spark Ejemplo de Uso de pyspark    \nAbrir en Google Colab:\n\nInstalacion Rapida Marzo/ 2020 De forma General para usar pyspark en Colab Marzo/2020 seria con los siguientes comandos en una celda en Colab:\nInstalar Java\n!apt-get install openjdk-8-jdk-headless -qq \u0026gt; /dev/null  import os # libreria de manejo del sistema operativo os.system(\u0026quot;wget -q https://www-us.apache.org/dist/spark/spark-2.4.5/spark-2.4.5-bin-hadoop2.7.tgz\u0026quot;) os.system(\u0026quot;tar xf /spark-2.4.5-bin-hadoop2.7.tgz\u0026quot;)  instalar pyspark\n!pip install -q pyspark  # Variables de Entorno os.environ[\u0026quot;JAVA_HOME\u0026quot;] = \u0026quot;/usr/lib/jvm/java-8-openjdk-amd64\u0026quot; os.environ[\u0026quot;SPARK_HOME\u0026quot;] = f\u0026quot;/content/{ver_spark}-bin-hadoop2.7\u0026quot;  # Cargar Pyspark from pyspark.sql import SparkSession spark = SparkSession.builder.appName(\u0026quot;Test_spark\u0026quot;).master(\u0026quot;local[*]\u0026quot;).getOrCreate() spark  Pero cuando salga una nueva version de spark sera necesario actualizar los links de descarga, ya que siempre borran las versiones 2.x.x cuando sale una nueva.\nLo mejor es configurar automaticamente para que descargue la version que sea mayor que 2.3.4 que es la anterior y menor que spark 3.0.0 que aun se encuentra en desarrollo\nPara esto el siguiente codigo detecta la version actual de spark, la descarga, la descomprime y luego realiza la instalacion de spark en google colab.\nInstalacion Automatica Instalacion de Java Google Colaboratory funciona en un ambiente linux, por lo tanto se pueden usar comandos shell de linux antecedidos del caracter \u0026lsquo;!\u0026rsquo;\n!apt-get install openjdk-8-jdk-headless -qq \u0026gt; /dev/null  Instalacion de Spark Obtener automaticamente la ultima version de spark de\nfrom bs4 import BeautifulSoup import requests  #Obtener las versiones de spark la pagina web url = 'https://downloads.apache.org/spark/' r = requests.get(url) html_doc = r.text soup = BeautifulSoup(html_doc)  # leer la pagina web y obtener las versiones de spark disponibles link_files = [] for link in soup.find_all('a'): link_files.append(link.get('href')) spark_link = [x for x in link_files if 'spark' in x] print(spark_link)  [\u0026lsquo;spark-2.3.4/\u0026rsquo;, \u0026lsquo;spark-2.4.5/\u0026rsquo;, \u0026lsquo;spark-3.0.0-preview2/']\nLa version a usar seran las superiores a spark-2.3.4 y menores a spark-3.0.0\nobtener la version y eliminar el caracter \u0026lsquo;/\u0026rsquo; del final\nver_spark = spark_link[1][:-1] # obtener la version y eliminar el caracter '/' del final print(ver_spark)  spark-2.4.5  import os # libreria de manejo del sistema operativo #instalar automaticamente la version deseadda de spark link = \u0026quot;https://www-us.apache.org/dist/spark/\u0026quot; os.system(f\u0026quot;wget -q {link}{ver_spark}/{ver_spark}-bin-hadoop2.7.tgz\u0026quot;) os.system(f\u0026quot;tar xf {ver_spark}-bin-hadoop2.7.tgz\u0026quot;) # instalar pyspark !pip install -q pyspark  |████████████████████████████████| 217.8MB 63kB/s |████████████████████████████████| 204kB 53.8MB/s Building wheel for pyspark (setup.py) ... done  Definir variables de entorno os.environ[\u0026quot;JAVA_HOME\u0026quot;] = \u0026quot;/usr/lib/jvm/java-8-openjdk-amd64\u0026quot; os.environ[\u0026quot;SPARK_HOME\u0026quot;] = f\u0026quot;/content/{ver_spark}-bin-hadoop2.7\u0026quot;  Cargar pyspark en el sistema from pyspark.sql import SparkSession spark = SparkSession.builder.appName(\u0026quot;Test_spark\u0026quot;).master(\u0026quot;local[*]\u0026quot;).getOrCreate() spark   SparkSession - in-memory\n SparkContext\nSpark UI\n Version v2.4.5 Master local[*] AppName pyspark-shell    Ejemplo de Uso de pyspark Leer archivo de prueba\narchivo = './sample_data/california_housing_train.csv' df_spark = spark.read.csv(archivo, inferSchema=True, header=True) # imprimir tipo de archivo print(type(df_spark))  \u0026lt;class 'pyspark.sql.dataframe.DataFrame'\u0026gt;  ¿Numero de registros en el dataframe?\ndf_spark.count()  17000  Estructura del dataframe\ndf_spark.printSchema()  root |-- longitude: double (nullable = true) |-- latitude: double (nullable = true) |-- housing_median_age: double (nullable = true) |-- total_rooms: double (nullable = true) |-- total_bedrooms: double (nullable = true) |-- population: double (nullable = true) |-- households: double (nullable = true) |-- median_income: double (nullable = true) |-- median_house_value: double (nullable = true)  ¿Nombre de las Columnas de dataframe?\ndf_spark.columns  ['longitude', 'latitude', 'housing_median_age', 'total_rooms', 'total_bedrooms', 'population', 'households', 'median_income', 'median_house_value']  Ver los primeros 20 registros del dataframe\ndf_spark.show()  +---------+--------+------------------+-----------+--------------+----------+----------+-------------+------------------+ |longitude|latitude|housing_median_age|total_rooms|total_bedrooms|population|households|median_income|median_house_value| +---------+--------+------------------+-----------+--------------+----------+----------+-------------+------------------+ | -114.31| 34.19| 15.0| 5612.0| 1283.0| 1015.0| 472.0| 1.4936| 66900.0| | -114.47| 34.4| 19.0| 7650.0| 1901.0| 1129.0| 463.0| 1.82| 80100.0| | -114.56| 33.69| 17.0| 720.0| 174.0| 333.0| 117.0| 1.6509| 85700.0| | -114.57| 33.64| 14.0| 1501.0| 337.0| 515.0| 226.0| 3.1917| 73400.0| | -114.57| 33.57| 20.0| 1454.0| 326.0| 624.0| 262.0| 1.925| 65500.0| | -114.58| 33.63| 29.0| 1387.0| 236.0| 671.0| 239.0| 3.3438| 74000.0| | -114.58| 33.61| 25.0| 2907.0| 680.0| 1841.0| 633.0| 2.6768| 82400.0| | -114.59| 34.83| 41.0| 812.0| 168.0| 375.0| 158.0| 1.7083| 48500.0| | -114.59| 33.61| 34.0| 4789.0| 1175.0| 3134.0| 1056.0| 2.1782| 58400.0| | -114.6| 34.83| 46.0| 1497.0| 309.0| 787.0| 271.0| 2.1908| 48100.0| | -114.6| 33.62| 16.0| 3741.0| 801.0| 2434.0| 824.0| 2.6797| 86500.0| | -114.6| 33.6| 21.0| 1988.0| 483.0| 1182.0| 437.0| 1.625| 62000.0| | -114.61| 34.84| 48.0| 1291.0| 248.0| 580.0| 211.0| 2.1571| 48600.0| | -114.61| 34.83| 31.0| 2478.0| 464.0| 1346.0| 479.0| 3.212| 70400.0| | -114.63| 32.76| 15.0| 1448.0| 378.0| 949.0| 300.0| 0.8585| 45000.0| | -114.65| 34.89| 17.0| 2556.0| 587.0| 1005.0| 401.0| 1.6991| 69100.0| | -114.65| 33.6| 28.0| 1678.0| 322.0| 666.0| 256.0| 2.9653| 94900.0| | -114.65| 32.79| 21.0| 44.0| 33.0| 64.0| 27.0| 0.8571| 25000.0| | -114.66| 32.74| 17.0| 1388.0| 386.0| 775.0| 320.0| 1.2049| 44000.0| | -114.67| 33.92| 17.0| 97.0| 24.0| 29.0| 15.0| 1.2656| 27500.0| +---------+--------+------------------+-----------+--------------+----------+----------+-------------+------------------+ only showing top 20 rows  Descricipcion Estadistica del dataframe df_spark.describe().toPandas().transpose()      0 1 2 3 4     summary count mean stddev min max   longitude 17000 -119.56210823529375 2.0051664084260357 -124.35 -114.31   latitude 17000 35.6252247058827 2.1373397946570867 32.54 41.95   housing_median_age 17000 28.58935294117647 12.586936981660406 1.0 52.0   total_rooms 17000 2643.664411764706 2179.947071452777 2.0 37937.0   total_bedrooms 17000 539.4108235294118 421.4994515798648 1.0 6445.0   population 17000 1429.5739411764705 1147.852959159527 3.0 35682.0   households 17000 501.2219411764706 384.5208408559016 1.0 6082.0   median_income 17000 3.883578100000021 1.9081565183791036 0.4999 15.0001   median_house_value 17000 207300.91235294117 115983.76438720895 14999.0 500001.0    Descripcion estadistica de una sola columna (\u0026lsquo;median_house_value\u0026rsquo;)\ndf_spark.describe(['median_house_value']).show()  +-------+------------------+ |summary|median_house_value| +-------+------------------+ | count| 17000| | mean|207300.91235294117| | stddev|115983.76438720895| | min| 14999.0| | max| 500001.0| +-------+------------------+  De esta forma se puede instalar automaticamente spark en google colab y hacer uno de el de forma gratis.\nEn la version gratis solo se cuenta con una CPU si se quiere aumentar la capacidad de procesamiento es necesario pagar.\n","date":1583812843,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1583812843,"objectID":"0ab34644955db890c3be41e87603ffaf","permalink":"https://joserzapata.github.io/post/pyspark-google-colab/","publishdate":"2020-03-09T23:00:43-05:00","relpermalink":"/post/pyspark-google-colab/","section":"post","summary":"Configuracion de Google Colab para usar pyspark","tags":["Python","Pyspark","Colab","Data-Science","Jupyter-notebook"],"title":"Pyspark con Google Colab","type":"post"},{"authors":["Jose R. Zapata"],"categories":[],"content":"","date":1581718199,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1581718199,"objectID":"021b3cc1a3a13daee2d277dd616cb2f2","permalink":"https://joserzapata.github.io/project/social-media-behaviour/","publishdate":"2020-02-14T17:09:59-05:00","relpermalink":"/project/social-media-behaviour/","section":"project","summary":"Research Project that combines exclusive Facebook data (Condor Dataset, Crowdtangle, Ad’s) and public data to analyze the Social Media behavior to determine if there is coordinated non-authentic behavior.\nThis project in founded by the Social Media and Democracy Research Grants from the Social Science Research Council and access to Facebook data via Social Science One. ","tags":["Data-Science","Python"],"title":"Social Media Behaviour with exclusive Facebook data","type":"project"},{"authors":["Jose R. Zapata"],"categories":null,"content":"","date":1581096900,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1581096900,"objectID":"1d90c62532768e66c51f7ee45ff2a2fd","permalink":"https://joserzapata.github.io/event/pycon2020/","publishdate":"2020-02-07T21:12:33-05:00","relpermalink":"/event/pycon2020/","section":"event","summary":"Python para demostrar y visualizar la naturaleza de las ondas del sonido, la relación entre diferentes notas y como se unen para crear la Armonía Musical desde la Física y la Matemática","tags":["Python","Jupyter-notebook","Music","Waves"],"title":"Comprensión de la música con Python, una mirada desde la Física y la Matemática","type":"event"},{"authors":["Estefanía Cano","Antonio Escamilla","Sascha Grollmisch","Christian Kehling","Fernando Mora-Ágnel","Gustavo López Gil","Jose R. Zapata"],"categories":[],"content":"","date":1572607689,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1572607689,"objectID":"1e6d7fcfd79982641828d5006ae64d18","permalink":"https://joserzapata.github.io/publication/acmus19/","publishdate":"2019-11-01T19:28:09+08:00","relpermalink":"/publication/acmus19/","section":"publication","summary":"This extended abstract describes the ACMus project, including its scope, partners, and current results.","tags":["Computational Musicology","Colombian Music","Music-information-retrieval","Python","ISMIR"],"title":"ACMUS - Advancing Computational Musicology: Semi-supervised and Unsupervised Segmentation and Annotation of Musical Collections","type":"publication"},{"authors":[],"categories":[],"content":"","date":1563220869,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1563220869,"objectID":"b6545793ef267683c42022143affe8b3","permalink":"https://joserzapata.github.io/project/acmus/","publishdate":"2019-07-15T15:01:09-05:00","relpermalink":"/project/acmus/","section":"project","summary":"Research Project to explore the use of machine learning techniques for computational musicology, digital music archive managment, and music information retrieval.Two main elements are the core of our project: 1. Emphasis on semi-supervised and unsupervised machine learning techniques that minimally rely on the availability of annotated data for a specific task. 2. Traditional Colombian music as the main focus of our study.","tags":["Music-information-retrieval","Musicology","Beat-tracking"],"title":"Acmus","type":"project"},{"authors":["Jose R. Zapata"],"categories":null,"content":"","date":1538521200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1538521200,"objectID":"d956a8fdc73094f46d8d4c1a4d72b193","permalink":"https://joserzapata.github.io/event/meetup_2018/","publishdate":"2019-11-25T22:07:22-05:00","relpermalink":"/event/meetup_2018/","section":"event","summary":"Mediante la extraccion automatica del contenido de las señales de audio y Machine learning se tienen aplicaciones de predicción, clasificación y Agrupamiento, como: Deteccion de emociones en la voz (Speech Emotion), Reconocieminto de Voz (Speech-to-Text), Deteccion y segmentacion de personas hablando(Speaker Diarization), Music Information Retrieval (Extraccion Automatica de Informacion Musical), entre otros.","tags":["Python","Music-information-retrieval","Machine-learning","Data-science"],"title":"Aplicaciones de Audio con Machine Learning","type":"event"},{"authors":["Jose R. Zapata"],"categories":["Audio"],"content":" Video tutoriales sobre el uso de Reaper para produccion de audio, los videos fueron creados como apoyo para el curso de Audio del programa Ingenieria en Diseño de entrentenimiento digital de la Universidad Pontificia Bolivariana.\nDescargar REAPER para Windows, macOS o Linux.\nTable of Contents  1. Introduccion a Reaper y Edicion 2. Manejo de canales y envios con Reaper 3 Automatizaciones y Envolventes en Reaper 4. Mezcla y Masterizacion en Reaper    \nPlaylist del tutorial en youtube\n1. Introduccion a Reaper y Edicion Introduccion a la configuración Manejo de REAPER para produccion de audio.\n  Audios para Insertar\nImagen para Insertar\nVideo para Insertar\n2. Manejo de canales y envios con Reaper   Ejemplo sesión completa de Reaper\n3 Automatizaciones y Envolventes en Reaper Automatización de Volumen, Panning y Efectos mediante envolventes en Reaper\n  Audios Practica de Automatización\nProyecto Reaper de Automatización finalizado\n4. Mezcla y Masterizacion en Reaper Principios Basicos de mezcla y masterización de audio en Reaper\n  Audios practica de mezcla y masterización\nSesión final de reaper con mezcla y masterización\n","date":1535811053,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1535811053,"objectID":"42e29105f594b9a5bbd8867d57f79dbb","permalink":"https://joserzapata.github.io/post/produccion-audio-reaper/","publishdate":"2018-09-01T09:10:53-05:00","relpermalink":"/post/produccion-audio-reaper/","section":"post","summary":"Tutorial en Video y material produccion basica de Audio con Reaper","tags":["Audio","Reaper"],"title":"Tutorial de Produccion de Audio con Reaper","type":"post"},{"authors":["Jose R. Zapata"],"categories":["Academia"],"content":" Este es el resumen de algunas ideas concretas que he recopilado tanto en mi experiencia como docente, investigador y director de varios trabajos de final de carrera en pregrado y postgrado, para realizar una buena presentación del anteproyecto y del trabajo de grado.\n\nTable of Contents  Tips para la Creación de las Diapositivas Exposición Anteproyecto  Diapositivas: Preguntas de Evaluación:  Exposición del Proyecto de grado  Diapositivas: Preguntas de evaluación:     Tips para la Creación de las Diapositivas  Usar letra Grande Poco Texto, solo escribir lo indispensable e importante Si las gráficas son pequeñas y no se ven (No Usarlas) Si el texto que se va poner es pequeño (Recortarlo hasta que se vea grande o quitarlo) Usar colores de la misma paleta de colores de la plantilla Tu haces la presentación no las diapositivas Los evaluadores deben estar concentrados en el presentador y solo ver las diapositivas cuando el presentador las señala o explica algo que esta en ellas como ayuda a la presentación No leer las diapositivas Ser directo Recuerde el Zen de Python Disfrutar la presentación! (Esta es la mas importante)  Exposición Anteproyecto Se puede usar para cualquier tipo de proyectos de grado, la regla del número de diapositivas (slides) es 1 diapositiva por 1 minuto de tiempo de exposicion.\nTiempo de presentación = 15 Minutos (Entre 15-17 diapositivas)\nDiapositivas:  Titulo, información del estudiante y del director Contexto Problema (un solo parrafo) Justificación Marco Conceptual Marco conceptual Estado del Arte Estado del Arte Marco Legal Objetivo General (un solo párrafo) Objetivo específicos (Poner Productos) Objetivo específicos (Poner Productos) Metodología Cronograma Presupuesto Bibliografia  Preguntas de Evaluación: Responder estas preguntas para tener claro lo que quieren saber los jurados\n Título:  ¿El titulo es claro y coherente con la temática y problema planteados?  Problema:  ¿Se describe claramente el problema y el contexto en el que se presenta? ¿Se describen claramente las causas del problema? ¿Se presentan evidencias válidas suficientes de la existencia del problema y las causas?  Justificación:  ¿Se describen claramente los beneficios y el impacto en la realización del proyecto?  Marco Conceptual:  ¿El marco conceptual reconoce los conceptos afines a las categorías desarrolladas en el anteproyecto? ¿El marco conceptual está respaldado con suficiencia en fuentes bibliográfica y documental?  Estado de arte:  ¿El estado de la cuestión da cuenta del reconocimiento del campo, por lo menos, del ámbito nacional con referencia al contexto internacional? ¿El estado del arte está respaldado con suficiencia en fuentes bibliográfica y documental?  Marco legal:  ¿Se examinan las regulaciones y/o normativas nacionales e internacionales relacionadas con la temática del proyecto?  Objetivo general:  ¿El objetivo posee un verbo en infinitivo y presenta claramente el qué, cómo y para qué se realiza el proyecto? ¿El título del proyecto está claramente relacionado con el objetivo? ¿El objetivo ofrece una solución al problema referenciado en el proyecto?  Objetivos específicos:  ¿Cada objetivo posee un verbo en infinitivo y presenta claramente el qué se desea realizar? ¿Cada uno de los productos de los objetivos específicos aporta siginificativamente al logro del objetivo general y la suma de ellos logra el objetivo general ? ¿Se presentan los objetivos cronológicamente?  Metodología:  ¿Existe una estrecha relación entre los objetivos específicos y las fases de la metodología? ¿Se describen las fases del proyecto y las actividades necesarias para cumplir con los objetivos? ¿Se describen de forma clara y coherente las técnicas, métodos, herramientas e instrumentos a utilizar en el proyecto que le aporten rigor a la realización en cada una de las fases? ¿El(los) producto(s) del proyecto son fruto de un ejercicio de investigación aplicada o caso de estudio?  Cronograma:  ¿El cronograma propuesto es claro, coherente con la metodologia y propone una duración de 6 meses?   Exposición del Proyecto de grado Las diapositivas donde se explica la metodología usada (13-18) en este caso es un ejemplo usando la metodología CRISP usada en ciencia de datos.\nTiempo de presentación = 30 Minutos (Limite Maximo 30 diapositivas)\nDiapositivas: Este es un tentativo de las diapositivas que se pueden realizar, hay algunos puntos que requieren mas diapositivas como el Estudio y comprensión de los datos y análisis de los datos (Usar Tablas con numeros grandes y graficas Grandes)\n Titulo, información del estudiante y del director Contexto Problema (un solo parrafo) Justificación Marco Conceptual Marco conceptual Marco Legal Estado del Arte Estado del Arte Objetivo General (un solo parrafo) Objetivo específicos Objetivo específicos metodología (CRISP, solo poner la gráfica y no gastar mucho tiempo en esto) a. Comprensión de la pregunta o el negocio b. Estudio y comprensión de los datos (¿ Que características tienen los datos?) c. Análisis de los datos y selección de características (Que encontraste de particular en los datos) d. Modelado (¿cuales modelos se utilizaron y por que?) e. Evaluación (¿cual medida de evaluación se tomo y por que?) Resultados Finales (¿Cual modelo se selecciono y cuales son las características mas importantes y por que?) Productos obtenidos (evidencias) Conclusiones Referencias  Preguntas de evaluación: Responder estas preguntas\n Objetivo general:\n ¿Se realizo el objetivo al 100%? ¿Que dificultades se presentaron?   Objetivos específicos:\n ¿Como se realizo cada objetivo especifico y que problemas ocurrieron en el desarrollo? ¿Cuales fueron los productos obtenidos?  Metodología:\n ¿Como se desarrollo cada etapa de la Metodología ? ¿Que problemas surgieron y como se solucionaron ?  Productos:\n Cuales productos se obtuvieron del proyecto  Conclusiones:\n ¿Que se concluye del análisis de los resultados? ¿Que te dejo de enseñanza el desarrollo de este proyecto?   ","date":1533132653,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1533132653,"objectID":"8aee31a1e213859cf9b40ad9af765ce9","permalink":"https://joserzapata.github.io/post/proyecto-grado/","publishdate":"2018-08-01T09:10:53-05:00","relpermalink":"/post/proyecto-grado/","section":"post","summary":"Tips para crear las diapositivas y para realizar la exposición del anteproyecto del trabajo de grado","tags":[],"title":"Tips para la Exposición del Proyecto de Grado","type":"post"},{"authors":["Jose R. Zapata"],"categories":null,"content":"","date":1518194435,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1518194435,"objectID":"97e7b094cd43d041338b9616d8a98a88","permalink":"https://joserzapata.github.io/event/pycon2018/","publishdate":"2019-11-25T17:09:35-05:00","relpermalink":"/event/pycon2018/","section":"event","summary":"Using Python for Audio signal analysis and Music Information Retrieval applications.","tags":["Music-information-retrieval","Python","Jupyter-notebook"],"title":"Audio signal analysis with python","type":"event"},{"authors":["Jose R. Zapata"],"categories":[],"content":"","date":1479424445,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1479424445,"objectID":"e27342eb41f7b23196a6aa49f2f702ba","permalink":"https://joserzapata.github.io/publication/tempo-estimation/","publishdate":"2019-10-19T07:52:23-05:00","relpermalink":"/publication/tempo-estimation/","section":"publication","summary":"","tags":["Tempo"],"title":"Tempo Estimation","type":"publication"},{"authors":["Jose R. Zapata"],"categories":null,"content":"","date":1469214000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1469214000,"objectID":"4021dfdafb0fc18462c764a683668b00","permalink":"https://joserzapata.github.io/event/aes2016/","publishdate":"2019-11-25T21:42:28-05:00","relpermalink":"/event/aes2016/","section":"event","summary":"El Music Information Retrieval (MIR), que tiene como objetivo extraer, analizar y procesar automaticamente la informacion musical en señales de audio para diferentes aplicaciones musicales e informaticas. Las principales tareas en MIR, son Beat tracking, deteccion de melodia, deteccion de acordes, fingerprinting, recomendacion de música, procesamiento de señales de audio, entre otras aplicaciones","tags":["Music-information-retrieval","Python"],"title":"Sistemas Automaticos para Extraccion de Información Musical (Generalidades y Aplicaciones)","type":"event"},{"authors":["Jose R. Zapata","Matthew E.P. Davies","Emilia Gómez"],"categories":[],"content":"","date":1396543937,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1396543937,"objectID":"3b89439c94c277af830c85d968bae75c","permalink":"https://joserzapata.github.io/publication/multifeature-beattracker/","publishdate":"2019-10-19T07:03:59-05:00","relpermalink":"/publication/multifeature-beattracker/","section":"publication","summary":"","tags":["Beat-tracking","Confidence-measure","IEEE"],"title":"Multi-Feature Beat Tracking","type":"publication"},{"authors":["Dmitry Bogdanov","Nicolas Wack","Emilia Gómez","Sankalp Gulati","Perfecto Herrera","Oscar Mayor","Gerard Roma","Justin Salamon","Jose R. Zapata","Xavier Serra"],"categories":[],"content":"","date":1395378695,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1395378695,"objectID":"9f230c47ee24c206024abe1fff9e2947","permalink":"https://joserzapata.github.io/publication/essentia-sigmm/","publishdate":"2019-10-20T06:49:16-05:00","relpermalink":"/publication/essentia-sigmm/","section":"publication","summary":"","tags":["Essentia","Beat-tracking","Music-information-retrieval","Python","ACM","Open-source"],"title":"Essentia: an open source library for audio analysis","type":"publication"},{"authors":["Jose R. Zapata"],"categories":[],"content":"","date":1388894438,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1388894438,"objectID":"f740bcb76b81f488e70bad46459faf14","permalink":"https://joserzapata.github.io/project/multifeature-beat-tracker/","publishdate":"2014-01-04T23:00:38-05:00","relpermalink":"/project/multifeature-beat-tracker/","section":"project","summary":"Matlab implementation of the Multi Feature Beat Tracker (Information Gain and Regularity), The essentia beat tracker, More details in [Multi-Feature Beat Tracking](https://joserzapata.github.io/publication/multifeaturebeattracker/)","tags":["Beat-tracking","Essentia","Matlab","Mirex","Python"],"title":"Multifeature Beat tracker","type":"project"},{"authors":[],"categories":[],"content":"","date":1385210712,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1385210712,"objectID":"ae28bdb513faa73c235bc5ea7467f50e","permalink":"https://joserzapata.github.io/project/essentia/","publishdate":"2013-11-23T07:45:12-05:00","relpermalink":"/project/essentia/","section":"project","summary":"Open-source C++ library for audio analysis and audio-based music information retrieval. It contains an extensive collection of reusable algorithms which implement audio input/output functionality, standard digital signal processing blocks, statistical characterization of data, and a large set of spectral, temporal, tonal and high-level music descriptors. More details in [Essentia: An Audio Analysis Library for Music Information Retrieval](https://joserzapata.github.io/publication/essentia-ismir)","tags":["Essentia","Beat-tracking","Python","Open-source"],"title":"Essentia","type":"project"},{"authors":["Dmitry Bogdanov","Nicolas Wack","Emilia Gómez","Sankalp Gulati","Perfecto Herrera","Oscar Mayor","Gerard Roma","Justin Salamon","Jose R. Zapata","Xavier Serra"],"categories":[],"content":"","date":1383539573,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1383539573,"objectID":"b2f301b98e01a2b1c2ef517e6e72334f","permalink":"https://joserzapata.github.io/publication/essentia-ismir/","publishdate":"2019-10-19T06:54:49-05:00","relpermalink":"/publication/essentia-ismir/","section":"publication","summary":"","tags":["Essentia","Beat-tracking","Music-information-retrieval","Python","ISMIR","Open-source"],"title":"Essentia: An Audio Analysis Library for Music Information Retrieval","type":"publication"},{"authors":["Dmitry Bogdanov","Nicolas Wack","Emilia Gómez","Sankalp Gulati","Perfecto Herrera","Oscar Mayor","Gerard Roma","Justin Salamon","Jose R. Zapata","Xavier Serra"],"categories":[],"content":"","date":1382332295,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1382332295,"objectID":"266fc3e7075adaa401902b92b429edba","permalink":"https://joserzapata.github.io/publication/essentia-acm/","publishdate":"2019-10-19T06:49:16-05:00","relpermalink":"/publication/essentia-acm/","section":"publication","summary":"","tags":["Essentia","Beat-tracking","Music-information-retrieval","Python","ACM","Open-source"],"title":"Essentia: an open-source library for sound and music analysis","type":"publication"},{"authors":["Jose R. Zapata"],"categories":[],"content":"","date":1379080190,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1379080190,"objectID":"3bbaff791be1d67f99d1269bde41adc9","permalink":"https://joserzapata.github.io/publication/phd-thesis/","publishdate":"2013-09-13T14:49:50+01:00","relpermalink":"/publication/phd-thesis/","section":"publication","summary":"describe a new method for the extraction of beat times with a confidence value from music audio, based on the measurement of mutual agreement between a committee of beat tracking systems.","tags":["Beat-tracking","Tempo","Comparative-evaluation","Phd"],"title":"Comparative evaluation and combination of automatic rhythm description systems","type":"publication"},{"authors":null,"categories":null,"content":"Comparative evaluation and combination of automatic rhythm description systems Zapata, Jose R. (2013). Comparative evaluation and combination of automatic rhythm description systems. Ph.D. thesis (Information and Communication Technologies), Universitat Pompeu Fabra, Barcelona, Spain, 2013.\n [TDX.cat] [PDF] [MTG] [Slides]  Thesis advisor  Dr. Emilia Gómez, Universitat Pompeu Fabra (MTG, UPF)  Defense board  Dr. Xavier Serra, Universitat Pompeu Fabra (MTG, UPF) Dr. Fabien Gouyon, Institute for Systems and Computer Engineering of Porto (SMC, INESC Porto) Dr. Juan Pablo Bello, New York University (MARL, NYU)  Abstract The automatic analysis of musical rhythm from audio, and more specifically tempo and beat tracking, is one of the fundamental open research problems in Music Information Retrieval (MIR) research. Automatic beat tracking is a valuable tool for the solution of other MIR problems, because enables the beat-synchronous analysis of music for tasks such as: structural segmentation, chord detection, music similarity, cover song detection, automatic remixing and interactive music systems. Even though automatic rhythm description is a relatively mature research topic in MIR and various algorithms have been proposed, tempo estimation and beat tracking remain an unsolved problem.\nRecent comparative studies of automatic rhythm description systems suggest that there has been little improvement in the state of the art over the last few years. In this thesis, we describe a new method for the extraction of beat times with a confidence value from music audio, based on the measurement of mutual agreement between a committee of beat tracking systems. Additionally, we present an open source approach which only requires a single beat tracking model and uses multiple onset detection functions for the mutual agreement. The method can also be used to identify music samples that are challenging for beat tracking without the need for ground truth annotations. Using the proposed method, we compiled a new dataset that consist of pieces that are difficult for state-of-the-art beat tracking algorithms. Through an international evaluation framework we show that our method yields the highest AMLc and AMLt accuracies obtained in this evaluation to date. Moreover, we compare our method to 20 reference systems using the largest existing annotated dataset for beat tracking and show that it outperforms all of the other systems under all the evaluation criteria used.\nIn the thesis we also conduct an extensive comparative evaluation and combination of automatic rhythm description systems. We evaluated 32 tempo estimation and 16 beat tracking state-of-the-art systems in order to identify their characteristics and investigated how they can be combined to improve performance. Finally, we proposed and evaluated the use of voice suppression algorithms in music signals with predominant vocals in order to improve the performance of existing beat tracking methods.\n","date":1379030400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1379030400,"objectID":"a5b0a930c95b729922bbf991c678c31e","permalink":"https://joserzapata.github.io/phd/","publishdate":"2013-09-13T00:00:00Z","relpermalink":"/phd/","section":"","summary":"Comparative evaluation and combination of automatic rhythm description systems","tags":null,"title":"Phd Tesis","type":"page"},{"authors":["Jose R. Zapata","Emilia Gómez"],"categories":[],"content":"","date":1369590165,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1369590165,"objectID":"2c227655fdc88a1d4b7bb84bd11157dc","permalink":"https://joserzapata.github.io/publication/voice-suppression-improve-beat-tracking/","publishdate":"2019-10-19T08:04:57-05:00","relpermalink":"/publication/voice-suppression-improve-beat-tracking/","section":"publication","summary":"","tags":["Beat-tracking","Source-separation","ICASSP","IEEE"],"title":"Using voice suppression algorithms to improve beat tracking in the presence of highly predominant vocals","type":"publication"},{"authors":["Andre Holzapfel","Matthew EP Davies","Jose R. Zapata","João Lobato Oliveira","Fabien Gouyon"],"categories":[],"content":"","date":1351962965,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1351962965,"objectID":"e70c170c3be55d89c3d164fb571decb6","permalink":"https://joserzapata.github.io/publication/selective-sampling-beat-tracking/","publishdate":"2019-10-19T07:16:58-05:00","relpermalink":"/publication/selective-sampling-beat-tracking/","section":"publication","summary":"","tags":["Beat-tracking","Selective-sampling","IEEE"],"title":"Selective Sampling for Beat Tracking Evaluation","type":"publication"},{"authors":["Jose R. Zapata","Matthew EP Davies","Andre Holzapfel","João Lobato Oliveira","Fabien Gouyon"],"categories":[],"content":"","date":1349743136,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1349743136,"objectID":"d1994bb3c7f67f1de178f42e0396ed2d","permalink":"https://joserzapata.github.io/publication/assigning-confidence/","publishdate":"2019-10-19T06:04:58-05:00","relpermalink":"/publication/assigning-confidence/","section":"publication","summary":"","tags":["Beat-tracking","Confidence-measure","ISMIR"],"title":"Assigning a confidence threshold on automatic beat annotation in large datasets","type":"publication"},{"authors":["Jose R. Zapata","Emilia Gómez"],"categories":[],"content":"","date":1340939862,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1340939862,"objectID":"8dbfa88e363fc9f2218f042feb3f4b47","permalink":"https://joserzapata.github.io/publication/improving-beat-tracking/","publishdate":"2019-10-19T06:58:59-05:00","relpermalink":"/publication/improving-beat-tracking/","section":"publication","summary":"","tags":["Beat-tracking","Source-separation","CMMR"],"title":"Improving Beat Tracking in the presence of highly predominant vocals using source separation techniques: Preliminary study","type":"publication"},{"authors":[],"categories":[],"content":"","date":1338143413,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1338143413,"objectID":"fc5dbad770fbd8b3b7e8be1d0746d834","permalink":"https://joserzapata.github.io/project/smc-beat-tracker-dataset/","publishdate":"2012-05-27T13:30:13-05:00","relpermalink":"/project/smc-beat-tracker-dataset/","section":"project","summary":"This beat tracking dataset contains 217 excerpts around 40s each, of which 19 are *easy* and the remaining 198 are *hard*. This dataset has been designed for radically new techniques which can contend with challenging beat tracking situations like: quiet accompaniment, expressive timing, changes in time signature, slow tempo, poor sound quality etc. More details in [Selective Sampling for Beat Tracking Evaluation](https://joserzapata.github.io/publication/selectivesampling/)","tags":["Beat-tracking","Dataset","Mirex","Open-source"],"title":"SMC Beat tracking Dataset","type":"project"},{"authors":["Andre Holzapfel","Matthew EP Davies","Jose R. Zapata","João Lobato Oliveira","Fabien Gouyon"],"categories":[],"content":"","date":1332722673,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1332722673,"objectID":"09b7f7f1c4f9e866e31a23eb77383ce8","permalink":"https://joserzapata.github.io/publication/automatic-identification/","publishdate":"2019-10-19T06:21:05-05:00","relpermalink":"/publication/automatic-identification/","section":"publication","summary":"","tags":["Beat-tracking","Selective-sampling","ICASSP","IEEE"],"title":"On the automatic identification of difficult examples for beat tracking: towards building new evaluation datasets","type":"publication"},{"authors":["Jose R. Zapata","Emilia Gómez"],"categories":[],"content":"","date":1311356015,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1311356015,"objectID":"29692cec3e93b1c622f0e22c687464e8","permalink":"https://joserzapata.github.io/publication/comparative-tempo/","publishdate":"2019-10-19T06:25:51-05:00","relpermalink":"/publication/comparative-tempo/","section":"publication","summary":"","tags":["Tempo","Comparative-evaluation","AES","Python"],"title":"Comparative Evaluation and Combination of Audio Tempo Estimation Approaches","type":"publication"},{"authors":["Jose R. Zapata","Ricardo A. Garcia"],"categories":[],"content":"","date":1222918493,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1222918493,"objectID":"bce0a52c36fdd4243279b6e0d92aa98d","permalink":"https://joserzapata.github.io/publication/efficient-detection/","publishdate":"2019-10-19T06:44:00-05:00","relpermalink":"/publication/efficient-detection/","section":"publication","summary":"","tags":["Audio-redundancy","AES"],"title":"Efficient Detection of Exact Redundancies in Audio Signals","type":"publication"},{"authors":["Jose R. Zapata","Tony Peñarredonda"],"categories":[],"content":"","date":1090780824,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1090780824,"objectID":"91b2e9159822a7089832be91b361afb0","permalink":"https://joserzapata.github.io/publication/control-rango-dinamico/","publishdate":"2019-11-14T13:40:24-05:00","relpermalink":"/publication/control-rango-dinamico/","section":"publication","summary":"","tags":["Fuzzy-logic","Dynamic-range-compression"],"title":"Control de Rango Dinamico en Audio con Logica Difusa","type":"publication"}]